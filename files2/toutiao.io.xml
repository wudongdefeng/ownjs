<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>b65acbac354cd57f38fa3af36d8110b6</guid>
<title>【Redis技术探索】「底层架构原理」帮你从底层彻底吃透RDB技术原理（入门第一步）_洛神灬殇的博客-CSDN博客</title>
<link>https://toutiao.io/k/2bcv6ot</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div id=&quot;content_views&quot; class=&quot;markdown_views prism-tomorrow-night&quot;&gt;
                    &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt;
                        &lt;path stroke-linecap=&quot;round&quot; d=&quot;M5,0 0,2.5 5,5z&quot; id=&quot;raphael-marker-block&quot;/&gt;
                    &lt;/svg&gt;
                    &lt;h3&gt;&lt;a id=&quot;_0&quot;/&gt;每日一句&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;低头是一种能力，它不是自卑，也不是怯弱，它是清醒中的嬗变。有时，稍微低一下头，或者我们的人生路会更精彩。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;a id=&quot;_4&quot;/&gt;前提概要&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Redis是一个的键-值（K-V）对的内存数据库服务，通常包含了任意个非空数据库。而每个非空的键值数据库中又可以存放任意个K-V，基本的结构如下图所示：&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/img_convert/4453aaa66c2960fe161b043a8c89e076.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt; 
 
&lt;h4&gt;&lt;a id=&quot;Redis_14&quot;/&gt;Redis服务器的结构&lt;/h4&gt; 
 
&lt;h3&gt;&lt;a id=&quot;RDB_22&quot;/&gt;RDB持久化方式&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;RDB持久化是指在指定的时间间隔内将redis内存中的数据集快照写入磁盘，实现原理是redis服务在指定的时间间隔内先fork一个子进程，由子进程将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储，生成dump.rdb文件存放在磁盘中。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/img_convert/dbd10c2f8f2bdc98e7371315cd4bd4ce.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a id=&quot;RDB_28&quot;/&gt;RDB机制&lt;/h3&gt; 
 
&lt;h3&gt;&lt;a id=&quot;RDB_35&quot;/&gt;RDB优势&lt;/h3&gt; 
&lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;相比于AOF机制，如果数据集很大，RDB的启动效率会更高。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt;&lt;/ul&gt; 
&lt;h3&gt;&lt;a id=&quot;RDB_45&quot;/&gt;RDB劣势&lt;/h3&gt; 
 
&lt;h3&gt;&lt;a id=&quot;RDB_51&quot;/&gt;RDB配置规则&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;在redis的6379.conf配置文件中：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;&lt;a id=&quot;_55&quot;/&gt;备份配置参数&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;save &amp;lt;seconds&amp;gt; &amp;lt;changes&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;save &amp;lt;指定时间间隔&amp;gt; &amp;lt;执行指定次数更新操作&amp;gt;，满足条件就将内存中的数据同步到硬盘中。官方出厂配置默认是 900秒内有1个更改，300秒内有10个更改以及60秒内有10000个更改，则将内存中的数据快照写入磁盘。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code&gt;save 900 1      #在900秒（15分钟）之后，如果至少有一个key发生变化，则dump内存快照
save 300 10     #在300秒（15分钟）之后，如果至少有10个key发生变化，则dump内存快照
save 60 10000   #在60秒（1分钟）之后，如果至少有10000个key发生变化，则dump内存快照
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;a id=&quot;_69&quot;/&gt;文件配置参数&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;默认的rdb文件路径是当前目录,文件名是dump.rdb,可以在配置文件中修改路径和文件名,分别是dir和dbfilename.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code&gt;# 存放快照的目录
dir ./ # rdb文件存储路径
dbfilename dump.rdb # rdb文件名
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;a id=&quot;_79&quot;/&gt;压缩配置参数&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;在进行镜像备份时,是否进行压缩。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code&gt;rdbcompression yes  #Redis默认是开启压缩的。
# yes：压缩，但是需要一些cpu的消耗。
# no：不压缩，需要更多的磁盘空间。
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;如果没有触发自动快照,需要对Redis执行手动快照操作,save和bgsave命令来手动快照,两个命令是：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt;&lt;li&gt;SAVE：由主进程进行快照，会阻塞其他请求。&lt;/li&gt;&lt;li&gt;BGSAVE：通过fork子进程进行快照，不会阻塞其他请求。&lt;/li&gt;&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;注意:由于Redis使用fork来复制一份当前进程,那么子进程就会占有和主进程一样的内存资源,比如说主进程8G内存,那么在备份的时候,必须保证有16G的内存,要不然会启用虚拟内存,性能非常的差。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;&lt;a id=&quot;_96&quot;/&gt;快照的过程如下：&lt;/h4&gt; 
&lt;ol&gt;&lt;li&gt;Redis使用fork函数复制一份当前进程（父进程）的副本（子进程）；&lt;/li&gt;&lt;li&gt;父进程继续接收并处理客户端发来的命令，而子进程开始将内存中的数据写入硬盘中的临时文件；&lt;/li&gt;&lt;li&gt;当子进程写入完所有数据后会用该临时文件替换旧的RDB文件，至此一次快照操作完成。（注意：会存在写一部命令压缩缓存区，记录写入rdb文件时候的操作）&lt;/li&gt;&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;在执行fork的时候操作系统会使用写时复制（copy-on-write）策略，即fork函数发生的一刻父子进程共享同一内存数据，当父进程要更改其中某片数据时（如执行一个写命令），操作系统会将该片数据复制一份以保证子进程的数据不受影响，所以新的RDB文件存储的是执行fork时那一刻的内存快照数据。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;通过上述过程可以发现Redis在进行快照的过程中不会修改RDB文件，只有快照结束后才会将旧的文件替换成新的，也就是说任何时候RDB文件都是完整的。这使得可以通过定时备份RDB文件来实现Redis数据库备份。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h5&gt;&lt;a id=&quot;_106&quot;/&gt;快照的过程压缩分析：&lt;/h5&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;RDB文件是经过压缩（上文介绍了：可以配置rdbcompression参数以禁用压缩节省CPU占用）的二进制格式，所以占用的空间会小于内存中的数据大小，更加利于传输。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h5&gt;&lt;a id=&quot;_110&quot;/&gt;快照的读取加载过程：&lt;/h5&gt; 
&lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;Redis启动后会读取RDB快照文件，将数据从硬盘载入到内存。根据数据量大小与结构和服务器性能不同，这个时间也不同。通常将一个记录一千万个字符串类型键、大小为1GB的快照文件载入到内存中需要花费20～30秒钟&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;通过RDB方式实现持久化，一旦Redis异常退出，就会丢失最后一次快照以后更改的所有数据。这就需要开发者根据具体的应用场合，通过组合设置自动快照条件的方式来将可能发生的数据损失控制在能够接受的范围。如果数据很重要以至于无法承受任何损失，则可以考虑使用AOF方式进行持久化。&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt;&lt;/ul&gt; 
&lt;h3&gt;&lt;a id=&quot;RDB__116&quot;/&gt;RDB 的优缺点&lt;/h3&gt; 
&lt;h3&gt;&lt;a id=&quot;_118&quot;/&gt;优点：&lt;/h3&gt; 
&lt;ol&gt;&lt;li&gt;适合大规模的数据恢复。&lt;/li&gt;&lt;li&gt;如果业务对数据完整性和一致性要求不高，RDB是很好的选择。&lt;/li&gt;&lt;/ol&gt; 
&lt;h3&gt;&lt;a id=&quot;_122&quot;/&gt;缺点：&lt;/h3&gt; 
&lt;ol&gt;&lt;li&gt;数据的完整性和一致性不高，因为RDB可能在最后一次备份时宕机了。&lt;/li&gt;&lt;li&gt;备份时占用内存，因为Redis 在备份时会独立创建一个子进程，将数据写入到一个临时文件（此时内存中的数据是原来的两倍），最后再将临时文件替换之前的备份文件。&lt;/li&gt;&lt;li&gt;由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。(回写和覆盖的时候用的是主进程)。&lt;/li&gt;&lt;/ol&gt; 
&lt;h4&gt;&lt;a id=&quot;RDBAOFAOF_128&quot;/&gt;RDB与AOF二者选择的标准（虽然还没有讲AOF，提前普及）&lt;/h4&gt; 
 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Redis允许同时开启AOF和RDB，既保证了数据安全又使得进行备份等操作十分容易。此时重新启动Redis后Redis会使用AOF文件来恢复数据，因为AOF方式的持久化可能丢失的数据更少。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;a id=&quot;_137&quot;/&gt;总结&lt;/h3&gt; 
&lt;ul&gt;&lt;li&gt; &lt;p&gt;Redis 默认开启RDB持久化方式，在指定的时间间隔内，执行指定次数的写操作，则将内存中的数据写入到磁盘中。&lt;/p&gt; &lt;/li&gt;&lt;li&gt; &lt;p&gt;RDB 持久化适合大规模的数据恢复但它的数据一致性和完整性较差。&lt;/p&gt; &lt;/li&gt;&lt;li&gt; &lt;p&gt;Redis 需要手动开启AOF持久化方式，默认是每秒将写操作日志追加到AOF文件中。&lt;/p&gt; &lt;/li&gt;&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;所以Redis的持久化和数据的恢复要选择在夜深人静的时候执行是比较合理的。&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt;
                &lt;/div&gt;
                
                
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>f7bfa24df85b120c385ee24fb6ab6d2d</guid>
<title>3分钟搞清楚 JVM逃逸分析</title>
<link>https://toutiao.io/k/7jeznmw</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot; data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你好，我是田哥&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;作为一个合格java开发者都知道，基本上所有对象都是在堆上创建。但是，这里还是没有把话说绝对哈，指的是&lt;strong&gt;基本上所有&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;昨天一位朋友在面试中，就说了所有对象都在堆中创建，然后背面试官一阵的嘲笑。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;开始我们的正文，我们今天来聊聊关于逃逸分析。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;逃逸分析&lt;/strong&gt;（Escape Analysis）是目前Java虚拟机中比较前沿的优化技术。这是一种可以有效减少Java 程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。通过逃逸分析，Java Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;逃逸分析的基本原理是&lt;/strong&gt;：分析对象动态作用域，当一个对象在方法里面被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，这种称为方法逃逸；甚至还有可能被外部线程访问到，譬如赋值给可以在其他线程中访问的实例变量，这种称为线程逃逸；从不逃逸、方法逃逸到线程逃逸，称为对象由低到高的不同逃逸程度。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;开启逃逸分析，编译器可以对代码进行如下优化&lt;/strong&gt;：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;同步消除：如果一个对象被逃逸分析发现只能被一个线程所访问，那对于这个对象的操作可以不同步。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;栈上分配：如果确定一个对象不会逃逸出线程之外，那让这个对象在栈上分配内存将会是一个很不错的主意，对象所占用的内存空间就可以随栈帧出栈而销毁。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;标量替换：如果一个对象被逃逸分析发现不会被外部方法访问，并且这个对象可以拆散，那么程序真正执行的时候将可能不去创建这个对象，而改为直接创建它的若干个比这个方法使用的成员变量来代替。&lt;strong&gt;将对象拆分后，可以让对象的成员变量在栈上分配和读写&lt;/strong&gt;。&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;JVM中通过如下参数可以指定是否开启逃逸分析&lt;/strong&gt;：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;-XX:+DoEscapeAnalysis&lt;/code&gt; ：表示开启逃逸分析（JDK 1.7之后默认开启）。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;-XX:-DoEscapeAnalysis&lt;/code&gt; ：表示关闭逃逸分析。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;同步消除&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;线程同步本身是一个相对耗时的过程，如果逃逸分析能够确定一个变量不会逃逸出线程，无法被其他线程访问，那么这个变量的读写肯定就不会有竞争，对这个变量实施的同步措施也就可以安全地消除掉。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如以下代码：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;method&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    Object o = &lt;span&gt;new&lt;/span&gt; Object();&lt;br/&gt;    &lt;span&gt;synchronized&lt;/span&gt; (o) {&lt;br/&gt;        System.out.println(o);&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对&lt;code&gt;对象o&lt;/code&gt;加锁，但是对象o的生命周期与方法method()一样，所以不会被其他线程访问到，不会发生线程安全问题，那么在JIT编译阶段会被优化为如下所示：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;method&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    Object o = &lt;span&gt;new&lt;/span&gt; Object();&lt;br/&gt;    System.out.println(o);&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这也被称为&lt;strong&gt;锁消除&lt;/strong&gt;。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;栈上分配&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在Java虚拟机中，Java堆上分配创建对象的内存空间几乎是Java程序员都知道的常识，Java堆中的对象对于各个线程都是共享和可见的，只要持有这个对象的引用，就可以访问到堆中存储的对象数据。虚拟机的垃圾收集子系统会回收堆中不再使用的对象，但回收动作无论是标记筛选出可回收对象，还是回收和整理内存，都需要耗费大量资源。但是，存在一种特殊情况，如果逃逸分析确认对象不会逃逸出线程之外，那么就可能被优化成栈上分配。这样就无需在堆上分配内存，也无须进行垃圾回收了。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如以下代码：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;main&lt;/span&gt;&lt;span&gt;(String[] args)&lt;/span&gt; &lt;span&gt;throws&lt;/span&gt; InterruptedException &lt;/span&gt;{&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;for&lt;/span&gt; (&lt;span&gt;int&lt;/span&gt; i = &lt;span&gt;0&lt;/span&gt;; i &amp;lt; &lt;span&gt;1000000&lt;/span&gt;; i++) {&lt;br/&gt;        alloc();&lt;br/&gt;    }&lt;br/&gt;&lt;br/&gt;    Thread.sleep(&lt;span&gt;100000&lt;/span&gt;);&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;private&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;alloc&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    User user = &lt;span&gt;new&lt;/span&gt; User();&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;代码很简单，就是循环创建100万次，使用alloc()方法创建100万个User对象。这里的alloc()方法中定义了User对象并没有被其他方法引用，所以符合栈上分配的要求。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;JVM参数如下：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;-Xmx2G -Xms2G -XX:+DoEscapeAnalysis -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError &lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;启动程序，通过jmap工具查看实例数：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;jmap -histo pid&lt;br/&gt;&lt;br/&gt;num     &lt;span&gt;#instances         #bytes  class name&lt;/span&gt;&lt;br/&gt;----------------------------------------------&lt;br/&gt;1:          3771        2198552  [B&lt;br/&gt;2:         10617        1722664  [C&lt;br/&gt;3:        104057        1664912  com.miracle.current.lock.StackAllocationTest&lt;span&gt;$User&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们可以看到程序总共创建了104057个User对象，远小于100万。我们可以关闭逃逸分析再来看下：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;-Xmx2G -Xms2G -XX:-DoEscapeAnalysis -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError &lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;启动程序，通过jmap工具查看实例数：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;jmap -histo 42928&lt;br/&gt;&lt;br/&gt; num     &lt;span&gt;#instances         #bytes  class name&lt;/span&gt;&lt;br/&gt;----------------------------------------------&lt;br/&gt;   1:           628       22299176  [I&lt;br/&gt;   2:       1000000       16000000  com.miracle.current.lock.StackAllocationTest&lt;span&gt;$User&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;可以看到，关闭逃逸分析后总共创建了100万个User对象。对比来看，栈上分配对堆内存消耗，GC都有着重要的作用。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;标量替换&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;若一个数据已经无法再分解成更小的数据来表示了，Java虚拟机中的原始数据类型（int 、long 等数值类型及reference类型等）都不能再进一步分解了，那么这些数据就可以被称为标量。相对的，如果一个数据可以继续分解，那它就被称为聚合量（Aggregate），Java中的对象就是典型的聚合量。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;假如逃逸分析能够证明一个对象不会被方法外部访问，并且这个对象可以被拆散，那么程序真正执行的时候将可能不去创建这个对象，而改为直接创建它的若干个被这个方法使用的成员变量来代替。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;有如下代码：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;main&lt;/span&gt;&lt;span&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;br/&gt;    method();&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;private&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;method&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    User user = &lt;span&gt;new&lt;/span&gt; User(&lt;span&gt;25&lt;/span&gt;);&lt;br/&gt;&lt;br/&gt;    System.out.println(user.age);&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;private&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;User&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; &lt;span&gt;int&lt;/span&gt; age;&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;User&lt;/span&gt;&lt;span&gt;(&lt;span&gt;int&lt;/span&gt; age)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        &lt;span&gt;this&lt;/span&gt;.age = age;&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在&lt;code&gt;method()&lt;/code&gt;方法中创建User对象，指定age为25，这里User不会被其他方法引用，也就是说它不会逃逸出方法，并且User是可以拆解为标量的。所以&lt;code&gt;alloc()&lt;/code&gt;代码会优化为如下：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;private&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;alloc&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    &lt;span&gt;int&lt;/span&gt; age = &lt;span&gt;25&lt;/span&gt;;&lt;br/&gt;&lt;br/&gt;    System.out.println(age);&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;总结&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;尽管目前逃逸分析技术仍在发展之中，未完全成熟，但它是即时编译器优化技术的一个重要前进方向，在日后的Java虚拟机中，逃逸分析技术肯定会支撑起一系列更实用、有效的优化技术。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你也在准备面试，记得找我，我给你做最专业面试辅导。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;欢迎加入我的知识星球：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;2.818840579710145&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/07BicZywOVtkfh91a09FywgiapFm9TjTt1dJcJsJSDDhT1FibtQy4X4gmrOdwnhSaFogqRVB7tLVnFvYBmMw2ZNqQ/640?wx_fmt=png&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1&quot; data-type=&quot;png&quot; data-w=&quot;1242&quot;/&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;知识星球详情，可以进入我的博客：&lt;span&gt;&lt;strong&gt;http://woaijava.cc/blog/1&lt;/strong&gt;&lt;/span&gt;&lt;span/&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;模拟面试重点：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;快速发现你的不足，如何改正&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;回答问题方式不对，如何回答&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;复习太慢，如何快速复习&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;软实力问题，如何回答&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;传授一些面试技巧&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>5b5473f7c69ce99ff6176df00f716cec</guid>
<title>阿里一面：Spring 如何解决循环依赖！</title>
<link>https://toutiao.io/k/rjvjmav</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot; data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;strong&gt;大家好，我是Tom哥！&lt;/strong&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;Spring 如何解决循环依赖，网上的资料很多，但是感觉写得好的极少，特别是源码解读方面，我就自己单独出一篇，&lt;/span&gt;&lt;strong&gt;这篇文章绝对肝！&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong/&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;不 BB，上文章目录。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.6021409455842998&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNO1bBHac1mGCxEHO35Xv8HLlc7wTQ7TJQJgFgP3sTkgUOmZjNYfbXQqg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1121&quot;/&gt;&lt;/figure&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;1. 基础知识&lt;/h1&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;1.1 什么是循环依赖 ？&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;一个或多个对象之间存在直接或间接的依赖关系，这种依赖关系构成一个环形调用，有下面 3 种方式。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.3935309973045822&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOCUk9DAyeeice7mC1Az8NHOCX2Hic3AcYZZEd4ZmjeTNhzH5Rd6lv02yg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;742&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们看一个简单的 Demo，对标“情况 2”。&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;@Service&lt;/span&gt;&lt;br/&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;Louzai1&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;@Autowired&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; Louzai2 louzai2;&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;test1&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;@Service&lt;/span&gt;&lt;br/&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;Louzai2&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    &lt;span&gt;@Autowired&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; Louzai1 louzai1;&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;test2&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这是一个经典的循环依赖，它能正常运行，后面我们会通过源码的角度，解读整体的执行流程。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;1.2 三级缓存&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;解读源码流程之前，spring 内部的三级缓存逻辑必须了解，要不然后面看代码会蒙圈。&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;第一级缓存&lt;/strong&gt;：singletonObjects，用于保存实例化、注入、初始化完成的 bean 实例；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;第二级缓存&lt;/strong&gt;：earlySingletonObjects，用于保存实例化完成的 bean 实例；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;第三级缓存&lt;/strong&gt;：singletonFactories，用于保存 bean 创建工厂，以便后面有机会创建代理对象。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这是最核心，我们直接上源码：&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.6916354556803995&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOvk3AiaCDWY8JN7rwGJvRQ8HMpOo41J3ibJbaRGB3105qyeQNmAzcLSyQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;801&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;执行逻辑：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;先从“第一级缓存”找对象，有就返回，没有就找“二级缓存”；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;找“二级缓存”，有就返回，没有就找“三级缓存”；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;找“三级缓存”，找到了，就获取对象，放到“二级缓存”，从“三级缓存”移除。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;1.3 原理执行流程&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我把“情况 2”执行的流程分解为下面 3 步，是不是和“套娃”很像 ？&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5404624277456648&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOQZgjQPEQ1hwh03fiaibvicukAp8QiaAj2ofz9uOsWFmRd1SZ9YayrkCnWw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1038&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;整个执行逻辑如下：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;在第一层中，先去获取 A 的 Bean，发现没有就准备去创建一个，然后将 A 的代理工厂放入“三级缓存”（&lt;strong&gt;这个 A 其实是一个半成品，还没有对里面的属性进行注入&lt;/strong&gt;），但是 A 依赖 B 的创建，就必须先去创建 B；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;在第二层中，准备创建 B，发现 B 又依赖 A，需要先去创建 A；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;在第三层中，去创建 A，因为第一层已经创建了 A 的代理工厂，&lt;strong&gt;直接从“三级缓存”中拿到 A 的代理工厂，获取 A 的代理对象，放入“二级缓存”&lt;/strong&gt;，并清除“三级缓存”；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;回到第二层，现在有了 A 的代理对象，对 A 的依赖完美解决（&lt;strong&gt;这里的 A 仍然是个半成品&lt;/strong&gt;），B 初始化成功；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;回到第一层，现在 B 初始化成功，完成 A 对象的属性注入，然后再填充 A 的其它属性，以及 A 的其它步骤（包括 AOP），完成对 A 完整的初始化功能（&lt;strong&gt;这里的 A 才是完整的 Bean&lt;/strong&gt;）。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;将 A 放入“一级缓存”。&lt;/strong&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;为什么要用 3 级缓存 ？我们先看源码执行流程，后面我会给出答案。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2. 源码解读&lt;/h1&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;注意：Spring 的版本是&lt;strong&gt;5.2.15.RELEASE&lt;/strong&gt;，否则和我的代码不一样！！！&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;上面的知识，网上其实都有，下面才是我们的重头戏，让你跟着楼仔，走一遍代码流程。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.1 代码入口&lt;/h2&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;1.0429594272076372&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOoCr5rjzaficibg7TicZ1gm8b8kR8u4HJyB1ABRtmH0icbbnqCslxZtIQYg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;838&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.7092651757188498&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOribwkzRb9Zyy8XicicyPHJlSO1AGaFLdSYmiaDD9k9WXo6m5WibWcDKl7jg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;939&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里需要多跑几次，把前面的 beanName 跳过去，只看 louzai1。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.9205103042198234&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOicWQQQckoiaDnu2xBtLe2XMnQwslx2sstS18OiczfP6UiaNic3ib1LFmkv1A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1019&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.11724137931034483&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOlLXkAbQickHqSevBt03Jtud05gCtpvPS9mesicCTwmz0j8qJT97yibacw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;870&quot;/&gt;&lt;/figure&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.2 第一层&lt;/h2&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.2490118577075099&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNODOytdibzgpUXNpSMDXp9iazlqwDMaqicxrCT315miajFsApWmtIiaDSX1KQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1012&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;进入 doGetBean()，从 getSingleton() 没有找到对象，进入创建 Bean 的逻辑。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.38477580813347234&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOLSwPYmzD0ic1I33ZPISBUm9aXDPWklLP1nibLAFoS3ZsoE61NcGF81TQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;959&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;1.0844390832328106&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOhCia75vp68b1syrIF5KggFicotRqV9YWEjgTo7EwoLE1k4WRs1PjLkew/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;829&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;进入 doCreateBean() 后，调用 addSingletonFactory()。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;1.0824622531939605&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNO9tx6jk4cLlTibhiax6dB67cQkArxyzspw3Wq12yvwORzP8jicB0sPC7rw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;861&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;往三级缓存 singletonFactories 塞入 louzai1 的工厂对象。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.478584729981378&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOY35CnbLdL7p2QmoyUcc0RTljdc5XUSHCc89UDibjah7Gmn3nSwlmDsw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1074&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5518814139110604&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNO4RibGWBk6C4KO5f61Bhm1F1ZwHjL4M9FtSs9shEneCib8IftiaibkbQQKw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;877&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;进入到 populateBean()，执行 postProcessProperties()，这里是一个策略模式，找到下图的策略对象。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.409250175192712&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOFYpibXKiawoibkYFH7QLUWs51elYkPA013efmDTj8AbvrIic96VldrM7Iw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1427&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;正式进入该策略对应的方法。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.554932735426009&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOibX5MOdF9p8Nhk2Eq0e3hd5OIMbSUIZ0RglMKiaibxI37SoaymamqlPtg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;892&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;下面都是为了获取 louzai1 的成员对象，然后进行注入。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.2819593787335723&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOl9A6NV70She9icl5WZqsvtiblAw2ZdqZCrNGHwdd1a8ZT2Iv54rx1FnA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;837&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.6040868454661558&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOPS5wsCT5BP1XibqAugQynytiaic0Zng5fRjHFLoVxtbBe66ZkZx4W0xew/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;783&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.34579439252336447&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNO7YhbX6SlqCU6KyaZLdQlwc7GY4SKyR6U9SEvia6L0HsptIOVquwZqrg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;963&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5714285714285714&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOSwmZegYMEzicxpGw5ZgkVW8vWOTAokbgWNK2ZpfXagZfc7tsMSsGDYw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;994&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;进入 doResolveDependency()，找到 louzai1 依赖的对象名 louzai2&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.24548049476688868&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNON6wldlCCdf6AaTXcDbcsSiaGOZDxGibp1gNEWVjpZBrJvso1PDfPv3bg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1051&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;需要获取 louzai2 的 bean，是 AbstractBeanFactory 的方法。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.34022988505747126&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNO9yW8MRkae4lQqnZLjnWnJ6AA1dbh6BoMFrEx1licVHA1RibMic8eggP4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;870&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;正式获取 louzai2 的 bean。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.1444723618090452&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOqtv8E8CX8O4MB830hMoSK2YqvIYI7wDlsdYxic7jsnyPnooSn6JebhA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;796&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;到这里，第一层套娃基本结束，因为 louzai1 依赖 louzai2，下面我们进入第二层套娃。&lt;/strong&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.3 第二层&lt;/h2&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.09919839679358718&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOpfvYbyxOvVlwvzxv94CtniaQzibXLK1G7HfM3PcPygUr3RgkXdKE2h0g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;998&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;获取 louzai2 的 bean，从 doGetBean()，到 doResolveDependency()，和第一层的逻辑完全一样，找到 louzai2 依赖的对象名 louzai1。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;前面的流程全部省略，直接到 doResolveDependency()。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5810945273631841&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOvKMaqFvSKo0qahT78icU1Uf4T70F54bRqk7hdqhorqW6r6efibwkwqHg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1005&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;正式获取 louzai1 的 bean。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.3952772073921971&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNO13vQDjUKscx8Rg7wExCQN0m7ic298kfXXg4grLoia0BfhHjgS7SMiadeg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;974&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;到这里，第二层套娃结束，因为 louzai2 依赖 louzai1，所以我们进入第三层套娃。&lt;/strong&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.4 第三层&lt;/h2&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.1619631901840491&quot; data-type=&quot;png&quot; data-w=&quot;815&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOI01icQAGFZWCibO1fFRz9ta32icwObTp0EkJiaVYuXCgm6G10OzK5nyibWQ/640?wx_fmt=png&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;获取 louzai1 的 bean，在第一层和第二层中，我们每次都会从 getSingleton() 获取对象，但是由于之前没有初始化 louzai1 和 louzai2 的三级缓存，所以获取对象为空。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.35911602209944754&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOiah5c2Z1hjtHoibMeFJXM37cuf7tHOnfjblJQwU1OPKc0yicBW7fibtHrg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;905&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.15081967213114755&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOxKSlncPw1azxuls349583gZsC1L88jTcE6sDpia4eCIyNOYF2N9RFbA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;915&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;敲重点！敲重点！！敲重点！！！&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;到了第三层，由于第三级缓存有 louzai1 数据，这里使用三级缓存中的工厂，为 louzai1 创建一个代理对象，塞入二级缓存。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.7829145728643216&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOZZNM1sBrrH0juatDb5IQfYUHicgsuJV7SYxCWia9AskoQ0Kkw4CgNhuQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;995&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里就拿到了 louzai1 的代理对象，解决了 louzai2 的依赖关系，返回到第二层。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.5 返回第二层&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;返回第二层后，louzai2 初始化结束，这里就结束了么？二级缓存的数据，啥时候会给到一级呢？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;甭着急，看这里，还记得在 doGetBean() 中，我们会通过 createBean() 创建一个 louzai2 的 bean，当 louzai2 的 bean 创建成功后，我们会执行 getSingleton()，它会对 louzai2 的结果进行处理。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.33444816053511706&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOW1eibGicxv7f0ThvM1hXdp0GeMA0BaY5KCRZIZZQr0F7LxJJdmZ0GWcQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;897&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们进入 getSingleton()，会看到下面这个方法。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.3785140562248996&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOIDF4icrhq4Iao8gQJrDfeIHz0MNjUBS7ZAEPZjl1CH9dloYX56pr5JA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;996&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里就是处理 louzai2 的 一、二级缓存的逻辑，将二级缓存清除，放入一级缓存。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.368006993006993&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOpT8PriaWibiaAdPP5ODaO47FM3cVyfL8NWvSSs1dQAnfnSuKMa6sKxllg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1144&quot;/&gt;&lt;/figure&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.6 返回第一层&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;同 2.5，louzai1 初始化完毕后，会把 louzai1 的二级缓存清除，将对象放入一级缓存。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.3647912885662432&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNO0RMJoTP25eOrA3r2FRKnJib0jtTTAglW2ibFTGygD0QgExl3zNFbDiaOQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1102&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;到这里，所有的流程结束，我们返回 louzai1 对象。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;3. 原理深度解读&lt;/h1&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;3.1 什么要有 3 级缓存 ？&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这是一道非常经典的面试题，前面已经告诉大家详细的执行流程，包括源码解读，但是没有告诉大家为什么要用 3 级缓存？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;这里是重点！敲黑板！！！&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们先说“一级缓存”的作用，变量命名为 singletonObjects，结构是 Map&amp;lt;String, Object&amp;gt;，它就是一个单例池，将初始化好的对象放到里面，给其它线程使用，&lt;strong&gt;如果没有第一级缓存，程序不能保证 Spring 的单例属性。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;“二级缓存”先放放，我们直接看“三级缓存”的作用，变量命名为 singletonFactories，结构是 Map&amp;lt;String, ObjectFactory&amp;lt;?&amp;gt;&amp;gt;，Map 的 Value 是一个对象的代理工厂，所以“三级缓存”的作用，其实就是用来存放对象的代理工厂。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那这个对象的代理工厂有什么作用呢，我先给出答案，&lt;strong&gt;它的主要作用是存放半成品的单例 Bean，目的是为了“打破循环”&lt;/strong&gt;，可能大家还是不太懂，这里我再稍微解释一下。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们回到文章开头的例子，创建 A 对象时，会把实例化的 A 对象存入“三级缓存”，这个 A 其实是个半成品，因为没有完成 A 的依赖属性 B 的注入，所以后面当初始化 B 时，B 又要去找 A，这时就需要从“三级缓存”中拿到这个半成品的 A（这里描述，其实也不完全准确，因为不是直接拿，为了让大家好理解，我就先这样描述），打破循环。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那我再问一个问题，&lt;strong&gt;为什么“三级缓存”不直接存半成品的 A，而是要存一个代理工厂呢 ？答案是因为 AOP。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在解释这个问题前，我们看一下这个代理工厂的源码，让大家有一个更清晰的认识。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;直接找到创建 A 对象时，把实例化的 A 对象存入“三级缓存”的代码，直接用前面的两幅截图。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;1.0824622531939605&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNO9tx6jk4cLlTibhiax6dB67cQkArxyzspw3Wq12yvwORzP8jicB0sPC7rw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;861&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.478584729981378&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOY35CnbLdL7p2QmoyUcc0RTljdc5XUSHCc89UDibjah7Gmn3nSwlmDsw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1074&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;下面我们主要看这个对象工厂是如何得到的，进入 getEarlyBeanReference() 方法。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.30063291139240506&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOxq2eMicDbZibKcXTChXZOT8BB0gS32m6yhhehKzhI3oESHLYOfFhu4RA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;948&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.2850194552529183&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNObvFJr4iaatfQ3dmG85qHfpGSTnBE0XCBiaxZ1Htakl1QDwR403SAicdJA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1028&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.14558979808714134&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOr1aMYsBoBib2qzI9uhhtvvjAey7P63KhOdqSelcRhlqgLiaMITJaGqAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;941&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.6501079913606912&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/sXFqMxQoVLHqKibakH9hO9wuncU5q2bNOCN3LKbGzmQqWaQ5Oj3bbsIsHXfiankZkBKYcyozYgicOmcWJia8q2paiag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;926&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;最后一幅图太重要了，我们知道这个对象工厂的作用：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;如果 A 有 AOP，就创建一个代理对象；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;如果 A 没有 AOP，就返回原对象。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;那“二级缓存”的作用就清楚了，就是用来存放对象工厂生成的对象，这个对象可能是原对象，也可能是个代理对象。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我再问一个问题，为什么要这样设计呢？把二级缓存干掉不行么 ？我们继续往下看。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;3.2 能干掉第 2 级缓存么 ？&lt;/h1&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;@Service&lt;/span&gt;&lt;br/&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;A&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;@Autowired&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; B b;&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;@Autowired&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; C c;&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;test1&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;@Service&lt;/span&gt;&lt;br/&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;B&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    &lt;span&gt;@Autowired&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; A a;&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;test2&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;@Service&lt;/span&gt;&lt;br/&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;C&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;@Autowired&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; A a;&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;test3&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;根据上面的套娃逻辑，A 需要找 B 和 C，但是 B 需要找 A，C 也需要找 A。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;假如 A 需要进行 AOP&lt;/strong&gt;，因为代理对象每次都是生成不同的对象，如果干掉第二级缓存，只有第一、三级缓存：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;B 找到 A 时，直接通过三级缓存的工厂的代理对象，生成对象 A1。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;C 找到 A 时，直接通过三级缓存的工厂的代理对象，生成对象 A2。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;看到问题没？&lt;strong&gt;你通过 A 的工厂的代理对象，生成了两个不同的对象 A1 和 A2&lt;/strong&gt;，所以为了避免这种问题的出现，我们搞个二级缓存，把 A1 存下来，下次再获取时，直接从二级缓存获取，无需再生成新的代理对象。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;所以“二级缓存”的目的是为了避免因为 AOP 创建多个对象，其中存储的是半成品的 AOP 的单例 bean。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果没有 AOP 的话，我们其实只要 1、3 级缓存，就可以满足要求。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;4. 写在最后&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们再回顾一下 3 级缓存的作用：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;一级缓存：&lt;strong&gt;为“Spring 的单例属性”而生&lt;/strong&gt;，就是个单例池，用来存放已经初始化完成的单例 Bean；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;二级缓存：&lt;strong&gt;为“解决 AOP”而生&lt;/strong&gt;，存放的是半成品的 AOP 的单例 Bean；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;三级缓存：&lt;strong&gt;为“打破循环”而生&lt;/strong&gt;，存放的是生成半成品单例 Bean 的工厂方法。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你能理解上面我说的三条，恭喜你，你对 Spring 的循环依赖理解得非常透彻！&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;关于循环依赖的知识，其实还有，因为篇幅原因，我就不再写了，&lt;strong&gt;这篇文章的重点，一方面是告诉大家循环依赖的核心原理，另一方面是让大家自己去 debug 代码&lt;/strong&gt;，跑跑流程，挺有意思的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;可能有同学会问 “楼哥，你之前是不是经常看源码，然后这个流程，你是不是 debug 了很久？”&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我之前其实没怎么看过开源代码，这个流程，前期理论知识看了 2.5 个小时，然后 debug 4.5 小时，就基本全部走通了，&lt;strong&gt;最难的地方，就是三层套娃，稍微有些绕。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里也简单说一下我看源码的心得：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;需要掌握基本的设计模式；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;看源码前，最好能找一些理论知识先看看；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;学会读英文注释，不会的话就百度翻译；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;debug 时，&lt;strong&gt;要克制自己，不要陷入无用的细节&lt;/strong&gt;，这个最重要。&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;其中最难的是第 4 步，因为很多同学看 Spring 源码，每看一个方法，就想多研究研究，这样很容易被绕进去了，这个&lt;strong&gt;要学会克制，有大局观，并能分辨哪里是核心逻辑&lt;/strong&gt;，至于如何分辨，可以在网上先找些资料，如果没有的话，就只能多看代码了。&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;··············&lt;/span&gt;  END  ··············&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;关于我：Tom哥，前阿里技术专家，拿过 鹅厂、百度、华为 等6家大厂offer，CSDN 博客专家，面试过 500+ 候选人，职场经验丰富。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;2022，异常寒冷，新建了「技术群」，广邀各路朋友探讨职场、&lt;span&gt;&lt;span&gt;技术、&lt;/span&gt;&lt;/span&gt;人生，多个朋友多条路。抱团取暖，一起牛逼。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27&quot;&gt;&lt;section mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;&quot; data-ratio=&quot;1.092130518234165&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/2KTof9YshwdaSEqIP60fNXXqd7e15vycGo75wsfR2yTDKxOpJaKG0BKzFod8CQxLtibu2vMp1GKBMDJRbk3rpLw/640?wx_fmt=jpeg&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1&quot; data-type=&quot;jpeg&quot; data-w=&quot;521&quot;/&gt;&lt;/section&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;最后，整理一份大厂面试题，&lt;/span&gt;&lt;span&gt;原创，500多页，30 +万字&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;span&gt;给大家助把力，有需要的小伙伴，给 Tom 哥发暗号「大厂」&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/pre&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>a8615812dde79ab8e91cc03cad8a0a1b</guid>
<title>基于开源体系的云原生微服务治理实践与探索</title>
<link>https://toutiao.io/k/qz5wt4d</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;profile_inner&quot;&gt;
                  &lt;strong class=&quot;profile_nickname&quot;&gt;携程技术&lt;/strong&gt;
                  &lt;img class=&quot;profile_avatar&quot; id=&quot;js_profile_qrcode_img&quot; src=&quot;&quot; alt=&quot;&quot;/&gt;

                  &lt;p class=&quot;profile_meta&quot;&gt;
                  &lt;label class=&quot;profile_meta_label&quot;&gt;Weixin ID&lt;/label&gt;
                  &lt;span class=&quot;profile_meta_value&quot;&gt;ctriptech&lt;/span&gt;
                  &lt;/p&gt;

                  &lt;p class=&quot;profile_meta&quot;&gt;
                  &lt;label class=&quot;profile_meta_label&quot;&gt;About Feature&lt;/label&gt;
                  &lt;span class=&quot;profile_meta_value&quot;&gt;携程技术官方账号，分享交流成长。&lt;/span&gt;
                  &lt;/p&gt;
                &lt;/div&gt;
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>729bba603ed7f0a16aec8890ea105af8</guid>
<title>ChatGPT进化的秘密</title>
<link>https://toutiao.io/k/mocow7j</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content js_underline_content             defaultNoSetting&amp;#10;            &quot; id=&quot;js_content&quot;&gt;&lt;section&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;143742396004352000&quot; data-gallerysupplier=&quot;4&quot; data-ratio=&quot;0.459375&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/lBhAE42wKWoK5WHiaddOjFwmBuI6wJTUeQeenZ2bt974MGoQZcyy0mS1tGMia3ZT1sSdnvSu3A3X75bpBzreVic0g/640?wx_fmt=jpeg&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)&quot; data-style=&quot;max-width: 100%; background-color: rgb(255, 255, 255); letter-spacing: 0.544px; font-family: -apple-system-font, system-ui, &amp;quot;Helvetica Neue&amp;quot;, &amp;quot;PingFang SC&amp;quot;, &amp;quot;Hiragino Sans GB&amp;quot;, &amp;quot;Microsoft YaHei UI&amp;quot;, &amp;quot;Microsoft YaHei&amp;quot;, Arial, sans-serif; visibility: visible; box-sizing: border-box !important; overflow-wrap: break-word !important; color: rgb(163, 163, 163) !important;&quot; class=&quot;js_darkmode__0&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16339314364542=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16339314364542=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16339314364542=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16339314364542=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16339314364542=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16339314364542=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16339314364542=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16339314364542=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)&quot;&gt;&lt;section&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(41, 41, 41)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)|rgb(239, 239, 239)&quot; data-darkmode-color-16221004879619=&quot;rgb(168, 168, 168)&quot; data-darkmode-original-color-16221004879619=&quot;#fff|rgb(62, 62, 62)&quot; data-style=&quot;padding: 10px; max-width: 100%; background-color: rgb(239, 239, 239); color: rgb(62, 62, 62); line-height: 25.6px; display: inline-block; width: 670px; border-width: 2px; border-style: dashed; border-color: transparent; visibility: visible; box-sizing: border-box !important; overflow-wrap: break-word !important;&quot; class=&quot;js_darkmode__1&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(41, 41, 41)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)|rgb(239, 239, 239)&quot; data-darkmode-color-16339314364542=&quot;rgb(168, 168, 168)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)|rgb(62, 62, 62)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(41, 41, 41)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)|rgb(239, 239, 239)&quot; data-darkmode-color-16221004879619=&quot;rgb(168, 168, 168)&quot; data-darkmode-original-color-16221004879619=&quot;#fff|rgb(62, 62, 62)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(41, 41, 41)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)|rgb(239, 239, 239)&quot; data-darkmode-color-16339314364542=&quot;rgb(168, 168, 168)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)|rgb(62, 62, 62)&quot;&gt;&lt;section data-darkmode-bgcolor-16221004879619=&quot;rgb(41, 41, 41)&quot; data-darkmode-original-bgcolor-16221004879619=&quot;#fff|rgb(255, 255, 255)|rgb(239, 239, 239)&quot; data-darkmode-color-16221004879619=&quot;rgb(168, 168, 168)&quot; data-darkmode-original-color-16221004879619=&quot;#fff|rgb(62, 62, 62)&quot; data-darkmode-bgcolor-16339314364542=&quot;rgb(41, 41, 41)&quot; data-darkmode-original-bgcolor-16339314364542=&quot;#fff|rgb(255, 255, 255)|rgb(239, 239, 239)&quot; data-darkmode-color-16339314364542=&quot;rgb(168, 168, 168)&quot; data-darkmode-original-color-16339314364542=&quot;#fff|rgb(163, 163, 163)|rgb(62, 62, 62)&quot;&gt;&lt;section&gt;&lt;span/&gt;&lt;span&gt;本文作者，符尧 &lt;/span&gt;&lt;span&gt;yao.fu@ed.ac.uk&lt;/span&gt;&lt;span&gt;，爱丁堡大学 (University of Edinburgh) 博士生，本科毕业于北京大学，与彭昊，Tushar Khot 在艾伦人工智能研究院 (Allen Institute for AI) 共同完成英文原稿，与剑桥大学郭志江共同翻译为中文，感谢上海交通大学&lt;/span&gt;&lt;span&gt;何俊贤&lt;/span&gt;&lt;span&gt;，加州大学洛杉矶分校&lt;/span&gt;&lt;span&gt;鲁盼&lt;/span&gt;&lt;span&gt;，达特茅斯学院&lt;/span&gt;&lt;span&gt;刘睿博&lt;/span&gt;&lt;span&gt;对初稿的讨论与建议。感谢 &lt;/span&gt;&lt;span&gt;Raj Ammanabrolu&lt;/span&gt;&lt;span&gt; (Allen Institute for AI), &lt;/span&gt;&lt;span&gt;Peter Liu&lt;/span&gt;&lt;span&gt; (Google Brain), &lt;/span&gt;&lt;span&gt;Brendan Dolan-Gavitt&lt;/span&gt;&lt;span&gt; (New York University), &lt;/span&gt;&lt;span&gt;Denny Zhou&lt;/span&gt;&lt;span&gt; (Google Brain) 对终稿的讨论和建议，他们的建议极大程度上增加了本文的完整度。&lt;span&gt;英文版原文：&lt;/span&gt;&lt;span&gt;https://franxyao.github.io/blog.html&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最近，OpenAI的预训练模型ChatGPT给人工智能领域的研究人员留下了深刻的印象和启发。毫无疑问，它又强又聪明，且跟它说话很好玩，还会写代码。它在多个方面的能力远远超过了自然语言处理研究者们的预期。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;于是，我们自然就有一个问题：ChatGPT 是怎么变得这么强的？它的各种强大的能力到底从何而来？&lt;br/&gt;&lt;br/&gt;在这篇文章中，我们试图剖析 ChatGPT 的突现能力（Emergent Ability），追溯这些能力的来源，希望能够给出一个全面的技术路线图，来说明 GPT-3.5 模型系列以及相关的大型语言模型是如何一步步进化成目前的强大形态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们希望这篇文章能够促进大型语言模型的透明度，成为开源社区共同努力复现 GPT-3.5 的路线图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;致国内的同胞们：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;span/&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;在国际学术界看来，ChatGPT / GPT-3.5 是一种划时代的产物，它与之前常见的语言模型 (Bert/ Bart/T5) 的区别，几乎是导弹与弓箭的区别，一定要引起最高程度的重视。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;在我跟国际同行的交流中，国际上的主流学术机构 (如斯坦福大学，伯克利加州大学) 和主流业界研究院（如谷歌大脑，微软研究院）都已经全面拥抱大模型。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;在当前这个阶段，国内的技术水准、学术视野、治学理念和国际前沿的差距似乎并没有减少，反而正在扩大，如果现状持续下去，极有可能出现技术断代。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;此诚危急存亡之秋。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote class=&quot;js_blockquote_wrap&quot; data-type=&quot;2&quot; data-url=&quot;&quot; data-author-name=&quot;&quot; data-content-utf8-length=&quot;66&quot; data-source-title=&quot;&quot; data-text=&quot;多年以后，面对行刑队，奥雷里亚诺·布恩迪亚上校将会回想起父亲带他去见识冰块的那个遥远的下午。 —— 《百年孤独》 加西亚·马尔克斯&quot; data-editid=&quot;v493g65ck02kf4mneo&quot;&gt;&lt;section class=&quot;js_blockquote_digest&quot;&gt;&lt;p&gt;&lt;span&gt;多年以后，面对行刑队，奥雷里亚诺·布恩迪亚上校将会回想起父亲带他去见识冰块的那个遥远的下午。 —— 《百年孤独》 加西亚·马尔克斯&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2020版初代GPT-3与大规模预训练&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;初代GPT-3展示了三个重要能力：&lt;/span&gt;&lt;strong&gt;&lt;span/&gt;&lt;/strong&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;语言生成：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;遵循提示词（prompt），然后生成补全提示词的句子 (completion)。这也是今天人类与语言模型最普遍的交互方式。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;上下文学习 (in-context learning): &lt;/span&gt;&lt;/strong&gt;&lt;span&gt;遵循给定任务的几个示例，然后为新的测试用例生成解决方案。很重要的一点是，GPT-3虽然是个语言模型，但它的论文几乎没有谈到“语言建模” (language modeling) —— 作者将他们全部的写作精力都投入到了对上下文学习的愿景上，这才是 GPT-3的真正重点。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;世界知识 (world knowledge)：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;包括事实性知识 (factual knowledge) 和常识 (commonsense)。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;那么这些能力从何而来呢？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基本上，以上三种能力都来自于大规模预训练：&lt;/span&gt;&lt;span&gt;在有3000亿单词的语料上预训练拥有1750亿参数的模型（ 训练语料的60%来自于 2016 - 2019 的 C4 + 22% 来自于 WebText2 + 16% 来自于Books + 3%来自于Wikipedia）。&lt;/span&gt;&lt;span&gt;其中：&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;语言生成&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;的能力来自于语言建模的&lt;strong&gt;训练目标&lt;/strong&gt; (language modeling)。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;世界知识&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;来自 3000 亿单词的&lt;strong&gt;训练语料库&lt;/strong&gt;（不然还能是哪儿呢）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;模型的 1750 亿参数&lt;/strong&gt;是为了&lt;strong&gt;存储知识&lt;/strong&gt;，Liang et al. (2022) 的文章进一步证明了这一点。他们的结论是，知识密集型任务的性能与模型大小息息相关（&lt;span&gt;https://crfm.stanford.edu/helm/v1.0/?group=knowledge&lt;/span&gt;）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;上下文学习的能力来源及为什么上下文学习可以泛化，&lt;strong&gt;仍然难以溯源。&lt;/strong&gt;直觉上，这种能力可能来自于同一个任务的数据点在训练时按顺序排列在同一个 batch 中。然而，很少有人研究为什么语言模型预训练会促使上下文学习，以及为什么上下文学习的行为与微调 (fine-tuning) 如此不同。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;令人好奇的是，初代的&lt;strong&gt;GPT-3有多强&lt;/strong&gt;。 &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实比较难确定初代 GPT-3（在 OpenAI API 中被称为davinci）到底是“强”还是“弱”。&lt;/span&gt;&lt;span&gt;一方面，它合理地回应了某些特定的查询，并在许多数据集中达到了还不错的性能；&lt;/span&gt;&lt;span&gt;另一方面，它在许多任务上的&lt;/span&gt;&lt;strong&gt;表现还不如 T5 这样的小模型&lt;/strong&gt;&lt;span&gt;（参见其原始论文）。&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;在今天（2022 年 12 月）ChatGPT 的标准下，很难说初代的 GPT-3 是“智能的”。&lt;/span&gt;&lt;span&gt;Meta 开源的 OPT 模型试图复现初代 GPT-3，但它的能力与当今的标准也形成了尖锐的对比。&lt;/span&gt;&lt;span&gt;许多测试过 OPT 的人也认为与现在的text-davinci-002相比，该模型确实 “不咋地”。&lt;/span&gt;&lt;span&gt;尽管如此，OPT 可能是初代 GPT-3 的一个足够好的开源的近似模型了（根据 OPT 论文和斯坦福大学的 HELM 评估）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然初代的 GPT-3 可能表面上看起来很弱，但后来的实验证明，初代 GPT-3 有着非常强的潜力。这些潜力后来被代码训练、指令微调 (instruction tuning) 和基于人类反馈的强化学习 (reinforcement learning with human feedback, RLHF) 解锁，最终体展示出极为强大的突现能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;从2020版GPT-3到2022版ChatGPT &lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从最初的 GPT-3 开始，为了展示 OpenAI 是如何发展到ChatGPT的，我们看一下 GPT-3.5 的进化树：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.600925925925926&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1080&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/lBhAE42wKWoK5WHiaddOjFwmBuI6wJTUeyibW6JqyGLP1qs0WetQWCKwbWibCCCTSK1icZCRe3ntRmHcLx40jYuWGQ/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;2020 年 7 月&lt;/strong&gt;，OpenAI 发布了模型索引为的 davinci 的初代 GPT-3 论文（&lt;span&gt;Brown. et. al. 2020. Language Models are Few-Shot Learners）&lt;/span&gt;，从此它就开始不断进化。&lt;br/&gt;&lt;strong&gt;&lt;br/&gt;2021 年 7 月&lt;/strong&gt;，Codex 的论文（&lt;span&gt;Chen et. al. 2021. Evaluating Large Language Models Trained on Code）&lt;/span&gt;发布，其中初始的 Codex 是根据（可能是内部的）120 亿参数的 GPT-3 变体进行微调的。后来这个 120 亿参数的模型演变成 OpenAI API 中的code-cushman-001。在 2022 年 3 月，OpenAI 发布了指令微调 (instruction tuning)（&lt;span&gt;Ouyang et. al. 2022. Training language models to follow instructions with human feedback）&lt;/span&gt; 的论文，其监督微调 (supervised instruction tuning) 的部分对应了davinci-instruct-beta和text-davinci-001。&lt;br/&gt;&lt;strong&gt;&lt;br/&gt;2022 年 4 月至 7 月&lt;/strong&gt;，OpenAI 开始对code-davinci-002模型进行 Beta 测试，也称其为 Codex。然后code-davinci-002、text-davinci-003和ChatGPT 都是从code-davinci-002进行指令微调得到的。详细信息请参阅 OpenAI的模型索引文档（&lt;span&gt;https://beta.openai.com/docs/model-index-for-researchers&lt;/span&gt;）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;尽管 Codex 听着像是一个只管代码的模型，但code-davinci-002可能是最强大的针对&lt;strong&gt;自然语言&lt;/strong&gt;的GPT-3.5 变体（优于 text-davinci-002和 -003）。code-davinci-002很可能在文本和代码上都经过训练，然后根据指令进行调整（将在下面解释）。&lt;br/&gt;&lt;strong&gt;&lt;br/&gt;2022 年 5-6 月&lt;/strong&gt;发布的text-davinci-002是一个基于code-davinci-002的有监督指令微调 (supervised instruction tuned) 模型。在text-davinci-002上面进行&lt;strong&gt;指令微调&lt;/strong&gt;很可能&lt;strong&gt;降低&lt;/strong&gt;了模型的&lt;strong&gt;上下文学习能力&lt;/strong&gt;，但是&lt;strong&gt;增强&lt;/strong&gt;了模型的&lt;strong&gt;零样本能力&lt;/strong&gt;（将在下面解释）。&lt;br/&gt;&lt;br/&gt;然后是text-davinci-003和 ChatGPT，它们都在 &lt;strong&gt;2022 年 11 月&lt;/strong&gt;发布，是使用的基于人类反馈的强化学习的版本指令微调 (instruction tuning with reinforcement learning from human feedback) 模型的两种不同变体。text-davinci-003 恢复了（但仍然比code-davinci-002差）一些在text-davinci-002 中丢失的部分&lt;strong&gt;上下文学习能力&lt;/strong&gt;（大概是因为它在微调的时候混入了语言建模） 并进一步改进了零样本能力（得益于RLHF）。另一方面，ChatGPT 似乎&lt;strong&gt;牺牲了几乎所有的上下文学习的能力&lt;/strong&gt;来&lt;strong&gt;换取&lt;/strong&gt;建模对话历史的能力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;总的来说，在 2020 - 2021 年期间，在code-davinci-002之前，OpenAI 已经投入了大量的精力通过代码训练和指令微调来增强GPT-3。当他们完成code-davinci-002时，所有的能力都已经存在了。很可能后续的指令微调，无论是通过有监督的版本还是强化学习的版本，都会做以下事情（稍后会详细说明）：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;指令微调&lt;strong&gt;不会为模型注入新的能力&lt;/strong&gt; —— 所有的能力都已经存在了。指令微调的作用是&lt;strong&gt;解锁 / 激发这些能力&lt;/strong&gt;。这主要是因为指令微调的数据量比预训练数据量少几个数量级（基础的能力是通过预训练注入的）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;指令微调&lt;strong&gt;将 GPT-3.5 的分化到不同的技能树&lt;/strong&gt;。有些更擅长上下文学习，如text-davinci-003，有些更擅长对话，如ChatGPT。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;指令微调&lt;strong&gt;通过牺牲性能换取与人类的对齐（alignment）&lt;/strong&gt;。OpenAI 的作者在他们的指令微调论文（&lt;span&gt;Ouyang et. al. 2022. Training language models to follow instructions with human feedback）&lt;/span&gt;中称其为 “对齐税” (alignment tax)。许多论文（比如：&lt;span&gt;Suzgun et. al. 2022. Challenging BIG-Bench tasks and whether chain-of-thought can solve them/&lt;span&gt;Chung et. al. 2022. Scaling Instruction-Finetuned Language Models&lt;/span&gt;等）&lt;/span&gt;都报道了code-davinci-002在基准测试中实现了最佳性能（但模型不一定符合人类期望）。在code-davinci-002上进行指令微调后，模型可以生成更加符合人类期待的反馈（或者说模型与人类对齐），例如：零样本问答、生成安全和公正的对话回复、拒绝超出模型它知识范围的问题。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;Code-Davinci-002和Text-Davinci-002在代码上训练，在指令上微调&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在code-davinci-002和text-davinci-002之前，有两个中间模型，分别是 davinci-instruct-beta 和 text-davinci-001。两者在很多方面都比上述的两个-002模型差（例如，text-davinci-001 链式思维推理能力不强（&lt;span&gt;参见附录中的图8 &lt;/span&gt;&lt;span&gt;https://arxiv.org/pdf/2201.11903v1.pdf&lt;/span&gt;））。所以我们在本节中重点介绍 -002 型号。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3.1 复杂推理能力的来源和泛化到新任务的能力&lt;/span&gt;&lt;/strong&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们关注code-davinci-002和text-davinci-002，这两兄弟是第一版的 GPT3.5 模型，一个用于代码，另一个用于文本。它们表现出了三种重要能力与初代 GPT-3 不同的能力：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;响应人类指令：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;以前，GPT-3 的输出主要训练集中常见的句子。现在的模型会针对指令 / 提示词生成更合理的答案（而不是相关但无用的句子）。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;泛化到没有见过的任务：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;当用于调整模型的指令数量超过一定的规模时，模型就可以自动在从没见过的新指令上也能生成有效的回答。 这种能力对于上线部署至关重要，因为用户总会提新的问题，模型得答得出来才行。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;代码生成和代码理解：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这个能力很显然，因为模型用代码训练过。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;利用思维链 (chain-of-thought) 进行复杂推理：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;初代 GPT3 的模型思维链推理的能力很弱甚至没有。 &lt;strong&gt;code-davinci-002 和 text-davinci-002 是两个拥有足够强的思维链推理能力的模型。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;思维链推理之所以重要，是因为思维链可能是解锁突现能力和超越缩放法则 (scaling laws) 的关键。请参阅上一篇博文（&lt;span&gt;https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f）&lt;/span&gt;。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;这些能力从何而来？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;与之前的模型相比，两个主要区别是&lt;strong&gt;指令微调&lt;/strong&gt;和&lt;strong&gt;代码训练&lt;/strong&gt;。具体来说：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;能够&lt;strong&gt;响应人类指令&lt;/strong&gt;的能力是&lt;strong&gt;指令微调&lt;/strong&gt;的直接产物。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;对没有见过的指令做出反馈&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;的泛化能力是在指令数量超过一定程度之后&lt;strong&gt;自动出现的&lt;/strong&gt;，T0（&lt;span&gt;Sanh. et. al. Oct 2021. Multitask Prompted Training Enables Zero-Shot Task Generalization&lt;/span&gt;）、Flan（&lt;span&gt;Wei et. al. Sep 2021. Finetuned Language Models Are Zero-Shot Learners&lt;/span&gt;） 和 FlanPaLM（&lt;span&gt;Chung et. al. Oct 2022. Scaling Instruction-Finetuned Language Models）&lt;/span&gt;论文进一步证明了这一点。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;使用&lt;strong&gt;思维链&lt;/strong&gt;进行&lt;strong&gt;复杂推理&lt;/strong&gt;的能力很可能是&lt;strong&gt;代码训练&lt;/strong&gt;的&lt;strong&gt;一个神奇的副产物&lt;/strong&gt;。对此，我们有以下的事实作为一些支持：&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;最初的 GPT-3 没有接受过代码训练，它不能做&lt;strong&gt;思维链&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;text-davinci-001 模型，虽然经过了指令微调，但第一版思维链论文（&lt;span&gt;第一个版本(&lt;/span&gt;&lt;span&gt;https://arxiv.org/pdf/2201.11903v1.pdf&lt;/span&gt;&lt;span&gt;) 报告了davinci在GSM8K上的准确率 12.4 v.s. 第五个版本 (&lt;/span&gt;&lt;span&gt;https://arxiv.org/pdf/2201.11903v5.pdf&lt;/span&gt;&lt;span&gt;) 报告了 code-davinci-002 准确率为 63.1）&lt;/span&gt;报告说，它的它思维链推理的能力非常弱 —— &lt;strong&gt;所以指令微调可能不是思维链存在的原因，代码训练才是模型能做思维链推理的最可能原因。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;PaLM（&lt;span&gt;Chowdhery et. al. Apr. 2022. PaLM: Scaling Language Modeling with Pathways）&lt;/span&gt; 有 5% 的代码训练数据，可以做思维链。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Codex论文（&lt;span&gt;Chen et. al. Jul 2021. Evaluating Large Language Models Trained on Code&lt;/span&gt;）中的代码数据量为 159G ，大约是初代 GPT-3 5700 亿训练数据的28%。code-davinci-002 及其后续变体可以做思维链推理。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;在 HELM 测试中，Liang et al. (2022) （&lt;span&gt;Liang et. al. Nov 2022. Holistic Evaluation of Language Models）&lt;/span&gt;对不同模型进行了大规模评估。他们发现了针对代码训练的模型具有很强的语言推理能力，包括 120亿参数的code-cushman-001.。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;我们在 AI2 的工作（&lt;span&gt;Fu et. al. Oct 2022. Complexity-based Prompting for Multi-Step Reasoning. &lt;/span&gt;&lt;span&gt;https://openreview.net/forum?id=yf1icZHC-l9&lt;/span&gt;）也表明，当配备复杂的思维链时，code-davinci-002 在 GSM8K 等重要数学基准上是目前表现最好的模型&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;直觉来说，&lt;strong&gt;面向过程的编程 (procedure-oriented programming) &lt;/strong&gt;跟人类&lt;strong&gt;逐步解决任务&lt;/strong&gt;的过程很类似，&lt;strong&gt;面向对象编程 (object-oriented programming)&lt;/strong&gt; 跟人类&lt;strong&gt;将复杂任务分解为多个简单任务&lt;/strong&gt;的过程很类似&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span/&gt;&lt;span&gt;以上所有观察结果都是代码与推理能力 / 思维链之间的相关性。代码和推理能力 / 思维链之间的这种相关性对研究社区来说是一个非常有趣的问题，但目前仍未得到很好的理解。然而，&lt;/span&gt;&lt;strong&gt;仍然没有确凿的证据表明代码训练就是CoT和复杂推理的原因&lt;/strong&gt;&lt;span&gt;。 思维链的来源仍然是一个开放性的研究问题。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;另外还要注意一些细节差异：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;h2&gt;&lt;strong&gt;&lt;span&gt;3.2 这些能力是在预训练之后已经存在还是在之后通过微调注入？&lt;/span&gt;&lt;/strong&gt;&lt;span/&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在这个阶段，我们已经确定了指令微调和代码训练的关键作用。一个重要的问题是如何进一步分析代码训练和指令微调的影响？具体来说：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上述三种能力&lt;strong&gt;是否已经存在于初代的GPT-3&lt;/strong&gt;中，只是&lt;strong&gt;通过指令和代码训练触发 / 解锁？&lt;/strong&gt;或者这些能力在初代的 GPT-3 中&lt;strong&gt;并不存在&lt;/strong&gt;，是通过指令和代码训练&lt;strong&gt;注入&lt;/strong&gt;？ &lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果答案已经在初代的 GPT-3 中，&lt;strong&gt;那么这些能力也应该在 OPT 中。因此，要复现这些能力，或许可以直接通过指令和代码调整 OPT&lt;/strong&gt;。但是，code-davinci-002 也可能不是基于最初的 GPT-3 davinci，而是基于比初代 GPT-3 更大的模型。如果是这种情况，可能就没办法通过调整 OPT 来复现了。研究社区需要进一步弄清楚 OpenAI 训练了什么样的模型作为 code-davinci-002 的基础模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们有以下的假设和证据：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;span/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;4&lt;/span&gt;&lt;/strong&gt;&lt;/h1&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;text-davinci-003和ChatGPT，基于人类反馈的强化学习(Reinforcement Learning from Human Feedback, RLHF) 的威力&lt;/span&gt;&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在当前阶段（2022 年 12 月）， text-davinci-002、text-davinci-003 和 ChatGPT&lt;strong&gt;之间几乎没有严格的统计上的比较&lt;/strong&gt;，主要是因为：&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;所以在这些模型之间的比较更多是&lt;strong&gt;基于研究社区的集体经验&lt;/strong&gt; （统计上不是很严格）。不过，我们相信初步的描述性比较仍然可以揭示模型的机制。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们首先注意到以下 text-davinci-002，text-davinci-003 和 ChatGPT 之间的比较：&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;这意味着大多数新模型的行为都是 RLHF 的产物&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;那么让我们看看 RLHF 触发的能力：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;翔实的回应&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt; text-davinci-003 的生成通常比text-davinci-002长（&lt;span&gt;https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-text-davinci-003-differ&lt;/span&gt;）。ChatGPT 的回应则更加冗长，以至于用户必须明确要求“用一句话回答我”，才能得到更加简洁的回答。这是 RLHF 的直接产物。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;公正的回应&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;ChatGPT 通常对涉及多个实体利益的事件（例如政治事件）给出非常平衡的回答。这也是RLHF的产物。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;拒绝不当问题&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;这是内容过滤器和由 RLHF 触发的模型自身能力的结合，过滤器过滤掉一部分，然后模型再拒绝一部分。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;拒绝其知识范围之外的问题：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;例如，拒绝在2021 年 6 月之后发生的新事件（因为它没在这之后的数据上训练过）。这是 RLHF 最神奇的部分，因为它使模型能够隐式地区分哪些问题在其知识范围内，哪些问题不在其知识范围内。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;有两件事情值得注意：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;所有的能力都是模型本来就有的，&lt;strong&gt; 而不是通过RLHF 注入的&lt;/strong&gt;。RLHF 的作用是&lt;strong&gt;触发 / 解锁突现能力&lt;/strong&gt;。这个论点主要来自于数据量大小的比较：因为与预训练的数据量相比，RLHF 占用的计算量 / 数据量要少得多。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;模型&lt;strong&gt;知道它不知道什么不是通过编写规则来实现的&lt;/strong&gt;， 而是通过RLHF解锁的。这是一个非常令人惊讶的发现，因为 RLHF 的最初目标是让模型生成复合人类期望的回答，这更多是让模型生成安全的句子，而不是让模型知道它不知道的内容。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;幕后发生的事情可能是：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;ChatGPT: 通过&lt;strong&gt;牺牲上下文学习&lt;/strong&gt;的能力&lt;strong&gt;换取建模对话历史&lt;/strong&gt;的能力。这是一个基于经验的观测结果，因为 ChatGPT 似乎不像 text-davinci-003 那样受到上下文演示的强烈影响。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;text-davinci-003：&lt;strong&gt;恢复了&lt;/strong&gt; text-davinci-002 所牺牲的&lt;strong&gt;上下文学习能力&lt;/strong&gt;（&lt;span&gt;见&lt;/span&gt;&lt;span&gt;https://arxiv.org/pdf/2210.11416.pdf&lt;/span&gt;&lt;span/&gt;&lt;span&gt;的附录D&lt;/span&gt;&lt;span&gt;）&lt;/span&gt;，&lt;strong&gt; 提高零样本的能力（&lt;span/&gt;&lt;/strong&gt;&lt;span&gt;https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-text-davinci-003-differ&lt;/span&gt;/ &lt;span&gt;https://scale.com/blog/gpt-3-davinci-003-comparison&lt;/span&gt;&lt;strong&gt;&lt;span/&gt;）。&lt;/strong&gt; 根据instructGPT的论文，这是来自于强化学习调整阶段混入了语言建模的目标（而不是 RLHF 本身）。&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;5&lt;/span&gt;&lt;/strong&gt;&lt;/h1&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;总结当前阶段 GPT-3.5 的进化历程&lt;/span&gt;&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;到目前为止，我们已经仔细检查了沿着进化树出现的所有能力，下表总结了演化路径：&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;&quot; data-ratio=&quot;1.1925925925925926&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/lBhAE42wKWoK5WHiaddOjFwmBuI6wJTUess1z0NCYvTY1JbYQicribzFxTN3IcmMicRCTBRiaqsQ3494DH0udXCibtcA/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以得出结论：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;语言生成能力 + 基础世界知识 + 上下文学习都是来自于预训练（davinci）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;存储大量知识的能力来自 1750 亿的参数量。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;遵循指令和泛化到新任务的能力来自于扩大指令学习中指令的数量（Davinci-instruct-beta)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;执行复杂推理的能力很可能来自于代码训练（code-davinci-002）&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;生成中立、客观的能力、安全和翔实的答案来自与人类的对齐。具体来说：&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;如果是监督学习版，得到的模型是text-davinci-002&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;如果是强化学习版 (RLHF) ，得到的模型是text-davinci-003&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;无论是有监督还是 RLHF ，模型在很多任务的性能都无法超过 code-davinci-002 ，这种因为对齐而造成性能衰退的现象叫做对齐税。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;对话能力也来自于 RLHF（ChatGPT），具体来说它牺牲了上下文学习的能力，来换取：&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;建模对话历史&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;增加对话信息量&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;拒绝模型知识范围之外的问题&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;6&lt;/span&gt;&lt;/strong&gt;&lt;/h1&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;GPT-3.5 目前不能做什么&lt;/span&gt;&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;虽然GPT-3.5是自然语言处理研究中的重要一步，但它并没有完全包含许多研究人员（包括 AI2）设想的所有理想属性。以下是GPT-3.5不具备的某些重要属性：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;实时改写模型的信念：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;当模型表达对某事的信念时，如果该信念是错误的，我们可能很难纠正它：&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;我最近遇到的一个例子是：ChatGPT 坚持认为 3599 是一个质数，尽管它承认 3599 = 59 * 61。另外，请参阅Reddit上关于游得最快的海洋哺乳动物（&lt;/span&gt;&lt;span&gt;https://www.reddit.com/r/ChatGPT/comments/zd7l8t/nice/&lt;/span&gt;&lt;span&gt;）的例子。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;然而，模型信念的强度似乎存在不同的层次。一个例子是即使我告诉它达斯·维达（星球大战电影中的人物）赢得了2020年大选，模型依旧会认为美国现任总统是拜登。但是如果我将选举年份改为 2024 年，它就会认为总统是达斯·维达是 2026 年的总统。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;形式推理：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;GPT-3.5系列不能在数学或一阶逻辑等形式严格的系统中进行推理：&lt;br/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;在自然语言处理的文献中， “推理” 一词的定义很多时候不太明确。但如果我们从模糊性的角度来看，例如一些问题 (a) 非常模棱两可，没有推理；(b) 有点儿逻辑在里面，但有些地方也可以模糊；(c) 非常严谨，不能有任何歧义。那么，&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;模型可以很好地进行 (b) 类的带模糊性的推理，例子有：&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;生成如何做豆腐脑的方法。做豆腐脑的时候，中间很多步骤模糊一点是可以接受的，比如到底是做咸的还是做甜的。只要整体步骤大致正确，做出来的豆腐脑儿就能吃。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;数学定理的证明思路。证明思路是用语言表达的非正式的逐步解法，其中每一步的严格推导可以不用太具体。证明思路经常被用到数学教学：只要老师给一个大致正确的整体步骤，学生就可以大概明白。然后老师把具体的证明细节作为作业布置给学生，答案略。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;GPT-3.5 不能进行类型 (c) 的推理（推理不能容忍歧义）。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;一个例子是严格的数学证明，要求中间步骤中不能跳，不能模糊，不能错。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;但这种严格推理到底是应该让语言模型做还是让符号系统做还有待讨论。一个例子是，与其努力让 GPT 做三位数加法，不如直接调 Python。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;从互联网进行检索：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;GPT-3.5 系列（暂时）不能直接搜索互联网&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;但是有一篇 WebGPT（&lt;span&gt;https://openai.com/blog/webgpt/&lt;/span&gt;） 论文发表于2021年12月，里面就让 GPT 调用了搜索引擎。所以检索的能力已经在 OpenAI 内部进行了测试。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;这里需要区分的一点是，GPT-3.5 的两个重要但不同的能力是&lt;strong&gt;知识&lt;/strong&gt;和&lt;strong&gt;推理&lt;/strong&gt;。一般来说，如果我们能够&lt;strong&gt;将知识部分卸载到外部的检索系统，让语言模型只专注于推理，这就很不错了&lt;/strong&gt;。 因为：&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;模型的内部知识总是在某个时间被切断。模型始终需要最新的知识来回答最新的问题。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;回想一下，我们已经讨论过 1750 亿的参数大量用于存储知识。如果我们可以将知识卸载到模型之外，那么模型参数可能会大大减少，最终它甚至可以在手机上运行（疯狂的想法，但 ChatGPT 已经足够科幻了，谁知道未来会怎样呢).&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;h1/&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;7&lt;/span&gt;&lt;/strong&gt;&lt;/h1&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;结论&lt;/span&gt;&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;span&gt;在这篇博文中，我们仔细介绍了GPT-3.5系列的能力范围，并追溯了它们所有突现能力的来源。初代GPT-3模型通过预训练获得生成能力、世界知识和in-context learning。然后通过instruction tuning的模型分支获得了遵循指令和能泛化到没有见过的任务的能力。经过代码训练的分支模型则获得了代码理解的能力，作为代码训练的副产品，模型同时潜在地获得了复杂推理的能力。&lt;br/&gt;&lt;br/&gt;结合这两个分支，code-davinci-002似乎是具有所有强大能力的最强GPT-3.5模型。接下来通过有监督的instruction tuning和 RLHF通过牺牲模型能力换取与人类对齐，即对齐税。RLHF 使模型能够生成更翔实和公正的答案，同时拒绝其知识范围之外的问题。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;我们希望这篇文章能够帮助提供一个清晰的GPT评估图，并引发一些关于语言模型、instruction tuning和code tuning的讨论。最重要的是， &lt;strong&gt;我们希望这篇文章可以作为在开源社区内复现GPT-3.5的路线图。&lt;br/&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;blockquote class=&quot;js_blockquote_wrap&quot; data-type=&quot;2&quot; data-url=&quot;&quot; data-author-name=&quot;&quot; data-content-utf8-length=&quot;29&quot; data-source-title=&quot;&quot; data-text=&quot;“因为山就在那里。”——乔治·马洛里，珠穆朗玛峰探险先驱&quot; data-editid=&quot;25vp0dpdt383tdxslc&quot;&gt;&lt;section class=&quot;js_blockquote_digest&quot;&gt;&lt;section&gt;&lt;span&gt;“因为山就在那里。”——乔治·马洛里，珠穆朗玛峰探险先驱&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/blockquote&gt;&lt;h1&gt;&lt;br/&gt;&lt;/h1&gt;&lt;h1/&gt;&lt;h1&gt;&lt;strong&gt;&lt;span&gt;常见问题&lt;/span&gt;&lt;/strong&gt;&lt;span/&gt;&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;（作者注：转发请在文章的开头标明出处，而不是在结尾列一行小字。）&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;br/&gt;其他人都在看&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;h2&gt;&lt;span data-style=&quot;margin: 0px; padding: 0px; outline: 0px; max-width: 100%; font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &amp;quot;Helvetica Neue&amp;quot;, &amp;quot;PingFang SC&amp;quot;, &amp;quot;Hiragino Sans GB&amp;quot;, &amp;quot;Microsoft YaHei UI&amp;quot;, &amp;quot;Microsoft YaHei&amp;quot;, Arial, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(136, 136, 136); font-size: 14px; letter-spacing: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;&quot; class=&quot;js_darkmode__201&quot;&gt;点击“&lt;/span&gt;&lt;strong data-style=&quot;margin: 0px; padding: 0px; outline: 0px; max-width: 100%; color: rgb(51, 51, 51); font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &amp;quot;Helvetica Neue&amp;quot;, &amp;quot;PingFang SC&amp;quot;, &amp;quot;Hiragino Sans GB&amp;quot;, &amp;quot;Microsoft YaHei UI&amp;quot;, &amp;quot;Microsoft YaHei&amp;quot;, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; box-sizing: border-box !important; overflow-wrap: break-word !important;&quot; class=&quot;js_darkmode__202&quot;&gt;&lt;span&gt;阅读原文&lt;/span&gt;&lt;/strong&gt;&lt;span data-style=&quot;margin: 0px; padding: 0px; outline: 0px; max-width: 100%; color: rgb(51, 51, 51); font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &amp;quot;Helvetica Neue&amp;quot;, &amp;quot;PingFang SC&amp;quot;, &amp;quot;Hiragino Sans GB&amp;quot;, &amp;quot;Microsoft YaHei UI&amp;quot;, &amp;quot;Microsoft YaHei&amp;quot;, Arial, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; letter-spacing: 1px; font-size: 14px; box-sizing: border-box !important; overflow-wrap: break-word !important;&quot; class=&quot;js_darkmode__203&quot;&gt;”&lt;/span&gt;&lt;span data-style=&quot;margin: 0px; padding: 0px; outline: 0px; max-width: 100%; font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, &amp;quot;Helvetica Neue&amp;quot;, &amp;quot;PingFang SC&amp;quot;, &amp;quot;Hiragino Sans GB&amp;quot;, &amp;quot;Microsoft YaHei UI&amp;quot;, &amp;quot;Microsoft YaHei&amp;quot;, Arial, sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; color: rgb(136, 136, 136); font-size: 14px; letter-spacing: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;&quot; class=&quot;js_darkmode__204&quot;&gt;，欢迎Star、试用OneFlow最新版本&lt;/span&gt;&lt;hr/&gt;&lt;/h2&gt;&lt;/section&gt;&lt;section&gt;&lt;mp-common-profile class=&quot;custom_select_card mp_profile_iframe&quot; data-pluginname=&quot;mpprofile&quot; data-id=&quot;MzU5ODY2MTk3Nw==&quot; data-headimg=&quot;http://mmbiz.qpic.cn/mmbiz_png/lBhAE42wKWomZrXDbZnegyP5qVxvr0Bg52ibDia1lqPAPvhiboIbuDsvznywQcExLn2dq6mfaE2hVBQ8cgZh8meMA/0?wx_fmt=png&quot; data-nickname=&quot;OneFlow&quot; data-alias=&quot;OneFlowTechnology&quot; data-signature=&quot;不止于成为世界上最快的开源深度学习框架&quot; data-from=&quot;2&quot; data-weuitheme=&quot;light&quot; data-weui-theme=&quot;light&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;333&quot; data-backw=&quot;562&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.5920529801324503&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;755&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/lBhAE42wKWqMs1w4gByiaaU9WpaHh8lXhZYibicCXtxbLpohuxOJUvvVYuaBq3VGOBrOn7soGUPPnzCTCdibPvkAQA/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;​​&lt;/p&gt;&lt;p&gt;&lt;mp-style-type data-value=&quot;10000&quot;/&gt;&lt;/p&gt;&lt;/div&gt;

          
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>