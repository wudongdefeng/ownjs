<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>f0b2518aaa223c61d9efbdf9c385707e</guid>
<title>7000 字讲清楚应用网关和负载均衡的技术原理并搭建一个支持 50000 QPS的真实架构</title>
<link>https://toutiao.io/k/dt572y9</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content js_underline_content             autoTypeSetting24psection&amp;#10;            &quot; id=&quot;js_content&quot;&gt;&lt;p&gt;&lt;span&gt;本文共 7000 字，阅读大约需要 23 分钟。&lt;/span&gt;&lt;/p&gt;&lt;hr/&gt;&lt;section&gt;上一篇文章的末尾，我们提到了一个假想出来的五万 QPS 的系统，以及这种规模的系统架构中必然存在的负载均衡器，那本篇文章我们就来一起利用负载均衡搭建一个能够支撑五万 QPS 的系统。&lt;/section&gt;&lt;h2&gt;“监听 HTTPS 443 端口的进程”这个单点&lt;/h2&gt;&lt;p&gt;之前，我们拆出了“监听 HTTPS 443 端口的进程”这个单点，并用 kong 网关来承载了这个单点。目前，在 2 vCore 的虚拟机上，2000 QPS 的压力对应的大约是 20% 的 CPU 占用率，经过换算我们可以知道：假如 kong 的性能可以随着核心数增加而线性提升的话，在维持最大 40% CPU 占用率的情况下，需要：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;(50000 / 2000) &lt;em&gt;2&lt;/em&gt; (20% / 40%) = 25 核&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在这里我们先假设我们搞了一个 25 核的虚拟机，接住了这五万 QPS（实际上接不住，我们后面会说），那这个 Kong 网关到底是什么玩意儿呢？它就是标题中的“应用网关”。&lt;/p&gt;&lt;h3&gt;应用网关&lt;/h3&gt;&lt;p&gt;应用网关，又称 API 网关，顾名思义，它就是所有 API 请求的大门：自己接下所有的 HTTP/HTTPS/TCP 请求，再将请求转发给真正的上游服务器。而这些上游服务器可能是一堆虚拟机，也可能是一堆容器，甚至可以是多个数据中心各自的应用网关。由于应用网关做的事情非常少，所以它能支撑很高 QPS 的系统。&lt;/p&gt;&lt;p&gt;常见的应用网关软件有 HAProxy、Nginx、Envoy 等，而 Cisco、Juniper、F5 等一体化设备厂商也有相关的硬件产品。&lt;/p&gt;&lt;p&gt;应用网关除了提升系统容量外，还有很多别的优势。&lt;/p&gt;&lt;h4&gt;1. 解放后端架构&lt;/h4&gt;&lt;p&gt;经过对应用网关两年的使用，我现在认为所有系统都应该放在应用网关的背后，包括开发环境。&lt;/p&gt;&lt;p&gt;应用网关对后端架构的解放作用实在是太大了，可以让你在后端玩出花来：各种语言、各种技术、各种部署形式、甚至全国各地的机房都可以成为某条 URL 的最终真实服务方，让你的后端架构彻底起飞。&lt;/p&gt;&lt;h4&gt;2. TLS 卸载&lt;/h4&gt;&lt;p&gt;终端用户访问应用网关的时候采用的是 HTTPS 协议，这个协议是需要对数据进行加密解密的，应用网关非常适合干这件事情，而背后的业务系统只提供标准 HTTP 协议即可，降低了业务系统的部署复杂度和资源消耗。&lt;/p&gt;&lt;h4&gt;3. 身份验证和安全性提升&lt;/h4&gt;&lt;p&gt;应用网关可以对后端异构系统进行统一的身份验证，无需一个一个单独实现。也可以统一防火墙白名单，后端系统防火墙只对网关 ip 开放，极大提升了后端系统的安全性，降低了海量服务器安全管理的难度。甚至可以针对某条 API 进行单独鉴权，让系统的安全管控能力大幅提升。&lt;/p&gt;&lt;h4&gt;4. 指标和数据收集&lt;/h4&gt;&lt;p&gt;由于所有流量都会经过网关，所以对指标进行收集也变的简单了，你甚至可以将双向流量的内容全部记录下来，用于数据统计和安全分析。&lt;/p&gt;&lt;h4&gt;5. 数据压缩与转换&lt;/h4&gt;&lt;p&gt;应用网关还可以统一对流量进行 gzip 压缩，可以将所有业务一次性升级到 HTTP/2 和 HTTP/3，可以对数据进行格式转换（XML 到 JSON）和修改（增加/修改/删除字段），总是就是能各种上下其手，翻云覆雨，随心所欲。&lt;/p&gt;&lt;h3&gt;负载均衡&lt;/h3&gt;&lt;p&gt;应用网关的另一个价值就是负载均衡了：可以将请求的流量按照各种比例分发给不同的后端服务器，提升系统容量；可以做红蓝发布和金丝雀发布；可以针对流量特点做灰度发布；可以主动调节各个后端服务器的压力；屏蔽失效的后端服务器等等。&lt;/p&gt;&lt;h4&gt;低负载下应用网关和负载均衡可以是同一个软件&lt;/h4&gt;&lt;p&gt;虽然应用网关和负载均衡是两个不同的概念，但在低负载系统里，他们两个往往由同一个软件来扮演，例如前面说到的 Kong 网关就同时具备这两个功能。&lt;/p&gt;&lt;h2&gt;拆分应用网关&lt;/h2&gt;&lt;p&gt;一个五万 QPS 的系统，是无法使用 25 核的单机安装 Kong 网关来承载的，因为此时单机 TCP 连接数已经达到了十万以上，在这个条件下强如 Nginx 也达到性能极限了，性能不再增长甚至会开始下降，用户体验也会迅速变差。此时，我们需要对应用网关进行拆分。&lt;/p&gt;&lt;h3&gt;应用网关怎么拆&lt;/h3&gt;&lt;p&gt;逻辑上，应用网关执行的是“反向代理+数据过滤”任务，并没有要求应用网关只能由一台服务器来承接，换句话说，应用网关&lt;code&gt;不是单点&lt;/code&gt;，只要多个节点的行为一致，那就可以共同承接这五万 QPS 的真实用户流量。&lt;/p&gt;&lt;p&gt;我们只需要在多台机器上装上同样版本的应用网关软件，然后在他们之间同步配置文件即可。Kong 采用的策略是让多个实例连接同一个&lt;code&gt;PostgreSQL&lt;/code&gt;数据库，每五秒从数据库获取一次最新的配置，如果数据库挂掉，那就保持内存中的现有配置继续运行。&lt;/p&gt;&lt;p&gt;Kong 集群追求的是“最终一致性”，不追求五秒的得失，反而让系统格外地容易扩展，格外的健壮，最后一篇文章我们还会见到使用类似思维的“DNS 分布式拆分”。这个朴素的分布式架构颇有毛子暴力美学的风范，后面我们讨论列存储 clickhouse 的时候还能见到。&lt;/p&gt;&lt;p&gt;如果单个应用网关扛不住五万 QPS，那我们搞一个负载均衡器放在应用网关的前面，架构图如下：&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.8650306748466258&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA3ENoeFVCSfeyaSoUibPZYLYSwKvVf2YNWyRL868SmEFxamMQFjia1gtFb5KN3cuBt7Z33mwUZMV3UA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1304&quot;/&gt;&lt;/p&gt;&lt;h2&gt;分层的网络&lt;/h2&gt;&lt;h3&gt;负载均衡器为何能抗住五万 QPS&lt;/h3&gt;&lt;p&gt;看到这里有人可能会疑惑，既然单机的 Nginx 都顶不住五万 QPS 带来的 TCP 资源开销，那负载均衡器如何抗住呢？因为负载均衡器承载的是比 Nginx 所承载的 TCP 更下面一层的协议：IP 协议。&lt;/p&gt;&lt;p&gt;至此，我们正式进入了网络拆分之路，这条路很难走，但收益也会很大，最终我们将得到一个 200Gbps 带宽的&lt;code&gt;软件定义负载均衡集群&lt;/code&gt;，让我们正式开始。&lt;/p&gt;&lt;h3&gt;网络是分层的&lt;/h3&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.3932702418506835&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA3ENoeFVCSfeyaSoUibPZYLYwM5J4u0tvW3oWeDcq9hiaF97E0rjnGnwoLvYlr8vCroTOIJYuIc9fJw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1902&quot;/&gt;&lt;/p&gt;&lt;center&gt;经典 TCP/IP 四层网络协议的首部&lt;/center&gt;&lt;p&gt;上面这张图引用自我的另一个系列文章：软件工程师需要了解的网络知识：从铜线到HTTP（三）—— TCP/IP¹。&lt;/p&gt;&lt;p&gt;如果你查看过网页的源代码，你就能知道网页背后是一段 HTML 代码，这段代码是被&lt;code&gt;层层包裹&lt;/code&gt;之后，再在网络中传输的，就像上图中一样。以太网之所以拥有如此之强的扩展性和兼容能力，就是因为它的“分层特性”：每一层都有专门的硬件设备来对网络进行扩展，最终组成了这个容纳全球数十亿台网络设备的“互联网”。最近，这些传统硬件设备的工作越来越多地被软件所定义，即&lt;code&gt;软件定义网络&lt;/code&gt;(SDN)。&lt;/p&gt;&lt;h3&gt;应用数据是什么&lt;/h3&gt;&lt;p&gt;应用数据就是网页背后的 HTTP 协议所包含的全部数据。&lt;/p&gt;&lt;p&gt;我们使用 Charles 反向代理软件可以轻易地得到 HTTP 协议的细节。下面我们展示一个普通的 GET 例子。使用浏览器访问 http://httpbin.org （自己尝试的时候不要选择 HTTPS 网站）：&lt;/p&gt;&lt;h4&gt;请求内容&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;GET / HTTP/1.1&lt;br/&gt;Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&lt;br/&gt;Accept-Encoding: gzip, deflate&lt;br/&gt;Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6&lt;br/&gt;Cache-Control: max-age=0&lt;br/&gt;Connection: keep-alive&lt;br/&gt;Host: httpbin.org&lt;br/&gt;Upgrade-Insecure-Requests: 1&lt;br/&gt;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36 Edg/108.0.1462.54&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;数据解释：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;第一行有三个元素：HTTP 方法、uri、HTTP 版本&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;之后的每一行均以冒号&lt;code&gt;:&lt;/code&gt;作为间隔符，左边是 key，右边是 value&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;HTTP 协议中，换行采用的不是 Linux 系统的 \n，而是跟 Windows 一样的 \r\n&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;响应内容&lt;/h4&gt;&lt;pre&gt;&lt;code&gt;HTTP/1.1 200 OK&lt;br/&gt;Date: Wed, 04 Jan 2023 12:07:36 GMT&lt;br/&gt;Content-Type: text/html; charset=utf-8&lt;br/&gt;Content-Length: 9593&lt;br/&gt;Connection: keep-alive&lt;br/&gt;Server: gunicorn/19.9.0&lt;br/&gt;Access-Control-Allow-Origin: *&lt;br/&gt;Access-Control-Allow-Credentials: true&lt;br/&gt;&lt;br/&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;br/&gt;&amp;lt;html lang=&quot;en&quot;&amp;gt;&lt;br/&gt;&lt;br/&gt;&amp;lt;head&amp;gt;&lt;br/&gt;    &amp;lt;meta charset=&quot;UTF-8&quot;&amp;gt;&lt;br/&gt;    &amp;lt;title&amp;gt;httpbin.org&amp;lt;/title&amp;gt;&lt;br/&gt;    &amp;lt;link href=&quot;https://fonts.googleapis.com/css?family=Open+Sans:400,700|Source+Code+Pro:300,600|Titillium+Web:400,600,700&quot;&lt;br/&gt;        rel=&quot;stylesheet&quot;&amp;gt;&lt;br/&gt;    &amp;lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/flasgger_static/swagger-ui.css&quot;&amp;gt;&lt;br/&gt;    &amp;lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; href=&quot;/static/favicon.ico&quot; sizes=&quot;64x64 32x32 16x16&quot; /&amp;gt;&lt;br/&gt;&amp;lt;/head&amp;gt;&lt;br/&gt;&lt;br/&gt;&amp;lt;body&amp;gt;&lt;br/&gt;    &amp;lt;a href=&quot;https://github.com/requests/httpbin&quot; class=&quot;github-corner&quot; aria-label=&quot;View source on Github&quot;&amp;gt;&lt;br/&gt;    &amp;lt;/a&amp;gt;&lt;br/&gt;    ... ... 此处省略一万个字&lt;br/&gt;&amp;lt;/div&amp;gt;&lt;br/&gt;&amp;lt;/body&amp;gt;&lt;br/&gt;&lt;br/&gt;&amp;lt;/html&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;响应数据的基本规则和请求一样，第一行的三个元素分别是 协议版本、状态码、状态码的简短解释。唯一的不同是，返回值里面还有 HTTP body。&lt;/p&gt;&lt;h5&gt;HTTP header 和 HTTP body&lt;/h5&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;两个换行即 \r\n\r\n 之前的内容为 HTTP header&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;两个换行之后的内容为 HTTP body&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;HTTP body 就是你在浏览器“查看源代码”所看到的内容&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;HTTP 下面是 TCP 层&lt;/h3&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.6573529411764706&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA3ENoeFVCSfeyaSoUibPZYLYm94vcoJnudG9DAov3BptbRSV7egY07mpyyvGKBmzkbbPicG2ljia7icXg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;680&quot;/&gt;&lt;/p&gt;&lt;center&gt;TCP 首部图示&lt;/center&gt;&lt;h4&gt;TCP 首部重要数据描述&lt;/h4&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;TCP 首部中最重要的数据是&lt;code&gt;源端口&lt;/code&gt;和&lt;code&gt;目的端口&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;他们各由 16 位二进制数组成，2^16 = 65536，所以网络端口的范围是 0-65535&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们可以注意到，目的端口号这个重要数据是放在 TCP 首部的，和更下层的 IP 首部、以太网帧首部毫无关系&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;TCP 下面是 IP 层&lt;/h3&gt;&lt;p&gt;全球所有公网 IPv4 组成了一个大型网络，这个 IP 网络其实就是&lt;code&gt;互联网&lt;/code&gt;的本体。（IPv6 比较复杂，本文再次不做详细讨论，以下示例均基于 IPv4）&lt;/p&gt;&lt;p&gt;在 IP 层中，每台设备都有一个 ip 地址，形如&lt;code&gt;123.123.123.123&lt;/code&gt;：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;IPv4 地址范围为 0.0.0.0 - 255.255.255.255&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;255 为 2 的 8 次方减一，也就是说用八位二进制可以表示 0-255&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;四个八位即为 32 位，4 个字节&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;IP 首部有哪些信息&lt;/h3&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5528846153846154&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA3ENoeFVCSfeyaSoUibPZYLYicAc4IbskwTZvLbeGwEuvQAFOkA9GURaUOGELTuPBhO897VSCaR1dQA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;624&quot;/&gt;&lt;/p&gt;&lt;p&gt;从上图可以看出，ip 首部有 20 字节的固定长度是用来存储这个 IP 数据包的基本信息的：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;源地址 32 位（4 个字节）：123.123.123.123&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;目的地址 32 位（4 个字节）：110.242.68.3&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;协议 8 位（1 个字节）：内部数据包使用的协议，即 TCP、UDP 或 ICMP（就是 ping 命令使用的协议）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;首部检验和 16 位（2 个字节）：此 IP 首部的数据校验和，用于验证 IP 首部的数据完整性&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;ip 首部最重要的数据是&lt;code&gt;源 ip 地址&lt;/code&gt;和&lt;code&gt;目的 ip 地址&lt;/code&gt;&lt;/h4&gt;&lt;h3&gt;IP 层下面是 MAC 层(物理层)&lt;/h3&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.3555992141453831&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA3ENoeFVCSfeyaSoUibPZYLYRu9jSgPq1m5EXJLYo4OL4ewZcjJ62niaumGY4Ar8KGyHZADuFGRaycw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1018&quot;/&gt;&lt;/p&gt;&lt;p&gt;物理层中的二进制数据以上图中的格式进行组织，其基本单元被称为“MAC 帧”。&lt;/p&gt;&lt;p&gt;每一台网络设备的 MAC 帧的长度不一定一致，默认为 1500，即 IP 层的数据会按照这个长度进行分包。在局域网速度跑不到协商速率，需要做性能优化时（例如 iSCSI 网络磁盘），可以使用“巨型帧”技术，将这个数字增加到一万，可以提升网络传输性能。不过，根据我的实际优化经验，绝大多数场景下，巨型帧对网络性能的提升小于 5%，属于一种聊胜于无的优化手段。&lt;/p&gt;&lt;p&gt;目的地址和源地址均为 MAC 地址，形式如 AA:BB:CC:DD:EE:FF，共有六段，每一段是一个两位的 16 进制数，两位 16 进制数换算成二进制就是 8 位，所以 MAC 地址的长度为 8*6 = 48 位。&lt;/p&gt;&lt;p&gt;类型字段采用 16 位二进制表示更上一层（ip 层）的网络层数据包的类型：IPv4、IPv6、ARP、iSCSI、RoCE 等等。&lt;/p&gt;&lt;p&gt;MAC 层就是交换机工作的地方，我们下篇文章会讲。&lt;/p&gt;&lt;h2&gt;Nginx 的性能极限&lt;/h2&gt;&lt;p&gt;在真实世界中，QPS 一般比保持 TCP 连接的客户端的数量要少，在此我们假设为四分之一，即：有 20 万个客户端设备在这段时间内访问我们的系统，每个客户端设备平均每 4 秒发送一个 HTTPS 请求。&lt;/p&gt;&lt;h3&gt;单台 Nginx 反向代理的性能极限&lt;/h3&gt;&lt;p&gt;由于 Nginx 不仅需要建立 TCP 连接，还需要将 TCP 连接中发送过来的数据包和某个进程/线程进行匹配，还需要对 HTTP 协议的信息进行解析、识别、转换、添加，所以它也有 QPS 上限：&lt;/p&gt;&lt;p&gt;在 2015 年主流的服务器 CPU 上，Nginx 官方在进行了极限优化的情况下进行了反向代理性能测试，在“建立 TCP 连接-发送 HTTPS 请求-断开 TCP 连接”的极限拉扯下，最高性能为 6W QPS（SSL TPS RSA 2048bit）²。&lt;/p&gt;&lt;p&gt;假设我们使用最新的服务器硬件，当虚拟机 CPU 达到 32 vCore 的时候，未经优化的单机 Nginx 性能就已经达到极限，能承受大约 1 万 HTTPS QPS，对应的连接用户就是 4 万，这个数字其实已经很夸张了。&lt;/p&gt;&lt;h2&gt;TCP 负载均衡器为何能抗住五万 QPS&lt;/h2&gt;&lt;p&gt;我们假设单台 Kong 应用网关的极限为 1 万 QPS，于是我们就需要五台 Kong，那这五台 Kong 前面的 TCP 负载均衡为何能够抗住呢？因为 TCP 负载均衡器要干的事情比 Kong 少非常多：它只需要在 IP 层做少量的工作即可。&lt;/p&gt;&lt;h3&gt;使用负载均衡器拆分 TCP 单点&lt;/h3&gt;&lt;p&gt;TCP 协议是一种“可靠地传输信息”的方法，它不仅有三次握手四次挥手等复杂的控制流程，还会对每一个报文段进行排序、确认、重发等操作来保证最终数据的完整和正确，所以，TCP 本身就是一种需要很多资源处理的&lt;code&gt;单点&lt;/code&gt;，接下来我们开始拆这个单点。&lt;/p&gt;&lt;h3&gt;TCP 负载均衡器的工作过程&lt;/h3&gt;&lt;p&gt;我们假设客户端 ip 为 123.123.123.123，负载均衡器的 ip 为 110.242.68.3（公网）和 10.0.0.100（私网），五台 Kong 服务器的 ip 为 10.0.0.1 ~ 10.0.0.5，架构图如下：&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.8775252525252525&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA3ENoeFVCSfeyaSoUibPZYLY0fpcxRuUicslPo9gALgZzI5SGicb41dRwTV7WY14GEnbDbf8SzwIVpOg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1584&quot;/&gt;&lt;/p&gt;&lt;p&gt;负载均衡器的工作过程如下：&lt;/p&gt;&lt;h4&gt;1. 接收数据（左侧）&lt;/h4&gt;&lt;p&gt;负载均衡器接收客户端数据包（报文）的过程如下：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;负载均衡器收到了一个 ip 报文：源地址 123.123.123.123，目的地址 110.242.68.3&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;ip 报文内包裹着一个 TCP 报文，详情如下：源端口 52387，目的端口 443&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;注意，负载均衡器只是接收了一个 IP 报文，并没有和客户端进行三次握手，并没有和客户端建立“TCP 连接”&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;2. 发送数据给上游服务器（右侧）&lt;/h4&gt;&lt;p&gt;在接收到客户端的 IP 报文以后，负载均衡器会找一台上游服务器，准备把数据发送过去：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;内部 TCP 报文首部：源端口 45234，目的端口 443&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;TCP 报文外面包裹的 IP 首部：源地址 10.0.0.100，目的地址 10.0.0.1&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;负载均衡器将包裹着 TCP 数据包的 IP 报文发送了出去&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;3. 建立两个报文的映射关系并进行数据转发&lt;/h4&gt;&lt;p&gt;负载均衡器会在内存里创建两个五元组：&lt;/p&gt;&lt;p&gt;左侧五元组&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;左侧源地址 123.123.123.123&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;左侧目的地址 110.242.68.3&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;左侧源端口 52387&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;左侧目的地址 443&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;协议 TCP&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;右侧五元组&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;右侧源地址 10.0.0.100&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;右侧目的地址 10.0.0.1&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;右侧源端口 45234&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;右侧目的地址 443&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;协议 TCP&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;然后，负载均衡器会关联这两个五元组：对两侧发来的数据包（报文）进行拆包和修改（两个地址+两个端口），并从另一侧发送出去。&lt;/p&gt;&lt;h3&gt;这是什么？这就是你家的路由器（网关）呀&lt;/h3&gt;&lt;p&gt;看过我《软件工程师需要了解的网络知识》系列文章的同学应该能一眼看出，这就是网关的工作模式，你家几百块的路由器主要干的就是这个工作。&lt;/p&gt;&lt;h4&gt;为什么性能开销比 Kong 低&lt;/h4&gt;&lt;p&gt;我们可以看出，负载均衡器/网关只需要做两件事：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;建立两个五元组并关联&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;修改数据包的地址和端口，再将数据包发送出去&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这个操作在网络领域内被称作 NAT（网络地址转换）。&lt;/p&gt;&lt;p&gt;由于这个工作非常简单，其中大部分的工作都可以用专用硬件来解决：例如开发专门的五元组存储和关联芯片，开发专门的 NPU（网络数据包处理器）来进行快速数据修改。所以，家用路由器可以做到在 300 块终端售价的情况下实现超过 1Gbit/S 的 NAT 性能。&lt;/p&gt;&lt;h4&gt;Kong 网关需要建立“TCP 连接”&lt;/h4&gt;&lt;p&gt;Kong 网关需要真的和客户端“建立 TCP 连接”：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;三次握手建立连接&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对数据包进行排序、校验，收到心跳包需要回复&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;需要将这个 TCP 连接和一个进程/线程进行绑定：&lt;/p&gt;&lt;/li&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;在收到数据以后，找出这个进程/线程，把数据发送给它&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;等进程/线程回复以后，再找到该进程/线程对应的那个 TCP 连接，把数据发送出去&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;h4&gt;四层负载均衡（L4）和七层负载均衡（L7）&lt;/h4&gt;&lt;p&gt;在卖负载均衡的商业公司那里，应用网关也叫七层负载均衡，因为它工作在 OSI 七层网络模型的第七层，而我们讨论的工作在 IP 层的负载均衡叫四层负载均衡，工作在 OSI 七层网络模型的第四层。再看到 L4 L7 这两个词，你们就能一眼看穿它了，其实一点都不神秘。&lt;/p&gt;&lt;h3&gt;还记得我们的目标吗？一百万 QPS&lt;/h3&gt;&lt;p&gt;我们通过使用一个负载均衡器，可以完美抗下五万 QPS 的负载：一个 TCP 负载均衡器，下挂五个安装了 Kong 应用网关的虚拟机，再下挂 N 台虚拟机，无论是 PHP 语言还是 golang，都可以实现五万 QPS 的设计目标。&lt;/p&gt;&lt;h3&gt;接下来&lt;/h3&gt;&lt;p&gt;下一篇文章，我们将着手突破普通 Linux 系统网络性能的上限：使用软件定义网络（SDN）替代百万人民币的负载均衡硬件，并最终搭建出一个能够支撑 200 Gbps 带宽的负载均衡集群。&lt;/p&gt;&lt;h3&gt;参考资料&lt;/h3&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;软件工程师需要了解的网络知识：从铜线到HTTP（三）—— TCP/IP https://lvwenhan.com/tech-epic/487.html&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;6W QPS（SSL TPS RSA 2048bit） https://www.nginx.com/resources/datasheets/nginx-plus-sizing-guide/&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;本系列文章已经汇总成开源技术书《PPHC》发布在 Github：https://github.com/johnlui/PPHC&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;mp-style-type data-value=&quot;3&quot;/&gt;&lt;/p&gt;&lt;/div&gt;

          
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>cb96d87cc46dd6843ddab3573a9c60eb</guid>
<title>优质网站同好者周刊（第 105 期） | 倾城博客</title>
<link>https://toutiao.io/k/d1uoofn</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://nicelinks.site/?utm_source=weekly&quot;&gt;倾城之链&lt;/a&gt;作为一个开放平台，旨在云集全球&lt;strong&gt;优秀网站&lt;/strong&gt;，探索互联网中更广阔的世界。此周刊，将汇聚过去一周&lt;a href=&quot;https://nicelinks.site/?utm_source=weekly&quot;&gt;倾城&lt;/a&gt;所收录的内容，以飨同好；欢迎推荐或自荐（仅限有独立域名的网站，可以是二级域名）。您如果要了解收录要求，请参见&lt;a href=&quot;https://nicelinks.site/about?utm_source=weekly&quot;&gt;关于倾城&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;备注&lt;/strong&gt;：本周刊&lt;strong&gt;每周五&lt;/strong&gt;生成，首发于个人微信公众号&lt;a href=&quot;https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzI5MDIwMzM2Mg==&amp;amp;action=getalbum&amp;amp;album_id=1530765143352082433&amp;amp;scene=173&amp;amp;from_msgid=2650641087&amp;amp;from_itemidx=1&amp;amp;count=3#wechat_redirect&quot;&gt;晚晴幽草轩&lt;/a&gt;、博客&lt;a href=&quot;https://www.jeffjade.com&quot;&gt;晚晴幽草轩&lt;/a&gt;，以及&lt;a href=&quot;https://forum.lovejade.cn/&quot;&gt;悠然宜想亭&lt;/a&gt;社区；此一键生成脚本基于 &lt;a href=&quot;https://nicelinks.site/post/602d30aad099ff5688618591&quot;&gt;Deno&lt;/a&gt; 编写，并在 Github 开源：&lt;a href=&quot;https://github.com/nicejade/nicelinks-weekly&quot;&gt;nicejade/nicelinks-weekly&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;标签&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/tags/runtime&quot;&gt;&lt;code&gt;runtime&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/JavaScript&quot;&gt;&lt;code&gt;JavaScript&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/%E5%B9%B3%E5%8F%B0&quot;&gt;&lt;code&gt;平台&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Bundle, transpile, install, and run JavaScript &amp;amp; TypeScript — all in Bun. Bun is a new JavaScript runtime with a native bundler, transpiler, task runner, and npm client built-in.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;https://oss.nicelinks.site/bun.sh.png?x-oss-process=style/png2jpg&quot; alt=&quot;倾城之链 - Bun — fast all-in-one JavaScript runtime&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推荐语&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/redirect?url=https://bun.sh/&quot;&gt;Bun&lt;/a&gt; （馒头）是一个快速的一体化 &lt;a href=&quot;https://nicelinks.site/tags/JavaScript&quot;&gt;JavaScript&lt;/a&gt; 运行时；捆绑、转译、安装和运行 JavaScript 和 TypeScript 项目——全部在 Bun 中。Bun 是一个新的 JavaScript 运行时，内置了原生的打包器、转译器、任务运行器和 &lt;a href=&quot;https://nicelinks.site/tags/npm&quot;&gt;npm&lt;/a&gt; 客户端。&lt;/p&gt;&lt;h3 id=&quot;什么是-bun&quot;&gt;&lt;a href=&quot;#%E4%BB%80%E4%B9%88%E6%98%AF-bun&quot; aria-label=&quot;什么是 bun permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;什么是 Bun？&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://nicelinks.site/redirect?url=https://bun.sh/&quot;&gt;Bun&lt;/a&gt; 是像 &lt;a href=&quot;https://nicelinks.site/post/5f376ebe1751843ef894c899&quot;&gt;Node&lt;/a&gt; 或 &lt;a href=&quot;https://nicelinks.site/post/602d30aad099ff5688618591&quot;&gt;Deno&lt;/a&gt; 一样的现代 JavaScript 运行时。它是从头开始构建的，专注于三个主要方面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;快速开始（考虑到优势）。&lt;/li&gt;&lt;li&gt;新的性能水平（扩展 JavaScriptCore，引擎）。&lt;/li&gt;&lt;li&gt;成为一个伟大而完整的工具（捆绑器、转译器、包管理器）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Bun 旨在替代您当前的 JavaScript 和 TypeScript 应用程序或脚本——在您的本地计算机、服务器或边缘。Bun 原生实现了数百个 Node.js 和 Web API，包括约 90% 的 &lt;a href=&quot;https://nodejs.org/api/n-api.html&quot;&gt;Node API&lt;/a&gt; functions (native modules), fs, path, Buffer 等等。&lt;/p&gt;&lt;p&gt;Bun 的目标是在浏览器之外运行世界上大部分的 JavaScript，为您未来的基础架构带来性能和复杂性增强，并通过更好、更简单的工具提高开发人员的生产力。&lt;/p&gt;&lt;h3 id=&quot;bun-是如何工作的&quot;&gt;&lt;a href=&quot;#bun-%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84&quot; aria-label=&quot;bun 是如何工作的 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;Bun 是如何工作的？&lt;/h3&gt;&lt;p&gt;Bun 使用 &lt;a href=&quot;https://github.com/WebKit/WebKit/tree/main/Source/JavaScriptCore&quot;&gt;JavaScriptCore&lt;/a&gt; 引擎，它的启动和执行速度往往比 &lt;a href=&quot;https://nicelinks.site/post/63986e3ce524a8432ed1d256&quot;&gt;V8&lt;/a&gt; 等更传统的选择要快一些。Bun 是用一种具有手动内存管理功能的低级编程语言编写的。Bun 的大部分内容都是从头开始编写的，包括 JSX/TypeScript 转译器、npm 客户端、捆绑器、SQLite 客户端、HTTP 客户端、WebSocket 客户端等。&lt;/p&gt;&lt;h3 id=&quot;bun-为什么快&quot;&gt;&lt;a href=&quot;#bun-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%AB&quot; aria-label=&quot;bun 为什么快 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;Bun 为什么快？&lt;/h3&gt;&lt;p&gt;大量时间花在分析、基准测试和优化上。Bun 的每个部分的答案都不同，但有一个普遍的主题：Bun 对内存的低级控制，和隐藏控制流的缺乏使得编写快速软件变得更加简单。&lt;/p&gt;&lt;p&gt;Bun 是  Jarred Sumner 在 2022 年 3 月开启的 &lt;a href=&quot;https://github.com/oven-sh/bun&quot;&gt;开源项目&lt;/a&gt; ，主打高性能、工具链；对标的是 &lt;a href=&quot;https://nicelinks.site/post/5f376ebe1751843ef894c899&quot;&gt;Node&lt;/a&gt; （基于 C++）或 &lt;a href=&quot;https://nicelinks.site/post/602d30aad099ff5688618591&quot;&gt;Deno&lt;/a&gt; （基于 Rust）；Node 和 Deno 用的 JS 引擎是 V8 (Chromium)；而 Bun 使用了新兴的系统编程语言 &lt;a href=&quot;https://ziglang.org/&quot;&gt;Zig&lt;/a&gt; （相当于加强版 C 语言），所使用的引擎是 JavaScriptCore (Safari)；Deno 于 2017 创立，尚未看到更多用于生产环境，可以预见 Bun 还有很长的路要走。截止目前（2023 年 02 月 16 日），版本更新至 &lt;code&gt;0.5.6&lt;/code&gt;（beta）；如果你对其感兴趣，可移步 &lt;a href=&quot;https://bun.sh/blog&quot;&gt;Bun Blog&lt;/a&gt; 以了解更多。&lt;/p&gt;&lt;p&gt;── 出自&lt;a href=&quot;https://nicelinks.site/post/63ee1d1d3ca14e0315d81b2f&quot;&gt;倾城之链 - Bun — fast all-in-one JavaScript runtime&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;标签&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/tags/WebAssembly&quot;&gt;&lt;code&gt;WebAssembly&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/%E8%B7%A8%E5%B9%B3%E5%8F%B0&quot;&gt;&lt;code&gt;跨平台&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/runtime&quot;&gt;&lt;code&gt;runtime&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. It powers serverless apps, embedded functions, microservices, smart contracts, and IoT devices.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;https://oss.nicelinks.site/wasmedge.org.png?x-oss-process=style/png2jpg&quot; alt=&quot;倾城之链 - WasmEdge ｜ A lightweight, high-performance, and extensible WebAssembly runtime&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推荐语&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/redirect?url=https://wasmedge.org/&quot;&gt;WasmEdge&lt;/a&gt; ：&lt;strong&gt;是一种轻量级、高性能且可扩展的 WebAssembly 运行时&lt;/strong&gt;，适用于云原生、边缘和去中心化应用程序。它为无服务器应用程序、嵌入式功能、微服务、智能合约和物联网设备提供支持。它具有如下功能特征：&lt;/p&gt;&lt;h3 id=&quot;高性能&quot;&gt;&lt;a href=&quot;#%E9%AB%98%E6%80%A7%E8%83%BD&quot; aria-label=&quot;高性能 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;高性能&lt;/h3&gt;&lt;p&gt;与 Linux 容器相比，WasmEdge 在启动时可以快 100 倍，在运行时可以快 20%。WasmEdge 应用程序的大小可能是类似 Linux 容器应用程序的 1/100。&lt;/p&gt;&lt;h3 id=&quot;javascript-支持&quot;&gt;&lt;a href=&quot;#javascript-%E6%94%AF%E6%8C%81&quot; aria-label=&quot;javascript 支持 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;JavaScript 支持&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;ES6 模块和标准 API 支持；&lt;/li&gt;&lt;li&gt;节点和 NPM 模块 API 支持；&lt;/li&gt;&lt;li&gt;React SSR 流式传输；&lt;/li&gt;&lt;li&gt;在 Rust 中实现 JS API；&lt;/li&gt;&lt;li&gt;比容器化的 v8 轻得多；&lt;/li&gt;&lt;/ul&gt;&lt;h3 id=&quot;跨平台支持&quot;&gt;&lt;a href=&quot;#%E8%B7%A8%E5%B9%B3%E5%8F%B0%E6%94%AF%E6%8C%81&quot; aria-label=&quot;跨平台支持 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;跨平台支持&lt;/h3&gt;&lt;p&gt;WasmEdge 支持 Linux、苹果操作系统、Windows、微内核和实时操作系统、Intel x86、ARM 和 M1 CPU 等。&lt;/p&gt;&lt;h3 id=&quot;易于扩展&quot;&gt;&lt;a href=&quot;#%E6%98%93%E4%BA%8E%E6%89%A9%E5%B1%95&quot; aria-label=&quot;易于扩展 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;易于扩展&lt;/h3&gt;&lt;p&gt;在 C++ 中构建 WasmEdge 扩展。使用 C、Go 和 Rust 中的本机主机函数构建自定义的 WasmEdge 运行时。&lt;/p&gt;&lt;h3 id=&quot;易于嵌入主机程序&quot;&gt;&lt;a href=&quot;#%E6%98%93%E4%BA%8E%E5%B5%8C%E5%85%A5%E4%B8%BB%E6%9C%BA%E7%A8%8B%E5%BA%8F&quot; aria-label=&quot;易于嵌入主机程序 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;易于嵌入主机程序&lt;/h3&gt;&lt;p&gt;在 C、Go、Rust、Node.js 和 Python 主机应用程序中嵌入 WasmEdge 函数。在云和边缘原生应用程序平台和框架中嵌入功能。&lt;/p&gt;&lt;p&gt;WasmEdge Runtime 为其包含的 &lt;a href=&quot;https://nicelinks.site/tags/WebAssembly&quot;&gt;WebAssembly&lt;/a&gt; 字节码程序提供了一个定义良好的执行沙箱。运行时为操作系统资源（例如，文件系统、套接字、环境变量、进程）和内存空间提供隔离和保护。WasmEdge 最重要的用例是安全地执行用户定义或社区贡献的代码作为软件产品（例如，SaaS、软件定义车辆、边缘节点，甚至区块链节点）的插件。它使第三方开发人员、供应商、供应商和社区成员能够扩展和定制软件产品。&lt;/p&gt;&lt;p&gt;WasmEdge 可以运行从 C/C++、Rust、Swift、AssemblyScript 或 Kotlin 源代码编译的标准 WebAssembly 字节码程序。它在安全、快速、轻量级、可移植和容器化的沙箱中运行 JavaScript，包括第 3 方 ES6、CJS 和 &lt;a href=&quot;https://nicelinks.site/tags/npm&quot;&gt;NPM&lt;/a&gt; 模块。它还支持混合使用这些语言（例如，使用 Rust 来实现 JavaScript API）、Fetch API 和边缘服务器（edge servers）上的服务器端渲染 (SSR)功能。&lt;/p&gt;&lt;p&gt;WasmEdge 支持所有标准的 WebAssembly 功能和许多建议的扩展。它还支持许多为云原生和边缘计算用途量身定制的扩展（例如，WasmEdge 网络套接字和 WasmEdge &lt;a href=&quot;https://nicelinks.site/tags/TensorFlow&quot;&gt;Tensorflow&lt;/a&gt; 扩展）。详细了解 WasmEdge 的 &lt;a href=&quot;https://wasmedge.org/book/en/intro/features.html&quot;&gt;技术亮点&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;── 出自&lt;a href=&quot;https://nicelinks.site/post/63ee15303ca14e0315d819ec&quot;&gt;倾城之链 - WasmEdge ｜ A lightweight, high-performance, and extensible WebAssembly runtime&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;标签&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/tags/%E6%95%B0%E6%8D%AE&quot;&gt;&lt;code&gt;数据&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96&quot;&gt;&lt;code&gt;可视化&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/%E5%9B%BE%E8%A1%A8&quot;&gt;&lt;code&gt;图表&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Make slick, interactive data visualisations with our free chart-maker tool — no log-in required. Easily embed into Notion or wherever you like!&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;https://oss.nicelinks.site/graphy.new.png?x-oss-process=style/png2jpg&quot; alt=&quot;倾城之链 - Graphy | Slick, interactive charts that make you stand out&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推荐语&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/redirect?url=https://graphy.new/&quot;&gt;Graphy&lt;/a&gt; ，一款免费图表制作工具，可用来制作光滑的、互动的数据 &lt;a href=&quot;https://nicelinks.site/tags/%E5%8F%AF%E8%A7%86%E5%8C%96&quot;&gt;可视化&lt;/a&gt; ，打开即可使用，不需要登录。支持下载、拷贝图片，也可以轻松地嵌入 Notion 或任何你喜欢的地方。而且支持各种定制，如图表类型、图标配色、配色横纵比（Aspect Ratio）、详情及主题。&lt;/p&gt;&lt;p&gt;更精确地说明， &lt;a href=&quot;https://nicelinks.site/redirect?url=https://graphy.new/&quot;&gt;Graphy&lt;/a&gt; 是一款独立开发项目，可以创建好看的 &lt;a href=&quot;https://nicelinks.site/tags/%E8%A1%A8%E6%A0%BC&quot;&gt;表格&lt;/a&gt; &lt;strong&gt;图片&lt;/strong&gt;；如果您有基于 PPT、Excel 创建可视化表格相关诉求，完全可以基于 &lt;code&gt;graphy&lt;/code&gt; 替代之；相比 Excel、PPT，基于 &lt;a href=&quot;https://nicelinks.site/redirect?url=https://graphy.new/&quot;&gt;Graphy&lt;/a&gt; 创建数据、打造样式、导入数据、生成图片、调整背景（配色），都更为高效且人性化。&lt;/p&gt;&lt;p&gt;── 出自&lt;a href=&quot;https://nicelinks.site/post/63ecca0c3ca14e0315d80fc1&quot;&gt;倾城之链 - Graphy | Slick, interactive charts that make you stand out&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;标签&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/tags/%E4%BD%8E%E4%BB%A3%E7%A0%81&quot;&gt;&lt;code&gt;低代码&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/%E5%B9%B3%E5%8F%B0&quot;&gt;&lt;code&gt;平台&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/Web%E5%BC%80%E5%8F%91&quot;&gt;&lt;code&gt;Web开发&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Retool is the fast way to build internal tools. Drag-and-drop our building blocks and connect them to your databases and APIs to build your own tools, instantly. Connects with Postgres, REST APIs, GraphQL, Firebase, Google Sheets, and more. Built by developers, for developers. Trusted by startups and Fortune 500s. Sign up for free.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;https://oss.nicelinks.site/retool.com.png?x-oss-process=style/png2jpg&quot; alt=&quot;倾城之链 - Retool | Build internal tools, remarkably fast.&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推荐语&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/redirect?url=https://retool.com/&quot;&gt;Retool&lt;/a&gt; ，是一个面向企业的 &lt;a href=&quot;https://nicelinks.site/tags/%E4%BD%8E%E4%BB%A3%E7%A0%81&quot;&gt;低代码&lt;/a&gt; 开发平台，它于 2017 年在美国著名创业孵化器 Y Combinator 中诞生，旨在为企业提供一个工具开发平台，让企业能够使用有限的编程语言，构建自定义业务工具。&lt;/p&gt;&lt;p&gt;作为优先探索低代码开发的平台之一，Retool 为开发者们提供了许多方便：无需任何前端编程基础的可拖拽的 UI 组件、与各种数据库和 API 的连接、安全且灵活的个性化开发模式。更详细介绍，它具有如下功能特征：&lt;/p&gt;&lt;h3 id=&quot;一整套强大的构建块&quot;&gt;&lt;a href=&quot;#%E4%B8%80%E6%95%B4%E5%A5%97%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%9E%84%E5%BB%BA%E5%9D%97&quot; aria-label=&quot;一整套强大的构建块 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;一整套强大的构建块&lt;/h3&gt;&lt;p&gt;所有内部工具都由相同的构建块组成：表格、列表、图表、表单、向导、地图等。Retool 提供开箱即用的这些块，因此您可以花时间组装您的 UI，而不是从头开始发发它。无需寻找最好的 React 表库，您可以通过拖放在几分钟内将您的应用程序组合在一起。&lt;/p&gt;&lt;h3 id=&quot;连接到任何东西&quot;&gt;&lt;a href=&quot;#%E8%BF%9E%E6%8E%A5%E5%88%B0%E4%BB%BB%E4%BD%95%E4%B8%9C%E8%A5%BF&quot; aria-label=&quot;连接到任何东西 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;连接到任何东西&lt;/h3&gt;&lt;p&gt;使用 REST、GraphQL 或 gRPC API 连接到大多数数据库或任何东西。Retool 使您能够在一个应用程序中无缝地处理所有数据源。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;如何存储数据取决于您&lt;/strong&gt;。默认情况下，Retool 中不存储任何内容。运行查询时，Retool 后端会将请求代理到您的后端。没有更多的 ETL 数据。&lt;/p&gt;&lt;h3 id=&quot;专为开发人员打造&quot;&gt;&lt;a href=&quot;#%E4%B8%93%E4%B8%BA%E5%BC%80%E5%8F%91%E4%BA%BA%E5%91%98%E6%89%93%E9%80%A0&quot; aria-label=&quot;专为开发人员打造 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;专为开发人员打造&lt;/h3&gt;&lt;p&gt;Retool 是高度可破解的，因此您永远不会受到开箱即用的限制。如果您可以使用 JavaScript 和 API 编写它，则可以在 Retool 中构建它。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;您几乎可以在 Retool 中的任何地方编写 JavaScript。&lt;/strong&gt;  只需用双括号包裹一个表达式 &lt;code&gt;{{ Math.max(select1.value, 10) }}&lt;/code&gt; ，它就会作为沙盒 JS 执行。使用 Transformers，您可以编写更大的、可重用的代码块来处理数据。还有一个本机 API，用于通过 JS 直接与组件和查询进行交互。&lt;/p&gt;&lt;h3 id=&quot;安全地部署和共享&quot;&gt;&lt;a href=&quot;#%E5%AE%89%E5%85%A8%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%92%8C%E5%85%B1%E4%BA%AB&quot; aria-label=&quot;安全地部署和共享 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;安全地部署和共享&lt;/h3&gt;&lt;p&gt;Retool 具有内置的安全性、可靠性和权限。您可以在本地、在您自己的 VPN 后面以及在您自己的 VPC 中托管 Retool。通过 Docker 或 Kubernetes 部署。此外，Retool 支持基于 &lt;a href=&quot;https://nicelinks.site/tags/Git&quot;&gt;Git&lt;/a&gt; 进行代码管理。&lt;/p&gt;&lt;p&gt;搭建软件的过程是颇为繁杂，但这却是许多公司不可避免的一项工作。&lt;code&gt;低代码&lt;/code&gt; 应运而生，且迅速打入了市场。作为首先进军低代码平台之一的 Retool 一直占据着大众视野，其多样的 UI 组件、大量的数据源连接器以及多种个性化使用功能为程序开发者带来了许多便利。&lt;/p&gt;&lt;p&gt;总体来说，Retool 通过「&lt;strong&gt;搭建程序界面&lt;/strong&gt;」「&lt;strong&gt;建立查询连接数据源&lt;/strong&gt;」「&lt;strong&gt;构建组件间的逻辑顺序&lt;/strong&gt;」为基本框架来快速搭建应用程序。在 UI 组件方面，Retool 不仅不断的在添加新的选择，他们还推出了开发者自定义设计组件的功能。Retool 还充分考虑到了应用的权限问题，开发者可以自行分组并规定每个分组对应用所享有的权限。&lt;/p&gt;&lt;p&gt;低代码并不意味着完全 &lt;a href=&quot;https://nicelinks.site/tags/%E6%97%A0%E4%BB%A3%E7%A0%81&quot;&gt;无代码&lt;/a&gt; ，根据低代码工具的属性和应用场景不同，对代码能力的要求也有区别。例如以 Airtable 和 &lt;a href=&quot;https://nicelinks.site/post/6199f84c60f6c5569db65f20&quot;&gt;飞书&lt;/a&gt; 多维表格这类以表格为主体，可以用来存放、统计、分析数据的平台，基本属于零代码。而 Bubble、Retool 等网页应用开发工具，则需要具备基本的编程思维和代码开发能力。&lt;/p&gt;&lt;p&gt;── 出自&lt;a href=&quot;https://nicelinks.site/post/63eb6bffdfb7f255c25dbcb0&quot;&gt;倾城之链 - Retool | Build internal tools, remarkably fast.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;标签&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/tags/%E6%96%87%E6%A1%A3&quot;&gt;&lt;code&gt;文档&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8&quot;&gt;&lt;code&gt;网站生成器&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/React&quot;&gt;&lt;code&gt;React&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;An optimized site generator in React. Docusaurus helps you to move fast and write content. Build documentation websites, blogs, marketing pages, and more.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;https://oss.nicelinks.site/docusaurus.io.png?x-oss-process=style/png2jpg&quot; alt=&quot;倾城之链 - Build optimized websites quickly, focus on your content | Docusaurus&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推荐语&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/redirect?url=https://docusaurus.io/&quot;&gt;Docusaurus&lt;/a&gt; ，基于 &lt;a href=&quot;https://nicelinks.site/post/5b1294b5e93ed2618cfac134&quot;&gt;React&lt;/a&gt; 的一个优化的、易于维护的 &lt;a href=&quot;https://nicelinks.site/tags/%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%99%A8&quot;&gt;网站生成器&lt;/a&gt; 。Docusaurus 帮助你快速移动和编写内容：建立文档网站、博客、营销页面等，使您可以专注于内容。它具有如下功能特征：&lt;/p&gt;&lt;h3 id=&quot;由-mdx-提供支持&quot;&gt;&lt;a href=&quot;#%E7%94%B1-mdx-%E6%8F%90%E4%BE%9B%E6%94%AF%E6%8C%81&quot; aria-label=&quot;由 mdx 提供支持 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;由 MDX 提供支持&lt;/h3&gt;&lt;p&gt;节省时间并专注于文本文档。只需使用 MDX 编写文档和博客文章，Docusaurus 就会将它们构建成静态 HTML 文件，随时可供使用。多亏了 &lt;a href=&quot;https://nicelinks.site/post/63e4e3eee63ccd089dee6686&quot;&gt;MDX&lt;/a&gt;，你甚至可以在你的 Markdown 中嵌入 React 组件。&lt;/p&gt;&lt;h3 id=&quot;使用-react-构建&quot;&gt;&lt;a href=&quot;#%E4%BD%BF%E7%94%A8-react-%E6%9E%84%E5%BB%BA&quot; aria-label=&quot;使用 react 构建 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;使用 React 构建&lt;/h3&gt;&lt;p&gt;通过编写 React 组件来扩展和自定义项目的布局。利用可插入架构，设计您自己的站点，同时重用由 Docusaurus 插件创建的相同数据。&lt;/p&gt;&lt;h3 id=&quot;准备翻译&quot;&gt;&lt;a href=&quot;#%E5%87%86%E5%A4%87%E7%BF%BB%E8%AF%91&quot; aria-label=&quot;准备翻译 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;准备翻译&lt;/h3&gt;&lt;p&gt;本地化是开箱即用的。使用 git、Crowdin 或任何其他翻译管理器来翻译您的文档并单独部署它们。&lt;/p&gt;&lt;h3 id=&quot;文档版本控制&quot;&gt;&lt;a href=&quot;#%E6%96%87%E6%A1%A3%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6&quot; aria-label=&quot;文档版本控制 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;文档版本控制&lt;/h3&gt;&lt;p&gt;支持用户使用您项目的所有版本。文档版本控制可帮助您使文档与项目发布保持同步。&lt;/p&gt;&lt;h3 id=&quot;内容搜索&quot;&gt;&lt;a href=&quot;#%E5%86%85%E5%AE%B9%E6%90%9C%E7%B4%A2&quot; aria-label=&quot;内容搜索 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;内容搜索&lt;/h3&gt;&lt;p&gt;让您的社区可以轻松地在您的文档中找到他们需要的内容。Docusaurus 支持 &lt;a href=&quot;https://nicelinks.site/post/5b141444ca049846c6ee6f91&quot;&gt;Algolia&lt;/a&gt; 文档搜索、Typesense 文档搜索、本地搜索、自定义搜索。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://nicelinks.site/redirect?url=https://docusaurus.io/&quot;&gt;Docusaurus&lt;/a&gt; 在 &lt;a href=&quot;https://github.com/facebook/docusaurus&quot;&gt;Github 开源&lt;/a&gt;，目前共获得 42K+ Star；在 &lt;a href=&quot;https://nicelinks.site&quot;&gt;倾城之链&lt;/a&gt; 收录了蛮多 &lt;a href=&quot;https://nicelinks.site/tags/%E6%96%87%E6%A1%A3&quot;&gt;文档&lt;/a&gt; 型工具库， &lt;a href=&quot;https://nicelinks.site/redirect?url=https://docusaurus.io/&quot;&gt;Docusaurus&lt;/a&gt; 是功能相对最为完善之一，而且界面美观、性能优越，无需额外做更多处理，如果您有搭建文档类诉求，推荐优先考虑。&lt;/p&gt;&lt;p&gt;── 出自&lt;a href=&quot;https://nicelinks.site/post/63ea2b08dfb7f255c25db67c&quot;&gt;倾城之链 - Build optimized websites quickly, focus on your content | Docusaurus&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;标签&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/tags/%E7%BC%96%E8%BE%91%E5%99%A8&quot;&gt;&lt;code&gt;编辑器&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/Markdown&quot;&gt;&lt;code&gt;Markdown&lt;/code&gt;&lt;/a&gt; · &lt;a href=&quot;https://nicelinks.site/tags/%E5%B7%A5%E5%85%B7&quot;&gt;&lt;code&gt;工具&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;TinyMCE is the most advanced WYSIWYG HTML editor designed to simplify website content creation. The rich text editing platform that helped launched Atlassian, Medium, Evernote and more.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;https://oss.nicelinks.site/www.tiny.cloud.png?x-oss-process=style/png2jpg&quot; alt=&quot;倾城之链 - The Most Advanced WYSIWYG Editor | Trusted Rich Text Editor&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推荐语&lt;/strong&gt;：&lt;a href=&quot;https://nicelinks.site/redirect?url=https://www.tiny.cloud/&quot;&gt;TinyMCE&lt;/a&gt; 是先进的 &lt;a href=&quot;https://en.wikipedia.org/wiki/WYSIWYG&quot;&gt;WYSIWYG&lt;/a&gt; （&lt;strong&gt;What You See Is What You Get&lt;/strong&gt;，所见即所得） HTML 编辑器，适用于 &lt;a href=&quot;https://nicelinks.site/post/5b1294b5e93ed2618cfac134&quot;&gt;React&lt;/a&gt; 、 &lt;a href=&quot;https://nicelinks.site/post/5b1a221c0526c920d6dfaada&quot;&gt;Vue&lt;/a&gt; 和 &lt;a href=&quot;https://nicelinks.site/post/5b2b7f663bd7ef3847a3fadf&quot;&gt;Angular&lt;/a&gt; ；它旨在简化网站内容创建，让它们能够为用户创造卓越的内容和体验；TinyMCE 帮助推出 Atlassian、Medium、Evernote 等的富文本编辑平台，并受到数百万开发人员的使用和信赖，是世界上最可定制、可扩展和灵活的富文本编辑器。它具有如下功能特征：&lt;/p&gt;&lt;h3 id=&quot;一体化&quot;&gt;&lt;a href=&quot;#%E4%B8%80%E4%BD%93%E5%8C%96&quot; aria-label=&quot;一体化 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;一体化&lt;/h3&gt;&lt;p&gt;借助官方组件，TinyMCE 可以轻松集成到您的项目中；拥有超过 29 个集成和 400 多个 API，请参阅 TinyMCE 文档以获取完整的编辑器集成列表。&lt;/p&gt;&lt;h3 id=&quot;可定制化&quot;&gt;&lt;a href=&quot;#%E5%8F%AF%E5%AE%9A%E5%88%B6%E5%8C%96&quot; aria-label=&quot;可定制化 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;可定制化&lt;/h3&gt;&lt;p&gt;可以轻松配置富文本编辑器的 UI 以匹配您的站点、产品或应用程序的设计。由于其灵活性，您可以根据需要为编辑器配置尽可能多或尽可能少的功能。&lt;/p&gt;&lt;p&gt;凭借 50 多个可用的强大插件，以及作为 TinyMCE 基础的可编辑内容，添加额外功能就像添加一行代码一样简单。实现大多数插件的全部功能只需要多几行代码。&lt;/p&gt;&lt;h3 id=&quot;可扩展性&quot;&gt;&lt;a href=&quot;#%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7&quot; aria-label=&quot;可扩展性 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;可扩展性&lt;/h3&gt;&lt;p&gt;有时您的编辑要求可能非常独特，您需要自由灵活地进行创新。由于 TinyMCE 是开源的，您可以查看源代码并开发自己的自定义功能扩展以满足您自己的要求。公开 TinyMCE API 使您更容易编写适合现有 TinyMCE UI 组件框架的自定义功能。&lt;/p&gt;&lt;h3 id=&quot;扩展功能和支持&quot;&gt;&lt;a href=&quot;#%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%E5%92%8C%E6%94%AF%E6%8C%81&quot; aria-label=&quot;扩展功能和支持 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;扩展功能和支持&lt;/h3&gt;&lt;p&gt;对于需要更深入的效率、合规性或按照企业级标准构建的协作功能的专业软件团队，请与他们的团队联系。Tiny 还为专业开发团队提供专门的 SLA 和支持。&lt;/p&gt;&lt;p&gt;TinyMCE 富文本编辑器的入门非常简单，简单的配置可以在不到 5 分钟的时间内完成。该项目在 &lt;a href=&quot;https://github.com/tinymce/tinymce&quot;&gt;Github 开源&lt;/a&gt; ，目前拥有 13K Star，有对富文本编辑器感兴趣或有需要的朋友，可前往以了解更多。&lt;/p&gt;&lt;p&gt;── 出自&lt;a href=&quot;https://nicelinks.site/post/63ea26ebdfb7f255c25db5b8&quot;&gt;倾城之链 - The Most Advanced WYSIWYG Editor | Trusted Rich Text Editor&lt;/a&gt;&lt;/p&gt;&lt;p&gt;对倾城之链感兴趣的朋友，可通过 Web，小程序，快应用等渠道进行访问(后续将支持更多，如 VsCode 插件，Chrome 扩展等)。您有任何问题，欢迎随时向我们反馈（您可以通过官网反馈渠道，或添加如下客服微信），🤲 。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://image.nicelinks.site/%E5%80%BE%E5%9F%8E%E4%B9%8B%E9%93%BE-%E5%BE%AE%E4%BF%A1-mini.jpeg&quot; alt=&quot;倾城之链 - 客服微信&quot;/&gt;&lt;/p&gt;&lt;h2 id=&quot;本期文末寄语&quot;&gt;&lt;a href=&quot;#%E6%9C%AC%E6%9C%9F%E6%96%87%E6%9C%AB%E5%AF%84%E8%AF%AD&quot; aria-label=&quot;本期文末寄语 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;本期文末寄语&lt;/h2&gt;&lt;p&gt;“我以为，最美的日子，当是晨起侍花，闲来煮茶，阳光下打盹，细雨中漫步，夜灯下读书，在这清浅时光里，一手烟火一手诗意，任窗外花开花落，云来云往，自是余味无尽，万般惬意。”── 中国当代作家、散文家、戏剧家 · 汪曾祺&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://nicelinks.site/?utm_source=weekly&quot;&gt;倾城之链&lt;/a&gt;作为一个开放平台，旨在云集全球&lt;strong&gt;优秀网站&lt;/strong&gt;，探索互联网中更广阔的世界；在这里，你可以轻松发现、学习、分享更多有用或有趣的事物。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;https://image.nicelinks.site/nicelinks-miniprogram-code.jpeg?imageView2/1/w/250/h/250/interlace/1/ignore-error/1&quot; alt=&quot;小程序码 - 倾城之链&quot;/&gt;&lt;/p&gt;&lt;h2 id=&quot;您可能感兴趣的文章&quot;&gt;&lt;a href=&quot;#%E6%82%A8%E5%8F%AF%E8%83%BD%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E6%96%87%E7%AB%A0&quot; aria-label=&quot;您可能感兴趣的文章 permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 16 16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;/&gt;&lt;/svg&gt;&lt;/a&gt;您可能感兴趣的文章&lt;/h2&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>d23af77d1ae7b8547ae8d4bccba213c6</guid>
<title>我们应该如何用好 AI？从 ChatGPT 到编程语言、大数据、前端</title>
<link>https://toutiao.io/k/egzdch8</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.6776084407971864&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/QRibyjewM1ICeLP64e9mz1ChDk0eYV0386A6bWdjmvCR3A0m0ZKB9Fsu0yXO6HgInw9ZpUDrevCrh6XibOeSz0wA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1706&quot;/&gt;&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;figcaption&gt;题图：达摩院文生图大模型绘制的一张照片&lt;/figcaption&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;2022 年是科技圈艰难的一年，很少有振奋人心的消息。惊喜的是年底 OpenAI 开放的 ChatGPT，一下点燃了整个科技圈，体验后感觉有点像人脑，智能化程度惊艳。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;曾经我和朋友常调侃人工智能就是“有多少人工，就有多少智能”，ChatGPT 打破了这种印象。这里少有“人工”的痕迹，更像是“人脑”。AI 就像从远处开来的火车，听腻了每年总有人说它来了、要来了、真的要来了，这一次感觉它从我旁边呼啸而过～&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;春节期间我围绕 ChatGPT、编程语言、大数据、前端 听了很多播客 Podcasts，脑海里一直在想，如此强大的 AI 能力，会给工作带来什么变化？作为工程师，工作机会是否会面临来自 AI 的威胁或者替代，我们应该如何在工作中更好地使用 AI？以下是我的观察和理解，本文不是硬核的 AI 技术解读，更多是从产品和应用角度阐述，我会先分享4个我认为内容较好的外部输入，然后是个人观点探讨。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;前置了解&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;ChatGPT 是现在的当红炸子鸡，但 AI 的进化不是一蹴而就，还有很多上层和周边的 AI 明星产品和概念，各自用一句话介绍下：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;AIGC：AI Generated Content，由 AI 生成内容。本文聊的 AI 都属于这个领域，常见的生成内容有 文本、图片、音频、视频。这两年文本、图片、音频均有大的技术突破&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;ChatGPT：OpenAI 开发的聊天机器人语言模型，基于 GPT 3.5，GPT 3.0 是在 2020.6 月发布，使用 大型语言模型（LLM,Large Language Model）来训练，并把结果存储到 Transformer 的模型参数中。预先训练得到的通用语言模型，所以能力强大，善解人意，即刻答复&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;DALL·E2：通过文字生成图片，OpenAI 公司发布&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Coliplot：微软开发的 VS Code 辅助编码插件，基于 GPT 3 和 Codex，使用 Github 数十亿公开代码库来训练，收费10刀/月&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Jasper：基于 OpenAI API 构建，帮企业和个人写营销推广文案和博客等文字内容&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;MidJourney：通过文字生成图片，使用 Discord 聊天室&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Stable Diffusion：通过文字生成图片，代码开源，有很多人拿来修改和使用&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;AGI：Artificial General Intelligence 通用人工智能，完全能模拟人的大脑思考创作。AI 的终极目标。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;4 个高质量的外部输入&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;1. StrictlyVC in conversation with Sam Altman (OpenAI)&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;https://www.bilibili.com/video/BV1qY411X71q/&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;推荐指数：⭐⭐⭐⭐⭐&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Sam Altman 是 ChatGPT 背后公司 OpenAI 的联合创始人兼 CEO，前任 YC 总裁。采访时间是 2023年1月份，应该是 Sam Altman 最新的公开分享。共有2部分，&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;第1部分是关于 Sam 的投资，他个人投资了 400+ 公司，有核聚变发电设备的 Helion，超音速飞行器，这部分和 AI 无关，可以简单略过。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;第2部分推荐一定要看一下。里面讲了很多 AI 宏观内容，主持人的问题很直接，关于商业竞争、盈利模式、创业建议的问题。包括 ChatGPT、AGI 、GPT 4、微软的合作、谷歌的竞争、OpenAI 盈利模式、AI 监管。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;内容总结如下：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Sam 没有预料到 ChatGPT 这么受欢迎（日常礼貌）。在 ChatGPT 发布前 10 个月前才确定用聊天的形式和 AI 交互。虽然以前 google 做过，后来失败了。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;没有 GPT 4 的能力和发布日期，会考虑 safety 和 responsibility 做好后再发布。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;GPT 4 没有网上流传的那么神奇，GPT 4 没有做到 AGI&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;盈利模式：OpenAI 是 platform 平台型公司。通过 licenses model 盈利很少，主要通过开放 API 给应用调用来收费（也就是 Model as a service）。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;AGI：未来肯定是多个 AGI，不会一家独大（我相信阿里早晚也会有 AGI 能力）&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;安全：AI 的 safety 非常重要。应该接受监管和监督。能发生最危险的事情就是 accidental misuse。（有一个主题是 AI alignment，确保未来 AI 被人控制和为人服务）&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;业务边界：Jasper 这类公司是安全的，OpenAI 主要做平台，未来只会做一些 killer app，来展示 AI 的能力边界。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;个人如何应对 AI 的挑战？：谷歌让 memorize 记忆不再那么重要，ChatGPT 挑战的是人类的 learn 学习能力。人类要不停地进化。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如何看待学生用 ChatGPT 写作业（甚至有人获奖）？这无法避免，要学会向前看。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Sam 的风格和 Elon Musk 完全不同。Sam 是尽可能降低人们预期，然后发布产品后打破预期。Elon 是不断吹牛，破裂以后再吹牛最迟明年。但共同点是，做的事情都很技术突破性。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;2. 科技早知道：AIGC可能改变人类未来，但它知道自己的未来在哪里吗？&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;https://guiguzaozhidao.fireside.fm/20220148&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;推荐指数：⭐⭐⭐⭐&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;主讲者都在 AI 圈多年创业或经验，OpenAI 的务实程度，模型迭代速度超过预期。讲了大模型、数据和算力这3个 AI 底层能力的相辅相成，一些 ChatGPT 内部的 transformer、训练方法。ChatGPT 用了更好的监督数据，让模型更符合人类的认知。以及在工业和商业上规模化落地的可能，ChatGPT 不仅是文本上的突破，未来也可以基于财务、营销、制造做一些智能的应用。（AI 对现有产品来讲，是一个新的能力维度，比如我们在做 Quick BI 时深刻感知到，无论仪表板多么精美，多少模板都不够，必须要 know how，结合制造、互联网、金融、零售等行业做特色的解决方案。）&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;3. 编程语言 From Turbo Pascal to Delphi to C# to TypeScript, an interview with PL legend Anders Hejlsberg&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;https://www.bilibili.com/video/BV1YG411L774&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;推荐指数：⭐⭐⭐⭐⭐&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;TS 是我最喜欢的语言，VS Code 是最受程序员欢迎的 Editor，前者是 Anders 创造，后者有他的深度参与。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Anders 有 40 年的编程经验，创造了一个又一个传奇，Delphi、C#、TypeScript 之父。视频只有20多分钟，很多传奇故事被一句带过。几个有趣的经历：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;最初在 Borland，开发了 Turbo Pascal，然后演变成 Delphi IDE，尝试过可视化生成代码，后来失败，原因是做不到 Scale，扩展性不好。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;在 Java 如日中天的时候，MS 也需要自己的平台，于是 Anders 编写了 C#，并为之工作 10 多年&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;为什么会有 TypeScript？因为 JavaScript 是真正跨平台的语言，但弱类型遇到最大的问题就是很难 Scale，于是在内部需求是先做 C# 到 JS 的解析器，叫 Script#。这样的目的是使用 C# 有真正的 IDE，类型检查，有很多强大的工具来校验。继而发现需要做一个新的语言，于是就有了开发者喜欢的 TypeScript。真正做到了让 JS scale to large projects。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对于 AI 对编程的影响，Anders 除了 “Surprise” 并没有太多反馈，看来现在大家都还在接受的过程中。但 AI 毫无疑问会降低在不同语言间切换的成本，在不同平台上更容易转换到对应的语言。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;视频中提到了 Language 是否有必要统一问题，是否有必要做一个跨 Language 的层，实际上没有必要。因为 Language 实际上是和 Platform 绑定，我们在谈 Language 实际上是在谈背后的 Platform，比如想到 Java 就对应 Server、想到 Swift 就对应 iOS、想到 Python 就对应算法模型，在 Platform 不打通的情况下把 Java 和 Swift 互相转毫无意义，但 Platform 打通牵涉太多内容，不切实际。所以程序员在预见的将来还是需要和不同的 Language 打交道，我想有了 ChatGPT 这类能力之后，Language 给开发者带来的门槛会降低，会有越来越多的开发者可以使用不同 Language 在不同 Platform 间自由切换。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;此外，由于 ChatGPT 有了大量开发者用户，编程语言的设计者也会考虑面向 ChatGPT 的可理解性做优化，因此描述性强的编程语言会有优势，新兴的编程语言或框架因为缺少数据训练不占优势。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;4. 跟 Shu Ding 聊聊前端和 Vercel&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;https://www.xiaoyuzhoufm.com/episode/638013b518554643b70ab197&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;推荐指数：⭐⭐⭐⭐⭐&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Vercel 工程师的一期节目，比较轻松。Shu Ding 是 Vercel 早期的员工，也是 SWR 的作者。Vercel 目前竟然已经到了 400 人。讲了一些小故事，印象深刻：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;和 Vercel 创始人 Guillermo Rauch 一起去日本参加 Conf 期间，因为倒时差，晚上2个人都睡不着，Guillermo 说，既然你也睡不着，我也睡不着，不如一起写代码吧。于是就有了 SWR。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Webpack 作者做事情的认真和负责程度上你难以想象！&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;对代码质量的执着：MR 过程中为了保证代码质量，发布前被老板催，但 Reviewer 拒绝 approve。每 review 一行代码就要承担责任。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;不要把问题分类，而是要学习去解决。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;以上是我看过听过感觉比较好的内容推荐。接下来是我的个人观点，和上面不是一一对应，欢迎讨论。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;目前 ChatGPT 的不足/缺点&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;不能一味去夸 ChatGPT，我尝试了 ChatGPT 的边界场景，发现仍有一些不足/缺点，这些是潜在的机会。&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;没有行业数据的训练，有些答案不准确。这是行业领域公司的机会，比如法律、医疗行业，缺少专业数据的训练，给出的答案完全是错的。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;没有实时性，ChatGPT 没有联网的能力，所有训练都是基于预先的数据集。这与企业做数据分析决策的及时性要求完全不符。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;2C 领域还没有杀手级的应用，除了 ChatGPT 本身外尚无网络效应的应用。预计接下来会是爆发期，预计会有新的交互形态提供颠覆式体验。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;无法做大段数据输入，聊天的方式虽然简单，但只能接受一小段数据输入。比如 Copilot 和 ChatGPT 可以帮你优化一小段代码，但我们 Quick BI 和 Dataphin 都是百万行以上的复杂度。优化几行对整体微不足道，怎么样保证代码安全的前提下去发现影响大的问题、通用性的一些问题。这个赛道未来会有工具产生。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;未来可能充斥着 AI 生成的“正确的废话”，是否需要另一个 AI 来去除噪音，理解核心意思，去掉这些废话？&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;擅长文本组织、语义理解、逻辑处理，但不擅长创造、逻辑推理、类比推理。比如你问 ChatGPT 脑筋急转弯、猜灯谜，它的回答会很奇怪。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;对大数据的影响&lt;/span&gt;&lt;/h1&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;机会：结合客户的行业数据做模型训练，给出贴合场景的深度洞察。瓴羊做的 DaaS 化的应用会真正发挥巨大潜力，我们相比 ChatGPT 最大的优势是客户允许我们安全访问他们的数据，缺点也明显是缺乏智能模型的能力。相信在依托阿里平台，模型能力迟早会补上，那个时候数据给商业带来智能化的潜力会真正被挖掘出来。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;大数据产出的结果更准确，几年前拜访商家的时候，经常听到有经验的商家说，你这个销售额的预测比我加减乘除的准确度都低，这背后的预测模型有很大的进步空间。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;新的交互方式，Ask data 这种交互形式可能真正落地。2年前 Quick BI 就上线过问答式查询，但用户并没有大规模应用，一是用户不知道问什么，我们通过推荐问题来解决；二是给的答案不精确缺乏可解释性。因为对数据指标来讲，每个人可能都有不同的计算方式。通过 ChatGPT 强大的语义理解能力，有望真正解决。现在报表的搭建大多依赖手工，ChatGPT 已经对语言运用自如，通过训练也有希望自动生成报表代码，未来不需要手动搭建仪表板，需要 insight 洞察，提问即回答。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;大数据处理能力更被重视，数据中台统一处理更迫切。很多批评数据中台的声音是因为他们没有做好，数据只有在统一处理才能发挥最大价值，这是前提。Dataphin 帮助很多企业客户打通了数据孤岛，但还有很多客户没有意识到数据孤岛的问题，这些客户会在看到 AI 智能化的强大能力以后快速转向。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;数据安全更加重要，隐私保护能力会构建护城河。苹果在这方面做得特别好，很多人是因为苹果的高安全性变成忠实用户。AI 公司的竞争力会聚集于私有数据的深度，这会加剧公司对数据的获取意愿，行业对安全可定价的数据交易市场需求更迫切。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;数据开发的方式大变化，拖拉拽生成 SQL 等过渡方案受威胁，更强大的脚本语言会保留，但脚本都是自动生成。有了聊天生成 SQL 以后，我为什么还要拖拉拽的中间环节？ChatGPT 目前已经能自动生成 统计计算函数、Power BI 的 DAX 表达式、Excel 公式函数，未来 Dataphin 和 Quick BI 提供的数据计算表达式和计算字段通过训练后，都可以由 AI 来做。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;对前端的影响&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;ChatGPT 会降低 AI 的使用门槛，让前端工程师更容易开发智能应用。也会改变前端的开发方式、用户体验。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;辅助代码编写&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;只要你描述清楚需求，写代码，那是小菜一碟。甚至还可以帮你优化代码。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我随手找了一段拼凑代码给 ChatGPT 来优化。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;帮我优化下这段代码，ChatGPT 能够熟练使用 React hooks，并做性能调优：&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.52&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/QRibyjewM1ICeLP64e9mz1ChDk0eYV038aKZWYNsDlpt2Olcqcj6Co4dbwmEZqI7hZtDrQM3MlGdv8IyQRVsDsA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1650&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;它居然还可以给你解释为什么这么优化！&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.52&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/QRibyjewM1ICeLP64e9mz1ChDk0eYV038aKZWYNsDlpt2Olcqcj6Co4dbwmEZqI7hZtDrQM3MlGdv8IyQRVsDsA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1650&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我感觉像是在和一个熟悉 React 开发规范、《Unix 编程艺术》《设计模式》《Clean Code》的同学在结对编程。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;编码更规范&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;AI 就像一个强大的结对编程助手，可以帮你检查编码的问题。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;比如我们团队 Code review 发现有很多 React hooks 滥用的问题，我看了前端圈很多大会的 topic，感觉讲的并不适用我们的场景，很多实践没有我们团队深入。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;实际上这个问题最好的答案就在 React 官方文档内。我们团队把它浓缩成了几个简单的规则：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;尽量不用 useEffect&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;若是组件内部状态（useState/useMemo/useCallback）监听，若非必要，禁止使用useEffect/useLayoutEffect触发副作用，推荐放到onClick这类事件回调中触发副作用&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;必须使用lint生成hooks的依赖项，否则需要加注释说明&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;useEffect 中若使用了资源类操作（接口请求、订阅/事件、localStorage存储等），则务必返回销毁函数&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;什么时候会用到 useEffect？需要对外部状态有相互影响的逻辑（副作用），才有必要放到useEffect/useLayoutEffect 中&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们一直很重视团队的工程化工具建设，能用工具解决的绝不停留在规范。但以上的规则很难用现有的工具检测出来，真正检测需要理解代码语义。这就是 ChatGPT 可以带来的底层能力变化。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;全栈开发更简单&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;ChatGPT 会让编程语言的上手门槛大幅降低，前后端的融合更加容易。因此新一代的全栈开发会流行。注重性能和开发效率。比如 Next.js 框架。敏捷开发类框架曾经我最喜欢的是 Ruby on Rails，主要是开发确实快，遗憾是 Ruby 弱类型语言没有 TypeScript 这么鲁棒性好。希望 Next.js 能快速达到 Ruby on Rails 的生态，同时能真正解决前后端同构渲染的问题。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;UI 交互方式的变革&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这是对产品和前端的一大挑战和机会。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对于 2B 复杂系统，目前主流的交互方式还是报表展现类、交互分析类、表单流程类、可视化搭建类。但回到客户需求，用户的真实需求是从数据中得到一些 insight，辅助决策。引入 ChatGPT 这类能力后，整个数据开发和分析的流程会大幅缩短。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;并且，在问题明确的场景下，通过 Chat 聊天来寻找答案会很快速，可以很容易扩展到语音问答。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;总结：工程师如何应对 AI 的“冲击”？&lt;/span&gt;&lt;/h1&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5454545454545454&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/QRibyjewM1ICeLP64e9mz1ChDk0eYV038cP7k8KoclwicW5icY0BCFEog3sGWPbaqgs3OFib2ibLCaT6ZWGYfaeGAjA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;814&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;虽然 AI 能回答很多问题，绘出很精美的图片。但 如果你不能发现问题，他无法给你答案。我们会不断地感受到好问题比答案更重要。借助 AI 的能力，普通人一步步体会到了爱因斯坦的洞见。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;面对 AI 的冲击，不必担心 AI 会抢掉你的饭碗。如果停止学习，饭碗迟早会丢。抢掉饭碗的何止AI...另外担心也没用，我们应该成为快速使用 AI 的一波人，这一轮受益最大的会是能快速利用 AI 提高工作效率的人，你不用别人会用。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;比如：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;如果你大量编码，推荐使用 Copilot，物有所值。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;如果你是运营解决方案架构师，要给客户写大段落的文案，可以提供一些关键字并让 ChatGPT 补充细节和润色。但不要只依赖 ChatGPT，它会生成很多空洞的车轱辘话，浪费客户的时间。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;如果你做规划和方案设计，可以使用 ChatGPT 帮你做利弊分析 SWOT 分析，ChatGPT 能帮助你做相对全面的结构化思考，查漏补缺。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;如果你想学习一个新的领域，可以问 ChatGPT 帮你快速上手和了解，然后自己再去了解细节。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;有了场景以后，你还需要掌握一些技巧，来发挥 AI 的潜能：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;提出更好的问题，花时间重点理解客户需求，并设计你的方案。最终限制你的还是你的想象力。AI 可以捏造出美丽的照片和代码，但没有你的想法就缺少意义，没有价值。你的竞争力有如何准确地捕获出用户需求，如何深入理解业务，如何协同上下游。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;学习一些 prompt，准确描述你的想法，会帮助你大幅提效。比如 &lt;span&gt;awesome-chatgpt-prompts&lt;/span&gt;&lt;sup&gt;[1]&lt;/sup&gt;这个 Github 仓库可以帮你学习各种 prompt，如果你是开发者，可以在 &lt;span&gt;chatgpt-engineer-prompts&lt;/span&gt;&lt;sup&gt;[2]&lt;/sup&gt; 找到一些编程相关的使用技巧。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;开放的态度，遇到新的 AI 周边工具去了解，去使用，为己所用。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;总之，AI 可以作为强大的生产力工具，会缩小不同编程语言间的差异，让开发者更容易在不同平台间切换来完成任务。前端工程师升级为终端工程师之后，未来能力边界会进一步扩大。大数据潜力进一步挖掘，数据处理能力更被重视，数据开发的门槛进一步降低。接下来会是 AI 上层应用井喷的一段时间，会有更简洁的用户交互，各行各业最终都会有 AI 深度结合的解决方案，未来可期。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;参考资料&lt;/span&gt;&lt;/h3&gt;&lt;section data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;span&gt;[1]&lt;/span&gt;&lt;p&gt;awesome-chatgpt-prompts: &lt;em&gt;https://github.com/f/awesome-chatgpt-prompts&lt;/em&gt;&lt;/p&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;[2]&lt;/span&gt;&lt;p&gt;chatgpt-engineer-prompts: &lt;em&gt;https://github.com/camsong/chatgpt-engineer-prompts&lt;/em&gt;&lt;/p&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>7b03dec54436eec012d5e4614a51bf96</guid>
<title>张俊林：万字详文解读由 ChatGPT 反思大语言模型的技术精要</title>
<link>https://toutiao.io/k/6ckel6v</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;👆&lt;/span&gt;&lt;span&gt;点击“&lt;/span&gt;&lt;span&gt;博文视点Broadview&lt;/span&gt;&lt;span&gt;”，获取更多书讯&lt;/span&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.05669291338582677&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/PW0wIHxgg3nr1VNxfeqxVOw2nPJHVH4xeZibzPY5F4ibOuOZLMsUMrzIibGB6KMw7EurSKv6DkrtLzuhYdBa30A9Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;635&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img __bg_gif&quot; data-ratio=&quot;0.10027855153203342&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_gif/PW0wIHxgg3npgLu0khEqdibc8UNib9mUOF5Oic7ofxC1krfeqOz5wtoEWMiczOpKGic8QcQLLuSPHicTEZhBqib9LGTeA/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;1077&quot;/&gt;&lt;/p&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;121695&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section data-width=&quot;100%&quot;&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.39866666666666667&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/PW0wIHxgg3kMbkibZkKBXgOJtTn7YFj45cU6M4DRk2QobjlXbBhJszFHhUKZl2199auibEnHPsiatlqibq9XPkjHNw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot;/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;section class=&quot;channels_iframe_wrp&quot;&gt;&lt;mpvideosnap class=&quot;js_uneditable custom_select_card channels_live_iframe&quot; data-pluginname=&quot;videosnap&quot; data-headimgurl=&quot;https://wx.qlogo.cn/finderhead/oLU121BePGlK5gCzY9FXd7w0qDZtIDcFtaSTNicpYrEvHvAA2kfCLKg/0&quot; data-username=&quot;v2_060000231003b20faec8c7e48b1dc6dccd04e436b0771df137f3e0832e32a92192ea2165411c@finder&quot; data-nickname=&quot;博文视点Broadview&quot; data-desc=&quot;将在02月16日 20:00 直播&quot; data-intro=&quot;共话ChatGPT背后技术&quot; data-noticeid=&quot;finderlivenotice-v2_060000231003b20faec8c7e48b1dc6dccd04e436b0771df137f3e0832e32a92192ea2165411c@finder-1676283326520109-256135983&quot; data-type=&quot;live&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;本文作者：张俊林 新浪微博，新技术研发负责人，著有《&lt;span&gt;这就是搜索引擎：核心技术详解&lt;/span&gt;》《&lt;span&gt;大数据日知录：架构与算法&lt;/span&gt;》&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;来源：知乎@张俊林&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;121695&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section data-autoskip=&quot;1&quot;&gt;&lt;p&gt;&lt;span&gt;ChatGPT出现后惊喜或惊醒了很多人。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;惊喜是因为没想到大型语言模型（LLM,Large Language Model）效果能好成这样；惊醒是顿悟到我们对LLM的认知及发展理念，距离世界最先进的想法，差得有点远。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我属于既惊喜又惊醒的那一批，也是典型的中国人，中国人善于自我反思，于是开始反思，而这篇文章正是反思的结果。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;实话实说，国内在LLM模型相关技术方面，此刻，距离最先进技术的差距进一步加大了。技术领先或技术差距这事情，我觉得要动态地以发展的眼光来看。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在Bert出现之后的一到两年间，其实国内在这块的技术追赶速度还是很快的，也提出了一些很好的改进模型，差距拉开的分水岭应该是在 GPT 3.0出来之后，也就是2020年年中左右。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在当时，其实只有很少的人觉察到：GPT 3.0它不仅仅是一项具体的技术，其实体现的是LLM应该往何处去的一个发展理念。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自此之后，差距拉得越来越远，ChatGPT只是这种发展理念差异的一个自然结果。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所以，我个人认为，抛开是否有财力做超大型LLM这个因素，如果单从技术角度看，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;差距主要来自于对LLM的认知以及未来应往何处去的发展理念的不同&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;国内被国外技术甩得越来越远，这个是事实，不承认也不行。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;前阵子网上很多人担忧说国内AI现在处于“危急存亡之秋”，我觉得倒也不至于这么严重。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;君不见，这个世界上，具备这么超前眼光的只有OpenAI一家吗？包括Google在内，其实对于LLM发展理念的理解，明显都落后OpenAI一个身位。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;现实是OpenAI表现过于优秀，把所有人都甩开了，不仅仅是国内。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;我觉得，OpenAI对LLM在理念及相关技术方面，领先国外的Google、DeepMind大约半年到一年的时间，领先国内大概两年左右的时间。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在LLM这个事情上，感觉梯队很明显，Google应该是排在第二位，最能体现Google技术眼光的是PaLM和Pathways，推出时间大概在22年2月到4月间，同一时期，OpenAI推出的却是InstructGPT，从这里就可以看出Google和OpenAI的差距了，至于为何这么说，你看了我后面的正文后大概能理解。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DeepMind之前的重心一直在强化学习攻克游戏和AI for science这些方面，切入LLM其实很晚，应该是21年才开始重视这个方向，目前也处于追赶状态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Meta就更不用说了，重心一直不在LLM上，目前感觉也发力开始追赶。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这还是目前做得最好的一批机构，尚且如此，更何况国内呢？我觉得情有可原。至于OpenAI关于LLM的理念是什么，我在本文的最后一部分，会谈谈我的认知。&lt;/span&gt;&lt;/p&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;7&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;本文梳理自GPT 3.0出现之后的主流LLM技术，在此之前的主流技术可以参考：&lt;/span&gt;&lt;span&gt;&lt;span&gt;《乘风破浪的PTM：两年来预训练模型的技术进展》&lt;/span&gt;&lt;span&gt;（https://zhuanlan.zhihu.com/p/254821426）&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我相信看完这两篇文章，能够让您对LLM领域的技术脉络，LLM技术发展过程中出现过的不同发展理念，乃至未来可能的发展趋势，有比较清晰的认知。当然，很多地方讲的内容是我个人看法，有很大的主观性，错漏难免，所以还请谨慎参考。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;本文试图回答下面一些问题：&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;ChatGPT是否带来了NLP乃至AI领域的研究范式转换？如果是，那会带来怎样的影响？LLM从海量数据中学到了什么知识？LLM又是如何存取这些知识的？随着LLM规模逐步增大，会带来什么影响？什么是In Context Learning?为什么它是一项很神秘的技术？它和Instruct又是什么关系？LLM具备推理能力吗？思维链CoT又是怎么做的？等等&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，相信看完，能让您对这些问题有一个答案。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;首先，在谈LLM技术现状前，先宏观地谈下我心目中的研究范式转换问题。这样，我们才能“先见森林，再见树木”，对具体技术为何会是如此变化有个更清晰的认知。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;117920&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;strong data-original-title=&quot;&quot; title=&quot;&quot;&gt;1&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;潮流之巅：NLP研究范式的转换&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果我们把时间线往前拉得更长一些，回到NLP领域的深度学习时代，在更长时间窗口内观察技术变迁及其影响，可能会更容易看清其中的一些关键节点。我个人认为，在最近10年来NLP领域的技术发展过程中，可能存在两次大的研究范型转换。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;1. 范式转换1.0:从深度学习到两阶段预训练模型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这个范式转换所涵盖的时间范围，大致在深度学习引入NLP领域（2013年左右），到GPT 3.0出现之前（2020年5月左右）。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;在Bert和GPT模型出现之前，NLP领域流行的技术是深度学习模型，而NLP领域的深度学习，主要依托于以下几项关键技术：以大量的改进LSTM模型及少量的改进CNN模型作为典型的特征抽取器；以Sequence to Sequence（或叫encoder-decoder亦可）+Attention作为各种具体任务典型的总体技术框架。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;在这些核心技术加持下，NLP领域深度学习的主要研究目标，如果归纳一下，是如何有效增加模型层深或模型参数容量。就是说，怎么才能往encoder和decoder里不断叠加更深的LSTM或CNN层，来达成增加层深和模型容量的目标。这种努力，尽管确实不断增加了模型层深，但是从解决具体任务的效果角度看，总体而言，不算很成功，或者说和非深度学习方法相对，带来的优势不算大。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;深度学习之所以不够成功，我认为主要原因来自于两个方面：一方面是某个具体任务有限的训练数据总量。随着模型容量的增加，需要靠更大量的训练数据来支撑，否则即使你能把深度做起来，任务效果也做不上去。而在预训练模型出现之前，很明显这是NLP研究领域一个严重问题；另外一个方面是LSTM／CNN特征抽取器，表达能力不够强。意思是就算给你再多的数据也没用，因为你不能有效地吸收数据里蕴含的知识。主要应该是这两个原因，阻碍了深度学习在NLP领域的成功突围。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Bert/GPT这两个预训练模型的出现，无论在学术研究角度看，还是工业应用角度来看，都代表了NLP领域的一个技术飞跃，并带来了整个领域研究范式的转换。这种范式转换带来的影响，体现在两个方面：首先，是部分NLP研究子领域的衰退乃至逐步消亡；其次，NLP不同子领域的技术方法和技术框架日趋统一，在Bert出现后一年左右，技术栈基本收敛到两种技术模式中。关于这两点，我们分头来谈。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;影响一：中间任务的消亡&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;NLP是一个宏观研究领域的统称，里面有五花八门具体的子领域与子方向，如果仔细分析，从任务的性质角度，可以把这些任务分成两大类：一类可以叫做“中间任务”，一类可以称为“最终任务”。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;典型的中间任务包括：中文分词、词性标注、NER、句法分析、指代消解、语义Parser等，这类任务一般并不解决应用中的实际需求，大多数是作为那些解决实际需求任务的中间阶段或者辅助阶段存在的，比如几乎没有需求说，我要一个句法Parser，把这个句子的句法分析树给用户看看，用户不需要看到这些NLP的中间阶段处理结果，他只关心某个具体任务你有没有干好。“最终任务”包括比如文本分类、文本相似性计算、机器翻译、文本摘要等等，有很多。这类任务的特点是每个子领域都解决某个实际需求，任务结果基本能直接呈现给用户，比如用户确实存在给你一句英文，告诉他中文是什么的需求。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;按理说，“中间任务”就不应该出现，而之所以会存在，这是NLP技术发展水平不够高的一种体现。在技术发展早期阶段，因为当时的技术相对落后，很难一步做好有难度的最终任务。比如机器翻译，早期技术要做好机器翻译是很困难的，于是科研人员就把难题分而治之，分解成分词、词性标注、句法分析等各种中间阶段，先把每个中间阶段做好，然后再拼起来完成最终任务，这也是没办法的事情。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;但是自从Bert／GPT出现之后，其实就没有必要做这些中间任务了，因为通过大量数据的预训练，Bert／GPT已经把这些中间任务作为语言学特征，吸收到了Transformer的参数里，此时我们完全可以端到端地直接解决那些最终任务，而无须对这种中间过程专门建模。这里可能争议最大的是中文分词，其实道理也是一样的，哪些字应该组成一个词，这个其实你不用管，让LLM自己当特征去学就行了，只要对于解决任务有帮助，它自然会去学该学的合理分词方式，也未必一定要和我们人类理解的分词规则相同。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;基于以上认知，其实在Bert/GPT一出现，你就应该得出这类NLP的中间阶段的任务，会逐步退出历史舞台这个结论。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;影响二：不同研究方向技术路线的统一&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;在说明具体影响前，我们先讨论下另外一种NLP任务划分方式，这对于理解后面内容有帮助。如果对“最终任务”进一步进行分类，又大致可以分为两大不同类型的任务：自然语言理解类任务和自然语言生成类任务。如果排除掉“中间任务”的话，典型的自然语言理解类任务包括文本分类、句子关系判断、情感倾向判断等，这种任务本质上都是分类任务，就是说输入一个句子（文章），或者两个句子，模型参考所有输入内容，最后给出属于哪个类别的判断。自然语言生成也包含很多NLP研究子方向，比如聊天机器人、机器翻译、文本摘要、问答系统等。生成类任务的特点是给定输入文本，对应地，模型要生成一串输出文本。这两者的差异主要体现在输入输出形式上&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;自从Bert/GPT模型诞生后，出现了明显的技术统一趋向。首先，NLP中不同的子领域，其特征抽取器都逐渐从LSTM/CNN统一到Transformer上。其实，自Bert公开后不久，就应该意识到，这必然会成为技术趋势。至于其原因，在几年前我写的这篇：&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;《放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较》（https://zhuanlan.zhihu.com/p/54743941）&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;其中做了说明和分析，感兴趣的同学可参考。而且，目前Transformer不仅统一了NLP诸多领域，也正在逐步地替换图像处理各种任务中被广泛使用的CNN等其它模型的进程之中，类似的，多模态模型目前也基本都采用了Transformer模型。这种Transformer从NLP出发，攻城略地逐步统一AI越来越多领域的趋势，起始于2020年底出现的Vision Transformer (ViT) ，之后蓬勃发展，到目前已大获成功，且其继续向更多领域拓展的势头会越来越迅猛。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;其次，大多数NLP子领域的研发模式切换到了两阶段模式：模型预训练阶段+应用微调（Fine-tuning）或应用Zero／Few Shot Prompt模式。更准确地说，NLP各种任务其实收敛到了两个不同的预训练模型框架里：对于自然语言理解类任务，其技术体系统一到了以Bert为代表的“双向语言模型预训练+应用Fine-tuning”模式；而对于自然语言生成类任务，其技术体系则统一到了以GPT 2.0为代表的“自回归语言模型（即从左到右单向语言模型）+Zero /Few Shot Prompt”模式。至于为何会分化成两条技术路线，有其必然性，关于这点我们放在后面解释。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这两种模式，看似比较相像，但其背后蕴含了迥异的发展思路，也会导向不同的未来发展方向。不过遗憾的是，我们中的绝大多数人，在当时都低估了GPT 这条发展路线的潜力，而把视觉中心聚焦到了Bert这种模式上。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;2. 范式转换2.0: 从预训练模型走向通用人工智能 （AGI，Artificial General Intelligence）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这个范式转换所涵盖的时间范围，大致在GPT3.0出现之后（20年6月左右），一直到目前为止，我们应该正处于这个范式转换过程中。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;ChatGPT是触发这次范型转换的关键节点，但是在InstructGPT出现之前，其实LLM处于这次范式转换前的一个过渡期。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;过渡期：以GPT 3.0为代表的“自回归语言模型+Prompting”模式占据统治地位&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;前面说过，在预训练模型发展的早期，技术框架收敛到了Bert模式和GPT模式这两种不同的技术范型，而且人们普遍更看好Bert模式一些，相当多数的后续技术改进，都是沿着Bert那条路走的。但是，随着技术的继续发展，你会发现，目前规模最大的LLM模型，几乎清一色都是类似GPT 3.0这种“自回归语言模型+Prompting”模式的，比如GPT 3、PaLM、GLaM、Gopher、Chinchilla、MT-NLG、LaMDA等，没有例外。为什么会这样呢？背后一定有其必然性，我认为可能主要源于两个原因。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;245&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.42314814814814816&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhLb6UW0OrJ7mVQ7NvGJI5ibymcb30Z4OiaMJzGuKzKKwC3z08y3g1uibWQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;首先，&lt;/span&gt;&lt;span&gt;Google的T5模型，在形式上统一了自然语言理解和自然语言生成任务的外在表现形式。如上图所示，标为红色的是个文本分类问题，黄色的是判断句子相似性的回归或分类问题，这都是典型的自然语言理解问题。在T5模型里，这些自然语言理解问题在输入输出形式上和生成问题保持了一致，也就是说，可以把分类问题转换成让LLM模型生成对应类别的字符串，这样理解和生成任务在表现形式就实现了完全的统一。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这说明自然语言生成任务，在表现形式上可以兼容自然语言理解任务，若反过来，则很难做到这一点。这样的好处是：同一个LLM生成模型，可以解决几乎所有NLP问题。而如果仍然采取Bert模式，则这个LLM模型无法很好处理生成任务。既然这样，我们当然倾向于使用生成模型，这是一个原因。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第二个原因，&lt;/span&gt;&lt;span&gt;如果想要以零示例提示语（zero shot prompting）或少数示例提示语（few shot prompting）的方式做好任务，则必须要采取GPT模式。现在已有研究（参考：On the Role of Bidirectionality in Language Model Pre-Training）证明：如果是以fine-tuning方式解决下游任务，Bert模式的效果优于GPT模式；若是以zero shot/few shot prompting这种模式解决下游任务，则GPT模式效果要优于Bert模式。这说明了，生成模型更容易做好zero shot/few shot prompting方式的任务，而Bert模式以这种方式做任务，是天然有劣势的。这是第二个原因。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;但是问题来了：为什么我们要追求zero shot/few shot prompting这种方式来做任务呢？要解释清楚这个问题，我们首先需要搞清楚另外一个问题：什么样的LLM模型，对我们是最理想的？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;383&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.6625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhsHZlIvuKUwsG4OnKD5juqMFpT8hu8R60MTgX1I6icbm95ibhMCEic6cCQ/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;上图展示了一个理想的LLM该有的样子。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先，&lt;/span&gt;&lt;span&gt;LLM应该具备强大的自主学习能力。假设我们把世界上能获得的所有文本或者图片等不同类型的数据喂给它，它应该能够自动从中学习到里面包含的所有知识点，学习过程不需要人的介入，并且能灵活应用所学知识，来解决实际问题。因为数据是海量的，要吸收所有知识，就要非常多的模型参数来存储知识，所以这个模型必然会是一个巨无霸模型。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;其次，&lt;/span&gt;&lt;span&gt;LLM应该能解决NLP任何子领域的问题，而不仅支持有限领域，甚至它应该可以响应NLP之外其它领域的问题，最好是任意领域的问题都能得到很好地回答。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;再者，&lt;/span&gt;&lt;span&gt;当我们使用LLM解决某个具体领域问题的时候，应该用我们人类习惯的表达方式，就是说LLM应该理解人类的命令。这体现出让LLM适配人，而不是反过来，让人去适配LLM模型。人适配LLM的典型例子，比如绞尽脑汁去尝试各种不同的prompt，以试图找到好的提示语，才能很好地解决手头问题。关于这点，上图在人类和LLM交互的接口层，举了几个例子，说明什么是好的人使用LLM模型的接口形式。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;看完这个理想中的LLM，我们再回头解释上面遗留的问题：&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;为什么我们要追求zero shot/few shot prompting这种方式来做任务呢？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有两个原因。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第一，&lt;/span&gt;&lt;span&gt;这个LLM模型规模必然非常巨大，有能力作出这个模型，或改动这个模型参数的机构必然很少。而任务需求方是千千万万的中小机构甚至是个人，就算你把模型开源出来，他们也无力部署这个模型，更不用说再用Fine-tuning这种模式去修改模型参数了。所以，我们应该追求不修正模型参数，就能让任务需求方完成任务的方式，也就是应该采取prompt模式完成任务，而非Fine-tuning模式（由此可看出，soft prompting技术方向是违背这个发展趋势的）。模型制作方则将LLM作成公用服务，以LLM as Service的模式运行。作为服务支持方，考虑到千变万化的用户需求，所以LLM模型制作方更要追求让LLM能完成尽可能多类型的任务，这是附带的影响，也是为何超级大模型一定会追求走向AGI的现实因素。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第二，&lt;/span&gt;&lt;span&gt;zero shot prompting也好，few shot prompting也好，甚至促进LLM推理能力的思维链（CoT,Chain of Thought）Prompting也好，就是上图中接口层中的现有技术。具体而言，zero shot prompting的初衷，其实就是人类和LLM的理想接口，直接用人类所习惯的任务表述方式让LLM做事情，但是发现LLM并不能很好地理解，效果也不好。经过继续研究，转而发现：对于某项任务，如果给LLM几个示例，用这些示例来代表任务描述，效果会比zero shot prompting好，于是大家都去研究更好的few shot prompting技术。可以理解为，本来我们希望LLM能够用人类常用的命令方式来执行某个任务，但是目前技术还做不到，所以退而求其次，用这些替代技术来表达人类的任务需求。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果理解了上述逻辑，很容易得出如下结论：few shot prompting（也被称为In Context Learning）只是一种过渡时期的技术。如果我们能够更自然地去描述一个任务，而且LLM可以理解，那么，我们肯定会毫不犹豫地抛弃这些过渡期的技术，原因很明显，用这些方法来描述任务需求，并不符合人类的使用习惯。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这也是为何我将GPT 3.0+Prompting列为过渡期技术的原因，ChatGPT的出现，改变了这个现状，用Instruct取代了Prompting，由此带来新的技术范式转换，并产生若干后续影响。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;影响一：让LLM适配人的新型交互接口&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;在理想LLM的背景下，我们再来看ChatGPT，能更好理解它的技术贡献。ChatGPT应该是目前所有的现有技术里，最接近理想LLM的技术方法。如果归纳下ChatGPT最突出特点的话，我会用下面八个字：“能力强大，善解人意”。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;“能力强大”这一点，我相信应该主要归功于ChatGPT所依托的基础LLM GPT3.5。因为ChatGPT 尽管加入了人工标注数据，但是量级只有数万，这个规模的数据量，和训练GPT 3.5模型使用的几千亿token级别的数据量相比，包含的世界知识（数据中包含的事实与常识）可谓沧海一粟，几可忽略，基本不会对增强GPT 3.5的基础能力发挥什么作用。所以它的强大功能，应该主要来自于隐藏在背后的GPT 3.5。GPT 3.5对标理想LLM模型中的那个巨无霸模型。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;那么，ChatGPT向GPT 3.5模型注入新知识了吗？应该是注入了，这些知识就包含在几万人工标注数据里，不过注入的不是世界知识，而是人类偏好知识。所谓“人类偏好”，包含几方面的含义：首先，是人类表达一个任务的习惯说法。比如，人习惯说：“把下面句子从中文翻译成英文”，以此表达一个“机器翻译”的需求，但是LLM又不是人，它怎么会理解这句话到底是什么意思呢？你得想办法让LLM理解这句命令的含义，并正确执行。所以，ChatGPT通过人工标注数据，向GPT 3.5注入了这类知识，方便LLM理解人的命令，这是它“善解人意”的关键。其次，对于什么是好的回答，什么是不好的回答，人类有自己的标准，例如比较详细的回答是好的，带有歧视内容的回答是不好的，诸如此类。这是人类自身对回答质量好坏的偏好。人通过Reward Model反馈给LLM的数据里，包含这类信息。总体而言，ChatGPT把人类偏好知识注入GPT 3.5，以此来获得一个听得懂人话、也比较礼貌的LLM。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;可以看出，ChatGPT的最大贡献在于：基本实现了理想LLM的接口层，让LLM适配人的习惯命令表达方式，而不是反过来让人去适配LLM，绞尽脑汁地想出一个能Work的命令（这就是instruct技术出来之前，prompt技术在做的事情），而这增加了LLM的易用性和用户体验。是InstructGPT/ChatGPT首先意识到这个问题，并给出了很好的解决方案，这也是它最大的技术贡献。相对之前的few shot prompting，它是一种更符合人类表达习惯的人和LLM进行交互的人机接口技术。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;而这必将启发后续的LLM模型，继续在易用人机接口方面做进一步的工作，让LLM更听话。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;影响二：很多NLP子领域不再具备独立研究价值&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;就NLP领域而言，这次范式转换，意味着很多目前独立存在的NLP研究领域，将被纳入LLM的技术体系，进而不再独立存在，逐步消失。经过第一次范式转换，尽管NLP中很多“中间任务”，继续作为独立研究领域存在不再必要，但是大多数“最终任务”，仍然是以独立研究领域存在的，只是切换成在“预训练+fine-tuning”框架下，面对领域独有问题，陆续提出新的改进方案。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;目前研究表明，很多NLP任务，随着LLM模型规模增长，效果会大幅提升。据此，我觉得可得到如下推论：大多数某领域所谓“独有”的问题，大概率只是缺乏领域知识导致的一种外在表象，只要领域知识足够多，这个所谓领域独有的问题，就可以被很好地解决掉，其实并不需要专门针对某个具体领域问题，冥思苦想去提出专用解决方案。也许AGI的真相超乎意料地简单：你只要把这个领域更多的数据交给LLM，让它自己学习更多知识即可。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;在这个背景下，同时，ChatGPT证明了我们现在是可以直接去追求理想LLM模型的，那么，未来的技术发展趋势应该是：追求规模越来越大的LLM模型，通过增加预训练数据的多样性，来涵盖越来越多的领域，LLM自主从领域数据中通过预训练过程学习领域知识，随着模型规模不断增大，很多问题随之得到解决。研究重心会投入到如何构建这个理想LLM模型，而非去解决某个领域的具体问题。这样，越来越多NLP的子领域会被纳入LLM的技术体系，进而逐步消失。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;我认为，判断某个具体领域是否该立即停止独立研究，其判断标准可采取以下两种方法，占其一即可：第一，判断某个任务，是否LLM的研究效果超过人类表现，对于那些LLM效果超过人类的研究领域，已无独立研究的必要。举个例子，GLUE与SuperGLUE测试集合里的很多任务，目前LLM效果已超过人类表现，与这个数据集合密切相关的研究领域，其实就没有继续独立存在的必要。第二，对比两种模式的任务效果，第一种模式是用较大的领域专用数据进行Fine-tuning，第二种是few-shot prompting或instruct-based方法。如果第二种方法效果达到或超过第一种方法，则意味着这个领域没有继续独立存在的必要性。如果用这个标准来看，其实很多研究领域，目前fine-tuning效果还是占优的（因为这种模式领域训练数据量大），看似还可独立存在。但是考虑到很多任务随着模型规模增大，few shot prompting效果持续增长，随着更大模型的出现，这个拐点很可能短期就会达到。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果上述猜测成立，将意味着如下残酷事实：对于很多NLP领域的研究人员，将面临往何处去的选择，是继续做领域独有问题呢？还是放弃这种看似前途不大的方式，转而去建设更好的LLM？如果选择转向去建设LLM，又有哪些机构有能力、有条件去做这个事情呢？你对这个问题的回答会是什么呢？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;影响三：更多NLP之外的研究领域将被纳入LLM技术体系&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果站在AGI的视角，参照之前描述的理想LLM模型，它所能完成的任务，不应局限于NLP领域，或某一两个学科领域，理想中的LLM应该是领域无关的通用人工智能模型，它现在在某一两个领域做得好，不代表只能做这些任务。ChatGPT的出现，证明了现在这个时期，我们去追求AGI是有可行性的，而现在是抛开“领域学科”这个思维束缚的时候了。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;ChatGPT除了展示出以流畅的对话形式解决各种NLP任务外，也具备强大的代码能力。很自然的，之后越来越多其它的研究领域，也会被逐步纳入LLM体系中，成为通用人工智能的一部分。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;378&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.6546296296296297&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/zHbzQPKIBPg14icntwicW9XhD0mCTz1Bhh01OnWf1UOG6p1NRFJDpyjno5h56DXF7klmKhHiaa44GhHOTJdEQ5GXA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;LLM从NLP向外进行领域拓展，一个自然的选择就是图像处理及多模态相关任务。目前已经有些工作在尝试把多模态融入，让LLM成为一个支持多模态输入输出的通用人机接口，典型的例子包括DeepMind的Flamingo和微软的“Language Models are General-Purpose Interfaces”，上图展示了这种方式的概念结构。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;我的判断是无论是图像还是多模态，未来被融入LLM成为好用的功能，可能比我们想象的进度要慢。主要原因在于：尽管图像领域最近两年也一直在模仿Bert预训练的路子，尝试引入自监督学习，释放模型自主从图像数据中学习知识的能力，典型技术就是“对比学习”和MAE，这是两条不同的技术路线。然而，从目前效果来看，尽管取得了很大的技术进步，但貌似这条路尚未走通，这体现在图像领域预训练模型应用到下游任务，带来的效果收益，远不如Bert或GPT应用在NLP下游任务那样显著。所以，图像预处理模型仍需深入探索，以释放图像数据的潜力，而这会迟滞它们被统一到LLM大模型的时间。当然，如果哪天这条路被趟通，大概率会复现NLP领域目前的局面，就是图像处理各个研究子领域可能会逐步消失，被融入到大型LLM中来，直接完成终端任务。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;除了图像与多模态，很明显，其它领域也会逐渐被纳入到理想LLM中来，这个方向方兴未艾，是具备高价值的研究主题。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;以上是我对范式转换的个人思考，接下来，我们来梳理下GPT 3.0之后LLM模型的主流技术进展。如理想LLM模型所示，相关的技术其实可以分为两大类；一类是关于LLM模型如何从数据中吸收知识，也包括模型规模增长对LLM吸收知识能力带来的影响；第二类是关于人如何使用LLM内在能力来解决任务的人机接口，包括In Context Learning和Instruct两种模式。思维链（CoT）prompting这种LLM推理技术，本质上也属于In Context Learning，因为比较重要，我就把它们单独拎出来讲一下。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;117920&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;strong data-original-title=&quot;&quot; title=&quot;&quot; data-num=&quot;2&quot;&gt;2&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;学习者：从无尽数据到海量知识&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;从目前研究结果看，Transformer是足够强大的特征抽取器，尚不需要做特别的改进。那么通过预训练过程，Transformer学到了什么？知识是如何存取的？我们又如何修正错误知识？本节讲述这方面的研究进展。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. 求知之路：LLM学到了什么知识&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;LLM从海量自由文本中学习了大量知识，如果把这些知识做粗略分类的话，可以分为语言类知识和世界知识两大类。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;语言类知识指的是词法、词性、句法、语义等有助于人类或机器理解自然语言的知识。关于LLM能否捕获语言知识有较长研究历史，自从Bert出现以来就不断有相关研究，很早就有结论，各种实验充分证明LLM可以学习各种层次类型的语言学知识，这也是为何使用预训练模型后，各种语言理解类自然语言任务获得大幅效果提升的最重要原因之一。另外，各种研究也证明了浅层语言知识比如词法、词性、句法等知识存储在Transformer的低层和中层，而抽象的语言知识比如语义类知识，广泛分布在Transformer的中层和高层结构中。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;世界知识指的是在这个世界上发生的一些真实事件（事实型知识，Factual Knowledge），以及一些常识性知识(Common Sense Knowledge)。比如“拜登是现任美国总统”、“拜登是美国人”、“乌克兰总统泽连斯基与美国总统拜登举行会晤”，这些都是和拜登相关的事实类知识；而“人有两只眼睛”、“太阳从东方升起”这些属于常识性知识。关于LLM模型能否学习世界知识的研究也有很多，结论也比较一致：LLM确实从训练数据中吸收了大量世界知识，而这类知识主要分布在Transformer的中层和高层，尤其聚集在中层。而且，随着Transformer模型层深增加，能够学习到的知识数量逐渐以指数级增加（可参考：BERTnesia: Investigating the capture and forgetting of knowledge in BERT）。其实，你把LLM看作是一种以模型参数体现的隐式知识图谱，如果这么理解，我认为是一点问题也没有的。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;“When Do You Need Billions of Words of Pre-training Data?”这篇文章研究了预训练模型学习到的知识量与训练数据量的关系，它的结论是：对于Bert类型的语言模型来说，只用1000万到1亿单词的语料，就能学好句法语义等语言学知识，但是要学习事实类知识，则要更多的训练数据。这个结论其实也是在意料中的，毕竟语言学知识相对有限且静态，而事实类知识则数量巨大，且处于不断变化过程中。而目前研究证明了随着增加训练数据量，预训练模型在各种下游任务中效果越好，这说明了从增量的训练数据中学到的更主要是世界知识。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2. 记忆之地：LLM如何存取知识&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;由上可知，LLM确实从数据中学到了很多语言类及世界知识。那么，对于某条具体的知识，LLM把它存储到了哪里？又是如何提取出来的？这也是一个有意思的问题。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;显然，知识一定存储在Transformer的模型参数里。从Transformer的结构看，模型参数由两部分构成：多头注意力（MHA）部分占了大约参数总体的三分之一，三分之二的参数集中在FFN结构中。MHA主要用于计算单词或知识间的相关强度，并对全局信息进行集成，更可能是在建立知识之间的联系，大概率不会存储具体知识点，那么很容易推论出LLM模型的知识主体是存储在Transformer的FFN结构里。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;306&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.5296875&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1Bhhz9D8F37dFxtficMzklBInxDowwL6b5o7DP4gWc5c31OBaBKUZnEloSA/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;但这样的定位，粒度还是太粗，无法很好回答具体某条知识是如何存储与提取的，比如 “中国的首都是北京”这条知识，以三元组表达就是&amp;lt;北京，is-capital-of，中国&amp;gt;，其中“is-capital-of”代表实体间关系。这条知识它存储在LLM的哪里呢？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;“Transformer Feed-Forward Layers Are Key-Value Memories”给出了一个比较新颖的观察视角，它把Transformer的FFN看成存储大量具体知识的Key-Value存储器。如上图所示（图左是原始论文图，其实不太好理解，可以看做了注释的图右，更好理解些），FFN的第一层是个MLP宽隐层，这是Key层；第二层是MLP窄隐层，是Value层。FFN的输入层其实是某个单词对应的MHA的输出结果Embedding，也就是通过Self Attention，将整个句子有关的输入上下文集成到一起的Embedding，代表了整个输入句子的整体信息。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Key层的每个神经元节点，记载了一对&amp;lt;Key,Value&amp;gt;信息。比如对于上图中FFN第一个隐层的第  个节点  ，也许就是它记载了&amp;lt;北京，is-capital-of，中国&amp;gt;这条知识。  节点对应的key向量，其实指的是节点  和输入层每个节点的权重向量；而对应的Value向量，指的是节点  和FFN第二层的Value层每个节点形成连接的权重向量。每个神经元的Key向量，用于识别输入中的某种语言或者知识模式，是一种模式探测器。如果输入中包含它要检测的某种模式，那么输入向量和  节点的key权重进行向量内积计算，加上Relu，形成  的大数值响应，意味着  检测到了这个模式，于是再把这个响应值，通过  节点的Value权重向量向FFN第二层传播。这等价于将Value向量的值，用响应值加权，然后传递并体现到第二层Value层每个节点的输出上。如此这般，FFN的正向传播计算过程，看起来就像是通过Key检测到某种知识模式，然后取出对应的Value，并把Value体现在FFN的第二层输出上。当然，FFN第二层每个节点，会收集FFN的Key层所有节点信息，所以是一种混合响应，而Value层所有节点的混合响应，可以解读为代表输出单词的概率分布信息。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;听着可能还是比较复杂，我们用个极端的例子来说明。我们假设上图的节点  就是记载&amp;lt;北京，is-capital-of，中国&amp;gt;这条知识的Key-Value存储器，它的Key向量，用于检测”中国的首都是…”这个知识模式，它的Value向量，基本存储了与单词“北京”的Embedding比较接近的向量。当Transformer的输入是“中国的首都是[Mask]”的时候，  节点从输入层探测到这个知识模式，所以产生较大的响应输出。我们假设Key层其它神经元对这个输入都没有任何响应，那么对应的Value层的节点，其实只会接收到“北京”这个Value对应的单词embedding，并通过  的大响应值，进行了进一步的数值放大。于是，Mask位置对应的输出，就自然会输出“北京”这个单词。基本就是这么个过程，看着很复杂，其实很简单。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;而且这篇文章还指出，Transformer低层对句子的表层模式作出反应，高层对语义模式作出反应，就是说低层FFN存储词法、句法等表层知识，中层和高层存储语义及事实概念知识，这和其它研究结论是一致的。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;要我猜，把FFN看成Key-Value存储器这种思路，很可能不是最终的正确答案，但是距离最终正确答案的距离，估计也不太远。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3. 知识涂改液：如何修正LLM里存储的知识&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;既然我们已知具体的某条世界知识存储在某个或者某些FFN节点的参数里，自然会引发另外一个问题：我们能否修正LLM模型里存储的错误或者过时的知识呢？比如对于问题：“英国的现任首相是谁？”鉴于近年来英国首相频繁更迭，你猜LLM更倾向输出“鲍里斯”还是更青睐“苏纳克”？很明显训练数据中包含“鲍里斯”的数据会更多，这种情况很大可能LLM会给出错误回答，于是我们就有修正LLM里存储的过时知识的必要性。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果归纳下，目前有三类不同方法来修正LLM里蕴含的知识：&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第一类方法从训练数据的源头来修正知识。“Towards Tracing Factual Knowledge in Language Models Back to the Training Data”这篇文章的研究目标是：对于指定的某条知识，我们是否可以定位到是哪些训练数据导致LLM学会了这条知识？答案是肯定的，这意味着我们可以逆向追踪到某条知识对应的训练数据源头。如果利用这项技术，假设我们想要删除某条知识，则可首先定位到其对应的数据源头，删除数据源，然后重新预训练整个LLM模型，这样即可达成删除LLM中相关知识的目的。但是这里有个问题，如果修正一小部分知识，我们就需要重新做一次模型预训练，这样做明显成本太高。所以这种方法不会太有发展前景，可能比较适合那种对于某个特定类别数据的一次性大规模删除场合，不适合少量多次的常规知识修正场景，比如可能比较适合用来做去除偏见等去toxic内容的处理。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第二类方法是对LLM模型做一次fine-tuning来修正知识。一个直观能想到的方法是：我们可以根据要修正成的新知识来构建训练数据，然后让LLM模型在这个训练数据上做fine-tuning，这样指导LLM记住新的知识，遗忘旧的知识。这个方法简单直观，但是也有一些问题，首先它会带来灾难遗忘问题，就是说除了忘掉该忘的知识，还忘掉了不该忘的知识，导致这么做了之后有些下游任务效果下降。另外，因为目前的LLM模型规模非常大，即使是做fine-tuning，如果次数频繁，其实成本也相当高。对这种方法感兴趣的可以参考“Modifying Memories in Transformer Models”。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;另外一类方法直接修改LLM里某些知识对应的模型参数来修正知识。假设我们想要把旧知识&amp;lt;英国，现任首相，鲍里斯&amp;gt;，修正到&amp;lt;英国，现任首相，苏纳克&amp;gt;。首先我们想办法在LLM模型参数中，定位到存储旧知识的FFN节点，然后可以强行调整更改FFN中对应的模型参数，将旧知识替换成新的知识。可以看出，这种方法涉及到两项关键技术：首先是如何在LLM参数空间中定位某条知识的具体存储位置；其次是如何修正模型参数，来实现旧知识到新知识的修正。关于这类技术的细节，可以参考“Locating and Editing Factual Associations in GPT”和“Mass-Editing Memory in a Transformer”。理解这个修正LLM知识的过程，其实对于更深入理解LLM的内部运作机制是很有帮助的。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;117920&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;strong data-original-title=&quot;&quot; title=&quot;&quot; data-num=&quot;3&quot;&gt;3&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;规模效应&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;：当LLM越来越大时会发生什么&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;我们知道，近年来，LLM模型规模在快速增长，目前效果最好的LLM模型，其参数规模大都超过了千亿（100B）参数规模。比如，OpenAI的GPT 3的规模为175B，Google的LaMDA规模为137B，PaLM的规模为540B，DeepMind的Gogher规模为280B等，不一而足。国内也有中文巨型模型，比如智源GLM规模130B，华为“盘古”规模200B，百度“文心”规模260B，浪潮“源1.0”规模245B。那么，一个很自然的问题就是：随着LLM模型规模不断增长，会发生些什么呢？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;预训练模型的应用往往是两阶段的：预训练阶段，及具体场景应用阶段。在预训练阶段，其优化目标是交叉熵，对GPT这种自回归语言模型来说，也就是看LLM是否正确预测到了下一个单词；而场景应用阶段，一般要看具体场景的评价指标。一般我们的直觉是：如果LLM模型在预训练阶段的指标越好，自然它解决下游任务的能力就越强。然而，事实并非完全如此。现有研究已证明，预训练阶段的优化指标确实和下游任务表现出正相关关系，但是并非完全正相关。也就是说，只看预训练阶段的指标，来判断一个LLM模型是否够好，这是不够的。基于此，我们分头来看在这两个不同阶段，随着LLM模型增大，有什么影响。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;246&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.4265625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhUSmdVPCfuzic5qS5FPkrCdFEmNWI0esic3qTgKww6SxnZtsVojD7QISQ/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;首先，我们先看在预训练阶段，随着模型规模逐步增大，会发生什么。OpenAI在“Scaling Laws for Neural Language Models”中专门研究了这个问题，并提出LLM模型所遵循的“伸缩法则”（scaling law）。如上图所示，这个研究证明：当我们独立增加训练数据量、模型参数规模或者延长模型训练时间（比如从1个Epoch到2个Epoch），预训练模型在测试集上的Loss都会单调降低，也就是说模型效果越来越好。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;既然三个因素都重要，那么我们在实际做预训练的时候，就有一个算力如何分配的决策问题：假设用于训练LLM的算力总预算（比如多少GPU小时或者GPU天）给定，那么是应该多增加数据量、减少模型参数呢？还是说数据量和模型规模同时增加，减少训练步数呢？此消彼长，某个要素规模增长，就要降低其它因素的规模，以维持总算力不变，所以这里有各种可能的算力分配方案。最终OpenAI选择了同时增加训练数据量和模型参数，但是采用早停策略(early stopping)来减少训练步数的方案。因为它证明了：对于训练数据量和模型参数这两个要素，如果只单独增加其中某一个，这不是最好的选择，最好能按照一定比例同时增加两者，它的结论是优先增加模型参数，然后才是训练数据量。假设用于训练LLM的算力总预算增加了10倍，那么应该增加5.5倍的模型参数量，1.8倍的训练数据量，此时模型效果最佳。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;DeepMind的一项研究（参考：Training Compute-Optimal Large Language Models）更深入地探究了这个问题，其基本结论和OpenAI的结论差不多，比如确实需要同时增加训练数据量和模型参数，模型效果才会更好。而很多大模型在做预训练的时候，并没有考虑这一点，很多LLM大模型只是单调增加模型参数，而固定住了训练数据量，这个做法其实是不对的，限制了LLM模型的潜力。但是它修正了两者的比例关系，认为训练数据量和模型参数是同等重要的，也就是说，假设用于训练LLM的算力总预算增加了10倍，那么应该增加3.3倍的模型参数量，3.3倍的训练数据量，这样模型效果才最好。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这意味着：增加训练数据量的重要性，比我们之前所认为的，还要重要。基于这个认知，DeepMind在设计Chinchilla模型时，在算力分配上选择了另外一种配置：对标数据量300B、模型参数量280B的Gopher模型，Chinchilla选择增加4倍的训练数据，但是将模型参数降低为Gopher的四分之一，大约为70B。但是无论预训练指标，还是很多下游任务指标，Chinchilla效果都要优于规模更大的Gopher。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这带给我们如下启示：我们可以选择放大训练数据，并同比例地减少LLM模型参数，以达到在不降低模型效果的前提下，极大缩小模型规模的目的。缩小模型规模有很多好处，比如在应用的时候，推理速度会快很多等，无疑这是一个很有前途的LLM发展路线。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;以上是从预训练阶段来看模型规模的影响，如果从LLM解决下游具体任务效果的角度来看，随着模型规模增大，不同类型的任务有不同的表现，具体而言，有以下三类情况。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;306&quot; data-backw=&quot;576&quot; data-ratio=&quot;0.53125&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhYtuNYnvxN7dlBcVu7emwXtupU0icjBBlSBdP15QjdaMfQx64a03t3IA/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第一类任务完美体现了LLM模型的scaling law，就是说随着模型规模逐步放大，任务的表现越来越好，如上图里的（a）图所示。这类任务通常符合如下共性：它们往往都是知识密集型任务，也就是说如果LLM模型包含的知识量越多，这类任务表现越好。而很多研究已经证明越大的LLM模型学习效率越高，也就是说相同训练数据量，模型越大任务效果越好，说明面对的即使是同样的一批训练数据，更大的LLM模型相对规模小一些的模型，从中学到了更多的知识。更何况一般情况下，在增大LLM模型参数的时候，往往会同步增加训练数据量，这意味着大模型可以从更多数据中学习更多的知识点。这些研究可以很好地解释上图，为何随着模型规模增大，这些知识密集型的任务效果越来越好。大多数传统的自然语言理解类任务，其实都属于这种知识密集型任务，而很多任务在近两年获得了极大的效果提升，甚至超过了人类表现。很明显，这大概率是LLM模型的规模增长带来的，而非归功于某项具体的技术改进。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第二类任务展现出LLM具备某种“涌现能力（Emergent Ability）”，如上图（b）所示。所谓“涌现能力”，指的是当模型参数规模未能达到某个阀值时，模型基本不具备解决此类任务的任何能力，体现为其性能和随机选择答案效果相当，但是当模型规模跨过阀值，LLM模型对此类任务的效果就出现突然的性能增长。也就是说，模型规模是解锁(unlock)LLM新能力的关键，随着模型规模越来越大，会逐渐解锁LLM越来越多的新能力。这是个很神奇的现象，因为它意味着如下让人对未来可报乐观预期的可能：或许很多任务，目前LLM还不能很好地解决，甚至站在现在这个时刻的我们看起来，LLM完全没有能力解决这类任务，但因LLM具备“涌现能力”，所以如果我们继续推大模型，也许某一天它的这项能力就被突然解锁了。LLM模型的规模增长会给我们带来意想不到的精彩礼物。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;“Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models”这篇文章指出，这类体现出“涌现能力”的任务也有一些共性：这些任务一般由多步骤构成，要解决这些任务，往往需要先解决多个中间步骤，而逻辑推理能力在最终解决这类任务中发挥重要作用。思维链（Chain of Thought）Prompting是典型的增强LLM推理能力的技术，能大幅提升此类任务的效果，关于CoT技术，在随后小节内容会做解释，此处暂不展开。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;问题是，为何LLM会出现这种“涌现能力”现象呢？上述文章以及“Emergent Abilities of Large Language Models”给出了几个可能的解释：&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;一种可能解释是有些任务的评价指标不够平滑。比如说有些生成任务的判断标准，它要求模型输出的字符串，要和标准答案完全匹配才算对，否则就是0分。所以，即使随着模型增大，其效果在逐步变好，体现为输出了更多的正确字符片段，但是因为没有完全对，只要有任何小错误都给0分，只有当模型足够大，输出片段全部正确才能得分。也就是说，因为指标不够平滑，所以不能体现LLM其实正在逐步改善任务效果这一现实，看起来就是“涌现能力”这种外在表现。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;另外一种可能的解释是：有些任务由若干中间步骤构成，随着模型规模增大，解决每个步骤的能力也在逐步增强，但是只要有一个中间步骤是错的，最终答案就是错的，于是也会导致这种表面的“涌现能力”现象。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;当然，上面的解释目前还都是猜想，至于为何LLM会出现这种现象，还需要进一步更深入的研究。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;279&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.4828125&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1Bhha7PbMy99ghqge10hVQCMXQ9Xw0UmKaicQjXQEWz8Zh7cjHXmS9uh61Q/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;还有少部分任务，随着模型规模增长，任务的效果曲线展现出U形特性：随着模型规模逐渐变大，任务效果逐渐变差，但是当模型规模进一步增长，则效果开始越来越好，呈现出U形增长趋势，如上图所示的粉红色PaLM模型在两个任务上的指标走势。为何这些任务表现得如此特殊呢？“Inverse scaling can become U-shaped”这篇文章给出了一种解释：这些任务，内部其实隐含了两种不同类型的子任务，一种是真正的任务，另外一种是“干扰任务（distractor task）”。当模型规模小的时候，无法识别任意一种子任务，所以模型的表现跟随机选择答案差不多，当模型增长到中等规模的时候，主要执行的是干扰任务，所以对真正的任务效果有负面影响，体现为真正任务效果的下降，而当进一步增加模型规模，则LLM可以忽略干扰任务，执行真正的任务，体现为效果开始增长。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;对于那些随着模型规模增大，效果一直下降的任务，如果采用思维链（CoT）Prompting，则部分任务的表现转换为遵循Scaling law，即模型规模越大效果越好，而其它任务则转换为U性增长曲线。这其实侧面说明了：此类任务应属于推理类型的任务，所以加入CoT后任务表现会发生质的变化。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;117920&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;strong data-original-title=&quot;&quot; title=&quot;&quot; data-num=&quot;4&quot;&gt;4&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;人机接口：从In Context Learning到Instruct理解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;一般我们经常提到的人和LLM的接口技术包括：zero shot prompting、few shot prompting、In Context Learning，以及Instruct。这些其实都是表达某个具体任务的描述方式。不过如果你看文献，会发现叫法比较乱。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;其中Instruct 是ChatGPT的接口方式，就是说人以自然语言给出任务的描述，比如“把这个句子从中文翻译成英文”，类似这种。zero shot prompting我理解其实就是现在的Instruct的早期叫法，以前大家习惯叫zero shot，现在很多改成叫Instruct。尽管是一个内涵，但是具体做法是两种做法。早期大家做zero shot prompting，实际上就是不知道怎么表达一个任务才好，于是就换不同的单词或者句子，反复在尝试好的任务表达方式，这种做法目前已经被证明是在拟合训练数据的分布，其实没啥意思。目前Instruct的做法则是给定命令表述语句，试图让LLM理解它。所以尽管表面都是任务的表述，但是思路是不同的。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;而In Context Learning和few shot prompting意思类似，就是给LLM几个示例作为范本，然后让LLM解决新问题。我个人认为In Context Learning也可以理解为某项任务的描述，只是Instruct是一种抽象的描述方式，In Context Learning是一种例子示范的例子说明法。当然，鉴于目前这几个叫法用的有点乱，所以上述理解仅代表个人看法。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;所以我们此处只对In Context Learning和Instruct进行介绍，不再提zero shot和few shot了。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. 神秘的In Context Learning&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果你细想，会发现In Context Learning是个很神奇的技术。它神奇在哪里呢？神奇在你提供给LLM几个样本示例  ，然后给它  ，LLM竟然能够成功预测对应的  。听到这你会反问：这有什么神奇的呢？Fine-tuning不就是这样工作的吗？你要这么问的话，说明你对这个问题想得还不够深入。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;p&gt;&lt;span&gt;如果你细想，会发现In Context Learning是个很神奇的技术。它神奇在哪里呢？神奇在你提供给LLM几个样本示例 &lt;span/&gt;，然后给它 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -442 1949.9 650&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;msub&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;78&quot; d=&quot;M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;TeXAtom&quot; transform=&quot;translate(572, -150) scale(0.707)&quot; data-mjx-texclass=&quot;ORD&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;6E&quot; d=&quot;M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(600, 0)&quot;&gt;&lt;path data-c=&quot;2B&quot; d=&quot;M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mn&quot; transform=&quot;translate(1378, 0)&quot;&gt;&lt;path data-c=&quot;31&quot; d=&quot;M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt;，LLM竟然能够成功预测对应的 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -442 1867.9 650&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;msub&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;79&quot; d=&quot;M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;TeXAtom&quot; transform=&quot;translate(490, -150) scale(0.707)&quot; data-mjx-texclass=&quot;ORD&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;6E&quot; d=&quot;M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(600, 0)&quot;&gt;&lt;path data-c=&quot;2B&quot; d=&quot;M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mn&quot; transform=&quot;translate(1378, 0)&quot;&gt;&lt;path data-c=&quot;31&quot; d=&quot;M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt;。听到这你会反问：这有什么神奇的呢？Fine-tuning不就是这样工作的吗？你要这么问的话，说明你对这个问题想得还不够深入。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;322&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.5578125&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhBS1erdCwDRLPtEBNHWWiamh8A9zmQvJN5vOgafHMfEsZtCgtUiadQwaA/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;Fine-tuning和In Context Learning表面看似都提供了一些例子给LLM，但两者有质的不同（参考上图示意）：Fine-tuning拿这些例子当作训练数据，利用反向传播去修正LLM的模型参数，而修正模型参数这个动作，确实体现了LLM从这些例子学习的过程。但是，In Context Learning只是拿出例子让LLM看了一眼，并没有根据例子，用反向传播去修正LLM模型参数的动作，就要求它去预测新例子。既然没有修正模型参数，这意味着貌似LLM并未经历一个学习过程，如果没有经历学习过程，那它为何能够做到仅看一眼，就能预测对新例子呢？这正是In Context Learning的神奇之处。这是否让你想起了一句歌词：“只是因为在人群中多看了你一眼 再也没能忘掉你容颜”，而这首歌名叫“传奇”。你说传奇不传奇？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;看似In Context Learning没从例子里学习知识，实际上，难道LLM通过一种奇怪的方式去学习？还是说，它确实也没学啥？关于这个问题的答案，目前仍是未解之谜。现有一些研究各有各的说法，五花八门，很难判断哪个讲述的是事实的真相，甚至有些研究结论还相互矛盾。这里提供几个目前的说法，至于谁对谁错，只能你自己把握了。当然，我认为追求这个神奇现象背后的真相，是一个好的研究课题。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;p&gt;&lt;span&gt;试图证明In Context Learning没有从例子中学习的工作是“Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?”。它发现了：在提供给LLM的样本示例 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -540 4206.1 745&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;mo&quot;&gt;&lt;path data-c=&quot;3C&quot; d=&quot;M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;msub&quot; transform=&quot;translate(1055.8, 0)&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;78&quot; d=&quot;M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(572, -150) scale(0.707)&quot;&gt;&lt;path data-c=&quot;69&quot; d=&quot;M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(1921.7, 0)&quot;&gt;&lt;path data-c=&quot;2C&quot; d=&quot;M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;msub&quot; transform=&quot;translate(2366.4, 0)&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;79&quot; d=&quot;M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(490, -150) scale(0.707)&quot;&gt;&lt;path data-c=&quot;69&quot; d=&quot;M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(3428.1, 0)&quot;&gt;&lt;path data-c=&quot;3E&quot; d=&quot;M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt; 中，&lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -442 784 647&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;msub&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;79&quot; d=&quot;M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(490, -150) scale(0.707)&quot;&gt;&lt;path data-c=&quot;69&quot; d=&quot;M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt; 是否 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -442 866 599.8&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;msub&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;78&quot; d=&quot;M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(572, -150) scale(0.707)&quot;&gt;&lt;path data-c=&quot;69&quot; d=&quot;M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt;对应的正确答案，其实并不重要，如果我们把正确答案 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -442 784 647&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;msub&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;79&quot; d=&quot;M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(490, -150) scale(0.707)&quot;&gt;&lt;path data-c=&quot;69&quot; d=&quot;M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt; 替换成随机的另外一个答案 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -442 831.3 736.2&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;msub&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;79&quot; d=&quot;M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(490, -150) scale(0.707)&quot;&gt;&lt;path data-c=&quot;6A&quot; d=&quot;M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt;，这并不影响In Context Learning的效果。这起码说明了一点：In Context Learning并没有提供给LLM那个从 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -442 572 453&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;78&quot; d=&quot;M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt; 映射到 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -442 490 647&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;79&quot; d=&quot;M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt; 的映射函数信息：&lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -750 3723.6 1000&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;79&quot; d=&quot;M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(767.8, 0)&quot;&gt;&lt;path data-c=&quot;3D&quot; d=&quot;M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(1823.6, 0)&quot;&gt;&lt;path data-c=&quot;66&quot; d=&quot;M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(2373.6, 0)&quot;&gt;&lt;path data-c=&quot;28&quot; d=&quot;M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(2762.6, 0)&quot;&gt;&lt;path data-c=&quot;78&quot; d=&quot;M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(3334.6, 0)&quot;&gt;&lt;path data-c=&quot;29&quot; d=&quot;M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt;，否则的话你乱换正确标签，肯定会扰乱这个 &lt;span&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; role=&quot;img&quot; focusable=&quot;false&quot; viewbox=&quot;0 -750 3723.6 1000&quot; aria-hidden=&quot;true&quot;&gt;&lt;g stroke=&quot;currentColor&quot; fill=&quot;currentColor&quot; stroke-width=&quot;0&quot; transform=&quot;matrix(1 0 0 -1 0 0)&quot;&gt;&lt;g data-mml-node=&quot;math&quot;&gt;&lt;g data-mml-node=&quot;mi&quot;&gt;&lt;path data-c=&quot;79&quot; d=&quot;M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(767.8, 0)&quot;&gt;&lt;path data-c=&quot;3D&quot; d=&quot;M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(1823.6, 0)&quot;&gt;&lt;path data-c=&quot;66&quot; d=&quot;M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(2373.6, 0)&quot;&gt;&lt;path data-c=&quot;28&quot; d=&quot;M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mi&quot; transform=&quot;translate(2762.6, 0)&quot;&gt;&lt;path data-c=&quot;78&quot; d=&quot;M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z&quot;/&gt;&lt;/g&gt;&lt;g data-mml-node=&quot;mo&quot; transform=&quot;translate(3334.6, 0)&quot;&gt;&lt;path data-c=&quot;29&quot; d=&quot;M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/span&gt;映射函数。也就是说，In Context Learning并未学习这个输入空间到输出空间的映射过程。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;真正对In Context Learning影响比较大的是：x 和 y 的分布，也就是输入文本 x 的分布和候选答案 y 有哪些，如果你改变这两个分布，比如把 y 替换成候选答案之外的内容，则In Context Learning效果急剧下降。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;总之，这个工作证明了In Context Learning并未学习映射函数，但是输入和输出的分布很重要，这两个不能乱改。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;有些工作认为LLM还是从给出的示例学习了这个映射函数 y=f(x) ，不过是种隐式地学习。比如“What learning algorithm is in-context learning? Investigations with linear models”认为Transformer能够隐式地从示例中学习 x 到 y 的映射过程，它的激活函数中包含了一些简单映射函数，而LLM通过示例能够激发对应的那一个。而“Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers”这篇文章则将ICL看作是一种隐式的Fine-tuning。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;总而言之，目前这还是一个未解之谜。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;神奇的Instruct理解&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;我们可以把Instruct当作一种方便人类理解的任务表述，在这个前提下，目前关于Instruct的研究可以分成两种：偏学术研究的Instruct，以及关于人类真实需求描述的Instruct。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;281&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.4875&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1Bhhk1aRyRSdy9VARf8p9jQVR0eCdjbjg2Khb8qdvLfHzVibBLCeOElNFgw/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;我们先来看第一种：偏学术研究的Instruct。它的核心研究主题是多任务场景下，LLM模型对Instruct理解的泛化能力。如上图中FLAN模型所示，就是说有很多NLP任务，对于每个任务，研究人员构造一个或者多个Prompt模版作为任务的Instruct，然后用训练例子对LLM模型进行微调，让LLM以同时学习多个任务。训练好模型后，给LLM模型一个它没见过的全新任务的Instruct，然后让LLM 解决zero shot任务，从任务解决得是否足够好，来判断LLM模型是否有对Instruct理解的泛化能力。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果归纳下目前的研究结论（可参考“Scaling Instruction-Fine-tuned Language Models”／“Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks”），能够有效增加LLM模型Instruct泛化能力的因素包括：增加多任务的任务数量、增加LLM模型大小、提供CoT Prompting， 以及增加任务的多样性。如果采取任意一项措施，都可以增加LLM模型的Instruct理解能力。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第二种是人类真实需求下的Instruct，这类研究以InstructGPT和ChatGPT为代表。这类工作也是基于多任务的，但是和偏向学术研究类工作最大的不同，在于它是面向人类用户真实需求的。为什么这么说呢？因为它们用于LLM多任务训练的任务描述Prompt，是从大量用户提交的真实请求中抽样而来的，而不是固定好研究任务的范围，然后让研究人员来写任务描述prompt。这里所谓的“真实需求”，体现在两个方面：首先，因为是从用户提交的任务描述里随机抽取的，所以涵盖的任务类型更多样化，也更符合用户的真实需求；其次，某个任务的prompt描述，是用户提交的，体现了一般用户在表达任务需求时会怎么说，而不是你认为用户会怎么说。很明显，这类工作改出来的LLM模型，用户体验会更好。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;InstructGPT论文里，也拿这种方法和FLAN那种Instruct based方法做了比较。首先在GPT3上用FLAN提到的任务、数据以及Prompt模版进行微调，来在GPT 3上复现FLAN方法，然后和InstructGPT进行比较，因为InstructGPT的基础模型也是GPT3，所以只有数据和方法的差别，两者可比，结果发现FLAN方法的效果，距离InstructGPT有很大的差距。那么背后的原因是什么呢？论文分析数据后认为，FLAN方法涉及到的任务领域相对少，是InstructGPT涉及领域的子集，所以效果不好。也就是说，FLAN论文里涉及到的任务和用户真实需求是不符的，而这导致在真实场景下效果不够好。而这对我们的启示是：从用户数据中收集真实需求，这事情是很重要的。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;In Context Learning和Instruct的联系&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果我们假设In Context Learning是用一些例子来具象地表达任务命令，Instruct是一种更符合人类习惯的抽象任务描述。那么，一个很自然的问题是：它们之间有什么联系吗？比如，我们是否能够提供给LLM完成某个任务的若干具体示例，让LLM找出其对应的自然语言描述的Instruct命令？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;252&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.4375&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhdQbZjHJrleCs1h4LjO9KRtvebjianKydLGCvr4ibm0TruZibyiceRQ5ulw/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;目前有零星的工作在探索这个问题，我认为这个方向是很有研究价值的。先说答案，答案是：Yes，LLM Can。“Large Language Models Are Human-Level Prompt Engineers”是做这个方向很有趣的工作，如上图所示，对于某项任务，给LLM一些示例，让LLM自动生成能够描述这项任务的自然语言命令，然后它再用LLM生成的任务描述去测试任务效果。它使用的基础模型是GPT 3和InstructGPT，经过这项技术加持后，LLM生成的Instruct的效果相比未采用这项技术的GPT 3 以及InstuctGPT来说，指标有极大地提升，而且在一些任务上超过人类的表现。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这说明了：具象的任务示例和任务的自然语言描述之间，有种神秘的内在联系。至于这种联系到底是什么？我们目前对此还一无所知。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;117920&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;strong data-original-title=&quot;&quot; title=&quot;&quot; data-num=&quot;5&quot;&gt;5&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;智慧之光：如何增强LLM的推理能力&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;目前很多研究已证明LLM对于知识具有强大的记忆能力，但是，一般我们不会因为一个人记忆能力强，就说这人很聪明，是否具有强大的推理能力，往往是我们判断一个人是否聪明的重要标准。类似的，如果LLM的效果想让人觉得很惊艳，强大的推理能力是必备的。推理能力本质上是综合运用很多相关知识点，去推导出新知识或新结论。关于LLM的推理能力，是最近一年来LLM里最重要和热门的研究领域之一。于是，我们关心的问题就是：LLM具备推理能力吗？如果具备，那么它的推理能力够强吗？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这两个问题目前的答案似乎应该是：当模型规模足够大的时候，LLM本身是具备推理能力的，在简单推理问题上，LLM已经达到了很好的能力，但是复杂推理问题上，还需要更多深入的研究。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果梳理现有LLM推理相关工作的话，我把它们归到两大类，体现出挖掘或促进LLM推理能力不同的技术思路：第一类研究比较多，可以统称为基于Prompt的方法，核心思想是通过合适的提示语或提示样本，更好地激发出LLM本身就具备的推理能力，Google在这个方向做了大量很有成效的工作。第二类做法是在预训练过程中引入程序代码，和文本一起参与预训练，以此进一步增强LLM的推理能力，这应该是OpenAI实践出的思路。比如ChatGPT肯定具备很强的推理能力，但它并不要求用户必须提供一些推理示例，所以ChatGPT强大的推理能力，大概率来源于使用代码参与GPT 3.5的预训练。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这两种思路其实大方向是迥异的：利用代码增强LLM推理能力，这体现出一种通过增加多样性的训练数据，来直接增强LLM推理能力的思路；而基于Prompt的方法，它并不会促进LLM本身的推理能力，只是让LLM在解决问题过程中更好地展示出这种能力的技术方法。可以看出，前者（代码方法）治本，后者治标。当然，两者其实也是互补的，但从长远看，治本的方法更重要。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. 基于Prompt的方法&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这方面工作非常多，如果归纳一下的话，大致可以分为三条技术路线。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;293&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.50625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhepreKRmCXDpgiaTpiaL62oDMtI6Ab7pdibKXGia1oFRclaP4K96WaglQQQ/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第一种思路是直接在问题上追加辅助推理Prompt。这种方法简单直接，但在众多领域都很有效。这个做法是由“Large language models are zero-shot reasoners”提出的，也被称为zero-shot CoT。具体而言，分为两个阶段（如上图所示），第一阶段在提问的问题上追加“Let’s think step by step”这句提示语，LLM会输出具体的推理过程；第二阶段，在第一阶段的问题后，拼接LLM输出的具体推理过程，并再追加Prompt=“Therefore, the answer (arabic numerals) is”，此时LLM会给出答案。如此简单的操作，却可以大幅增加LLM在各项推理任务中的效果，比如在数学推理测试集GSM8K上，加上提示语后，推理准确率直接从原先的10.4%提升到了40.4%，可谓神奇。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;为什么LLM会具备给一句“Let’s think step by step”提示语，就能列出详细的推理步骤并算出答案呢？其原因目前尚无定论，我的猜测是：很可能因为预训练数据里面存在大量的此种数据，就是以“Let’s think step by step”开头，然后后面是详细的推理步骤，最后给出答案，而LLM在预训练的时候记住了这些模式。而当我们输入这个提示语的时候，激发LLM模糊得“回忆”起某些例子的推导步骤，于是即可模仿这些例子进行步骤推理并给出答案。当然这只是我的无依据推论，若事实真的如此，如果你看过后面介绍的标准CoT做法，会发现Zero-shot CoT 本质上和标准CoT很可能没什么区别，只是标准CoT由人工来写推理步骤的示例，而Zero-shot CoT大概率是通过提示语，激活了记忆中的某些包含推理步骤的示例，很可能是如此区别。而标准CoT效果比Zero-Shot CoT效果好也完全可以理解，因为毕竟靠LLM回忆示例，精准性估计不会太高，而人工给出的示例，准确性是有保障的，所以自然标准CoT效果会更好。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这侧面说明了一个道理，就是LLM本身是具备推理能力的，只是我们没有办法把它的这种能力激发出来而已，通过合适的提示语来进行两步提示，就在一定程度上可以释放出它的这种潜力。另外，对于中文，很可能存在另外一个黄金提示语，比如“详细解题思路如下”，类似这种，因为中文语料在讲解推理步骤的时候，经常用的引导句和“让我们一步一步来思考”应该是不同的，这是明显的西方说法，而探索出这个中文黄金提示语，其实也是很有必要的。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第二种思路一般被称为基于示例的思维链（few-shot CoT,Chain of Thought）Prompting。这个方向目前是LLM推理研究的主方向，很多工作都是在这个思路上做的，我们简单介绍几个效果显著的代表性工作，基本能代表CoT的技术发展方向。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;329&quot; data-backw=&quot;576&quot; data-ratio=&quot;0.571875&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhaXYF4KTv6zAmU7fyiaIxzeuFFDFG2jiaFYZ3lXCONfWIV6C9jmiaLMIog/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;CoT的主体思想其实很直白；为了教会LLM模型学会推理，给出一些人工写好的推理示例，示例里把得到最终答案前，一步步的具体推理步骤说清楚，而这些人工写的详细推理过程，就是思维链Prompting，具体例子可参照上图中蓝色文字部分。CoT的意思是让LLM模型明白一个道理；就是在推理过程中，步子不要迈得太大，否则很容易出错，改变思维模式，化大问题为小问题，步步为营，积小胜为大胜。最早明确提出CoT这个概念的文章是“Chain of thought prompting elicits reasoning in large language models”，论文发布于22年1月份，虽然做法很简单，但是应用CoT后LLM模型的推理能力得到了巨大提升，GSM8K数学推理测试集准确率提高到60.1%左右。当然，这种给出详细推理步骤和中间过程的思想，并非CoT最早提出的，更早一些的“scratchpad”技术（可参考：Show Your Work: Scratchpads for Intermediate Computation with Language Models）首先采用了类似的思路。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;310&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.5359375&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhZYGja1ux7ic6QAdWOu09wVBGQTicI8saLzYicaxwkb7bvADn39jUHF1tA/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;CoT提出不久，很快在22年3月份，一项被称为“Self-Consistency”的改进技术就将GSM8K测试集准确率提高到74.4%，提出这项改进的论文是“Self-Consistency Improves Chain of Thought Reasoning in Language Models”。“Self-Consistency”的思路也很直观（参考上图）：首先可以利用CoT给出几个写了推理过程的示例，然后要求LLM对给定的问题进行推理，如果是CoT，直接输出一个推理过程和答案，整个过程就结束了。“Self-Consistency”则不然，它要求LLM输出多个不同的推理过程和答案，然后采用投票的方式选出最佳答案，思路非常简单直接，但是效果也确实好。“Self-Consistency”其实是教导LLM学会这么一个道理：孔乙己说过茴香豆的“茴”字有四种写法，类似的，一个数学题的正确解法也可以有很多种，每个不同的推导过程都指向最终的答案。条条大路通罗马，虽说也有个别迷路走到北京的，但是迷路的毕竟是少数，看看大多数人走到哪里，哪里就是正确答案。简单的方法往往蕴含着深刻的哲学含义，是不是这道理？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;再往后，“On the Advance of Making Language Models Better Reasoners”这个工作在“Self-Consistency”基础上，进一步集成了“从一个Prompt问题拓展到多个Prompt问题、检查推理中间步骤的正确性以及对多个输出的回答加权投票”这三个改进点，将GSM8K测试集准确率提高到83%左右。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;418&quot; data-backw=&quot;576&quot; data-ratio=&quot;0.725&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhkeDQG4V9IqFiaEfxZQ5EFBa29cxIOdTQBJD5LtudWwhOxEw1vbdvzxg/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第三种思路体现了一种分治算法的思想。当然这个所谓“分治”是我归纳的，别人没这么说。这种思路的核心思想是：对于一个复杂的推理问题，我们把它分解成若干容易解决的子问题，一一解决掉子问题后，我们再从子问题的答案推导复杂问题的答案。你看这确实比较类似分治算法的思想吧。我个人觉得，这种思路可能才是揭示问题本质、最终解决LLM复杂推理问题正宗的道路。我们以“Least-to-most prompting”技术为例来说明这种思路的一种具体实现方式，如上图所示：它分为两个阶段，第一个阶段，从原始问题我们可以得知最终要问的问题是什么，我们假设最终问题是Final Q，然后从原始问题填充Prompt模版：“如果要解决Final Q问题，那么我需要先解决”，然后把原始问题和这个Prompt交给LLM，让LLM模型给出答案，等于让LLM给出最终问题的前置子问题Sub Q；接下来我们进入第二个阶段，让LLM先回答刚才拿到的子问题Sub Q，并拿到对应的答案，然后原始问题拼接子问题Sub Q及对应答案，再去问LLM最终那个问题Final Q，此时LLM会给出最后的答案。如此这般，体现出拆解子问题，并从子问题的答案逐步找出最终答案的思路。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2. 代码预训练增强LLM推理能力&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;以上是目前利用Prompt激发LLM模型推理能力的三种主流做法，而关于LLM的推理能力，目前还观察到一个有趣且费解的现象：除了文本外，如果能够加入程序代码一起参与模型预训练，则能大幅提升LLM模型的推理能力。这个结论从不少论文的实验部分都可以得出（可以参考：AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS／Challenging BIG-Bench tasks and whether chain-of-thought can solve them等论文的实验部分）。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;243&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.4203125&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhJWPe8qzzDWraChyKvK73RUl34m1Ge8w8LGrFUhNPzlaypic6zSvc8tg/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;上图给出了一份实验数据，来自于论文“On the Advance of Making Language Models Better Reasoners”，其中GPT3 davinci就是标准的GPT 3模型，基于纯文本训练；code-davinci-002（OpenAI内部称为Codex）是同时在Code和NLP数据上训练的模型。如果比较两者效果，可以看出，不论采用具体哪种推理方法，仅仅是从纯文本预训练模型切换到文本和Code混合预训练模型，在几乎所有测试数据集合上，模型推理能力都得到了巨大的效果提升，比如我们以“Self Consistency”方法为例，在大多数据集合上的性能提升，都直接超过了20到50个百分点，这是很恐怖的性能提升，而其实在具体推理模型层面，我们什么也没做，仅仅是预训练的时候除了文本，额外加入了程序代码而已。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;除了这个现象，从上图数据中，我们还可以得出其它一些结论，比如GPT 3这种纯文本预训练模型，其实是具备相当程度的推理能力的，除了在GSM8K这种数学推理上效果比较差外，其它推理数据数据集合表现也还可以，前提你需要采用合适的方法，来激发出它本身就具备的这种能力；再比如，text-davinci-002，也就是在code-davinci-002基础上加入instruct fine-tuning后的模型（就是加入InstructGPT或ChatGPT模型的第一步），其推理能力要弱于Codex，但是有其它研究表明它在自然语言处理任务又要强于Codex。而这貌似说明了，加入instruct fine-tuning，会损害LLM模型的推理能力，但是会在一定程度上提升自然语言理解能力。而这些结论其实都是很有意思的，也能启发后续进一步的思考和探索。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;那么，一个自然的疑问是：为何预训练模型可以从代码的预训练中获得额外的推理能力？确切原因目前未知，值得深入探索。我猜测可能是因为原始版本的Codex（只使用代码训练，可参考文献：Evaluating Large Language Models Trained on Code）的代码训练是从文本生成代码，而且代码中往往包含很多文本注释，本质上这类似于预训练模型做了&amp;lt;文本,Code&amp;gt;两种数据的多模态对齐工作。而数据中必然包含相当比例的数学或逻辑问题的代码、描述和注释，很明显这些数学类或逻辑推理类的数据，对于解决下游数学推理问题是有帮助的，我猜大概率原因在此。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3. 关于LLM推理能力的思考&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;上面介绍了LLM推理的主流技术思路和现有的一些结论，接下来谈谈我对LLM模型推理技术的思考，以下内容纯个人推断，没有太多证据，还请谨慎参考。我的判断是：虽然最近一年来，关于激发LLM的推理能力，这方面的技术进展很快，也取得了很大的技术进步，但是总体感觉是，我们可能走在正确的方向上，但是距离接触到真正的问题本质还有一段距离，对此要有更深入的思考和探索。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;首先，我比较赞同上述分治算法的主体思路，对于复杂的推理问题，我们应该把它拆解成若干简单的子问题，因为子问题对于LLM来说回答正确的概率就大很多，让LLM一一回答子问题后，再逐步推导出最终答案。受到“Least-to-most prompting”技术的启发，如果进一步思考，我觉得LLM推理本质上很可能会是如下两种可能的其中之一：不断和LLM进行交互的图上推理问题，抑或是不断和LLM进行交互的程序流程图执行问题。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;229&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.3953125&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1Bhhja5Jh9nICJHNwjDZaWj4sxDoibhLDPxJ14CKeeME3WRwQEgicUnDicOibQ/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;先说图上推理问题，如上图所示，假设我们有办法能够把复杂问题拆解成由子问题或者子步骤构成的图结构，图中的节点是子问题或者子步骤，图中的边代表了子问题之间的依赖关系，就是说只有回答好子问题A，才能回答子问题B，而且图中大概率存在循环结构，就是反复做某几个子步骤。假设我们能够得到上述的子问题拆解图，那么可以根据依赖关系，引导LLM一步一步按照图结构，回答必须首先回答的子问题，直到推导出最终答案。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;242&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.41875&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz/zHbzQPKIBPg14icntwicW9XhD0mCTz1BhhYXZDX2X6Wxyib1d15F4RjbEfnyVhe6jlS0mSGpYxrSV1n94KDgzTAaA/640?wx_fmt=jpeg&quot; data-type=&quot;other&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;再说程序流程图问题，参考上图，假设我们有办法把复杂问题拆解成子问题或子步骤，并产生一个由子步骤构成的类似程序流程图的结构，在这个结构里，有些步骤会反复执行多次（循环结构），有些步骤的执行需要进行条件判断（条件分支）。总而言之，在执行每个子步骤的时候和LLM进行交互，得到子步骤的答案，然后按照流程不断执行，直到输出最终答案。类似这种模式。假设这个思路大致正确的话，也许可以从这个角度来解释为何加入代码会增强预训练模型的推理能力：大概率因为&amp;lt;文本，代码&amp;gt;的多模态预训练模型，在模型内部是通过类似这种隐含的程序流程图作为两个模态的桥梁，将两者联系起来的，即由文本描述到隐含的流程图，再映射到由流程图产生具体的代码。也就是说，这种多模态预训练，可以增强LLM模型从文本构建出隐含的流程图并按照流程图执行的能力，也就是加强了它的推理能力。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;当然，上述思路最大的问题是，我们如何根据文本描述的问题，能够靠LLM模型，或者其它模型，得到图结构或者流程图结构？这个可能是其中的难点。一种可能的思路就类似继续增强文本和更高质量的代码预训练，走隐式学习内部隐含结构的方法。而目前的CoT技术，如果套到上述思路来思考的话，可以这么理解：标准CoT，其实就是靠自然语言文本来描述图结构或者程序流程图的；而“Least-to-most prompting”技术，则是试图根据最后一个图节点，靠倒推来试图推导出其中的图结构，但是很明显，目前的方法限制了它倒推的深度，也就是说它只能推导出非常简单的图结构，这正是限制它能力的所在。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;117920&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;strong data-original-title=&quot;&quot; title=&quot;&quot; data-num=&quot;6&quot;&gt;6&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;未来之路：LLM研究趋势及值得研究的重点方向&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;这里列出一些我个人认为比较重要的LLM研究领域，或值得深入探索的研究方向。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. 探索LLM模型的规模天花板&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;尽管继续推大LLM模型的规模，这事看似没有技术含量，但是其实这个事情异常重要。我个人判断，自从Bert出现以来，到GPT 3，再到ChatGPT，大概率这些给人印象深刻的关键技术突破，核心贡献都来自于LLM模型规模的增长，而非某项具体技术。说不定，揭开AGI真正的钥匙就是：超大规模及足够多样性的数据、超大规模的模型，以及充分的训练过程。再者，做超大规模的LLM模型，对技术团队的工程实现能力要求是非常高的，也不能认为这事情缺乏技术含量。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;那么继续推大LLM模型规模，有什么研究意义呢？我觉得有两方面的价值。首先，如上所述，我们已知，对于知识密集型的任务，随着模型规模越大，各种任务的效果会越来越好；而对很多推理类型的有难度的任务，加上CoT Prompting后，其效果也呈现出遵循Scaling law的趋向。那么，很自然的一个问题就是：对于这些任务，LLM的规模效应，能将这些任务解决到何种程度？这是包括我在内，很多人关心的问题。其次，考虑到LLM具备的神奇的“涌现能力”，如果我们继续增加模型规模，它会解锁哪些让我们意想不到的新能力呢？这也是很有意思的问题。考虑到以上两点，我们仍然需要不断增大模型规模，看看模型规模对解决各类任务的天花板在哪里。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;当然，这种事情也就只能说说，对99.99%的从业者来说，是没有机会和能力做这个事情的。要做这个事情，对研究机构的财力及投入意愿、工程能力、技术热情，都有极高的要求，缺一不可。能做这事情的机构，粗估下来，国外不超过5家，国内不超过3家。当然，考虑到成本问题，未来也许会出现“股份制大模型”，就是有能力的几家机构合作，群策群力，一起来共建超级大模型的现象。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2. 增强LLM的复杂推理能力&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;正如之前对LLM推理能力的叙述，尽管LLM在最近一年推理能力得到了很大的提升，但是很多研究（参考：Limitations of Language Models in Arithmetic and Symbolic Induction／Large Language Models Still Can’t Plan）表明，目前LLM能够解决得比较好的推理问题，往往都相对简单，LLM的复杂推理能力仍然薄弱，比如即使是简单的字符拷贝推理或者加减乘除运算，当字符串或者数字非常长的时候，LLM推理能力会极速下降，再比如行为规划能力等复杂推理能力很弱。总而言之，加强LLM的复杂推理能力，应该是LLM未来研究中最重要的环节之一。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;前文有述，加入代码加入预训练，这是一种直接增强LLM推理能力的方向。这个方向目前研究尚显不足，更像是实践经验的总结，探索背后的原理，并进而引入更多类型除代码外的新型数据来增强LLM的推理能力，这可能是更本质提升推理能力的方向。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3. LLM纳入NLP之外更多其它研究领域&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;目前的ChatGPT擅长NLP和Code任务，作为通向AGI的重要种子选手，将图像、视频、音频等图像与多模态集成进入LLM，乃至AI for Science、机器人控制等更多、差异化更明显的其它领域逐步纳入LLM，是LLM通往AGI的必经之路。而这个方向才刚刚开始，因此具备很高的研究价值。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;4. 更易&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;用的人和LLM的交互接口&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如前所述，ChatGPT的最大技术贡献即在此。但是很明显，目前的技术并不完美，肯定还有很多命令LLM理解不了。所以，沿着这个方向，寻找更好的技术，来让人类使用自己习惯的命令表达方式，而LLM又能听懂，这是个新的，且非常有前景的技术方向。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;5. 建设高难度的综合任务评测数据集&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;好的评测数据集，是引导技术不断进步的基石。随着LLM模型逐步增大，任务效果快速提升，导致很多标准测试集快速过时。也就是说，这些数据集合相对现有技术来说，太容易了，在没有难度的测试集合下，我们不知道目前技术的缺陷和盲点在哪里。所以构建高难度的测试集合，是促进LLM技术进步的关键所在。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;目前行业应出现了一些新的测试集，有代表性的包括BIGBench、OPT-IML等。这些测试集合体现出一些特性，比如相对LLM现有技术具备一定的难度、综合了各种各样多种类型的任务等。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;受到ChatGPT的启发，我觉得除此外应纳入另一考虑因素：体现真实用户需求。就是说，这些任务的表述由用户真实发起，这种方式构建出来的LLM模型，才能解决用户实际需求。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;除此外，相信LLM会快速将能力溢出到NLP之外的领域，而如何融入更多其它领域的评测数据，也是需要提前去考虑。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;6. 高质量数据工程&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;对于预训练模型来说，数据是其根本，预训练过程可以理解为从数据中吸取其中所包含知识的过程。因此，我们需要进一步加强对高质量数据的挖掘、收集及清洗等工作。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;关于数据，需要考虑两个方面：数据的质量和数量。而根据T5的对比实验，我们可以得出结论：在数量和质量两个因素里，质量优先，正确的道路应该是在保证数据质量的前提下，再去增大数据规模。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;数据质量，包括数据的信息含量以及数据的多样性等多个衡量标准，比如Wiki明显就属于世界知识密度极高的高质量数据，这是从信息含量来说的；而增加数据类型的多样性，无疑是激发LLM各种新能力的根本，比如加入问答网站的数据，对于LLM的QA能力提升是有直接帮助的。多样化的数据赋予了LLM更好解决更多不同类型任务的能力，所以，这可能是数据质量里最关键的标准。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;关于数据数量，原则上互联网上公开发布的数据都可以纳入LLM模型的预训练过程。那么，它的极限在哪里？“Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning” 对此进行了估算，结论是到2026年左右，高质量的NLP数据将会用光，低质量NLP数据会在2030到2050年用光，而低质量图像数据会在2030到2060年用光。而这意味着：要么到时我们有新类型的数据源，要么我们必须增加LLM模型对数据的利用效率。否则，目前这种数据驱动的模型优化方式将会停止进步，或者收益减少。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;7. 超大LLM模型Transformer的稀疏化&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;目前规模最大的LLM中，有相当比例的模型采取了稀疏（Sparse）结构，比如GPT 3、PaLM、GLaM等，GPT 4大概率也会走稀疏模型路线。之所以采用Sparse 化的模型，主要好处是它可以极大减少LLM的训练时间和在线推理时间。Switch Transformer论文里指出：在相同算力预算的前提下，使用稀疏化Transformer，相对Dense Transformer，LLM模型的训练速度可以提升4倍到7倍。为何Sparse模型可以加快训练和推理时间呢？这是因为尽管模型参数巨大，但是对于某个训练实例，Sparse模型通过路由机制，只使用整个参数中的一小部分，参与训练和推理的活跃参数量比较少，所以速度快。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;我认为未来超大的LLM模型大概率会收敛到稀疏模型。主要有两个原因：一方面，现有研究表明（参考：Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers），标准的Dense Transformer在训练和推理时，它本身也是稀疏激活的，就是说只有部分参数会被激活，大部分参数没有参与训练和推理过程。既然这样，我们不如直接迁移到稀疏模型；另外，毫无疑问LLM模型的规模会继续推大，而高昂的训练成本是妨碍其进一步扩大模型的重要阻力，使用稀疏模型可以极大降低超大模型的训练成本，所以随着模型规模越大，稀疏模型带来的收益越明显。考虑到这两个方面，大概率未来更大的LLM模型会采用稀疏模型方案。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;那为何目前其它大规模模型不走稀疏模型的路线呢？因为Sparse模型存在训练不稳定、容易过拟合等问题，不太容易训练好。所以，如何修正稀疏模型面临的问题，设计出更容易训练的稀疏模型，是很重要的未来研究方向。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;117920&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;strong data-original-title=&quot;&quot; title=&quot;&quot; data-num=&quot;7&quot;&gt;7&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;取经之路：复刻ChatGPT时要注意些什么&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;如果希望能复刻类似ChatGPT这种效果令人惊艳的LLM模型，综合目前的各种研究结论，在做技术选型时需要重点权衡如下问题：&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;首先，在预训练模式上，我们有三种选择：GPT这种自回归语言模型，Bert这种双向语言模型，以及T5这种混合模式(Encoder-Decoder架构，在Encoder采取双向语言模型，Decoder采取自回归语言模型，所以是一种混合结构，但其本质仍属于Bert模式)。我们应选择GPT这种自回归语言模型，其原因在本文范式转换部分有做分析。目前看，国内LLM在做这方面技术选型的时候，貌似很多都走了Bert双向语言模型或T5混合语言模型的技术路线，很可能方向走偏了。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第二，强大的推理能力是让用户认可LLM的重要心理基础，而如果希望LLM能够具备强大的推理能力，根据目前经验，最好在做预训练的时候，要引入大量代码和文本一起进行LLM训练。至于其中的道理，在本文前面相关部分有对应分析。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第三，如果希望模型参数规模不要那么巨大，但又希望效果仍然足够好，此时有两个技术选项可做配置：要么增强高质量数据收集、挖掘、清理等方面的工作，意思是我模型参数可以是ChatGPT/GPT 4的一半，但是要想达到类似的效果，那么高质量训练数据的数量就需要是ChatGPT/GPT 4模型的一倍（Chinchilla的路子）；另外一个可以有效减小模型规模的路线是采取文本检索（Retrieval based）模型+LLM的路线，这样也可以在效果相当的前提下，极大减少LLM模型的参数规模。这两个技术选型不互斥，反而是互补的，也即是说，可以同时采取这两个技术，在模型规模相对比较小的前提下，达到超级大模型类似的效果。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第四，超级大模型因为模型规模大，所以训练成本过高，导致很少有机构有能力去做这件事。而且由上文分析可见，继续不断推大LLM模型规模是肯定会发生、也应该去做的事情。于是，如何通过技术手段降低LLM的训练成本就很重要。LLM的特征抽取器Sparse化是有效降低模型训练及推理成本的技术选择。由此可见，随着模型越来越大，LLM模型Sparse化是一个应该考虑的选项。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第五，ChatGPT是目前最接近理想LLM的技术方案，而理想中的LLM应该是以一个几乎无所不能的基础通用大模型作为依托，来支持各种各样的上层任务类型。目前看，支持越来越多的任务类型，主要是通过增加LLM预训练数据的多样性来达成的，数据多样性越好，LLM能够支持的任务类型就越丰富。所以，应该重视通过增加数据多样性来增加LLM新能力的思路。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;第六，易用的人机操作接口。人类用他们自己习惯的表达方式来描述任务，而LLM要能够理解这些Instruct的真实含义。另外，也要注意这些Instruct是符合人类真实需求的，就是说，要从最终用户那里收集任务表述方式，而不能靠研发人员自己的臆想或猜测。ChatGPT给我最大的启发其实是这一点，至于是否用增强学习我倒觉得不重要，其它替代技术应该也能做类似的事情。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;117920&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;strong data-original-title=&quot;&quot; title=&quot;&quot; data-num=&quot;8&quot;&gt;8&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;ChatGPT：为什么是OpenAI&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;为什么是OpenAI作出了ChatGPT，而不是其它机构呢？我们在这里可以做个简单分析。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;在本文开头，我们提到了OpenAI看待LLM的理念。OpenAI是怎么看待LLM的呢？回顾它不断推出的技术，可以看出，它其实从GPT 1.0开始，基本就坚定地把LLM看做是通往AGI的一条必由之路。具体而言，在OpenAI眼中，未来的AGI应该长这个样子：有一个任务无关的超大型LLM，用来从海量数据中学习各种知识，这个LLM以生成一切的方式，来解决各种各样的实际问题，而且它应该能听懂人类的命令，以便于人类使用。其实对LLM发展理念的理解，在前半部分，就是“构建一个任务无关的超大型LLM，让它从海量数据中学习各种知识”，这一点几乎是大家的共识，能体现出OpenAI眼光的其实是后半部分。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;OpenAI的理念比较超前，对自我定位从一开始就定得比较高，始终坚定不移地探索上述方式是否可以实现AGI。OpenAI之所以能作出ChatGPT，胜在一个是定位比较高，另一个是不受外界干扰，态度上坚定不移。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;我们可以回顾下它走的一些关键路程：GPT 1.0走的是生成模式的自回归语言模型路线，比Bert出来的还早些。Bert证明了：双向语言模型对于很多NLP理解类任务，效果比自回归这种单向语言模型效果更好。尽管如此，GPT 2.0并没有因此切换到双向语言模型这条路上，仍然走文本生成的路，而且开始尝试零示例（zero shot）prompt和少量示例（few shot）prompt。其实这时候， OpenAI心目中的AGI已经开始浮出水面，逐渐显示出轮廓了。只是因为zero shot/few shot效果比Bert+fine-tuning差的比较远，所以大家都没太当回事，甚至不理解它为什么要始终坚持走单向语言模型的路线。这个时候，我估计即使是OpenAI自己，也不一定能确保这条路肯定能走通。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;但是，这不妨碍它继续在这条路上往后走。GPT 3.0已经展示出了比较强大的zero shot/few shot prompt能力，这时候OpenAI心目中的AGI已经完全漏出水面，轮廓清晰，而且它的效果也证明了这条路，是有较大可能走得通的。GPT 3.0是一个决定LLM发展方向的叉路口和分水岭，与之对应的另外一条路是“Bert+fine-tuning”模式。在这个岔路口，不同的从业者选择走上了不同的道路，后面的技术差距也是从这里开始拉开的。很遗憾地是，国内很多从业者选择继续在“Bert+fine-tuning”这条路上往后走，这也是造成今天落后局面的一个关键时间节点。再往后，就是InstructGPT和ChatGPT了，OpenAI通过ChatGPT证明了一点；虽然我们距离真正的AGI，可能还有很长的路要走，但是通过超大LLM走向AGI这条路，目前看是可行的。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;今天的分享就到这里，谢谢大家。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-custom=&quot;rgb(172, 29, 16)&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;版权声明&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;p&gt;&lt;span&gt;版权属于原作者，仅用于学术分享&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;section&gt;&lt;section data-role=&quot;title&quot; data-tools=&quot;135编辑器&quot; data-id=&quot;93528&quot;&gt;&lt;section&gt;&lt;img data-ratio=&quot;1&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_gif/PW0wIHxgg3kMbkibZkKBXgOJtTn7YFj453Pf43ltO6m2veBiaibrZ6WhJlwHTZfyiaLzRHqguicjcKtDJZPpKbt3Ekg/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;55&quot; data-width=&quot;10%&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;想了解更多关于ChatGPT的内容？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;快来预约明晚的直播吧！&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section class=&quot;channels_iframe_wrp&quot;&gt;&lt;mpvideosnap class=&quot;js_uneditable custom_select_card channels_live_iframe&quot; data-pluginname=&quot;videosnap&quot; data-headimgurl=&quot;https://wx.qlogo.cn/finderhead/oLU121BePGlK5gCzY9FXd7w0qDZtIDcFtaSTNicpYrEvHvAA2kfCLKg/0&quot; data-username=&quot;v2_060000231003b20faec8c7e48b1dc6dccd04e436b0771df137f3e0832e32a92192ea2165411c@finder&quot; data-nickname=&quot;博文视点Broadview&quot; data-desc=&quot;将在02月16日 20:00 直播&quot; data-intro=&quot;共话ChatGPT背后技术&quot; data-noticeid=&quot;finderlivenotice-v2_060000231003b20faec8c7e48b1dc6dccd04e436b0771df137f3e0832e32a92192ea2165411c@finder-1676283326520109-256135983&quot; data-type=&quot;live&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;1.7373333333333334&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/PW0wIHxgg3kMbkibZkKBXgOJtTn7YFj45eBQZMZzATVgXajwVCzWhQib9zGKyWibGfEQWpiae5ibHEuTxsmEkiamxOHQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;ChatGPT相关图书请戳&lt;/strong&gt;&lt;/span&gt;👇&lt;/p&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>3787efe13c8471f107cbbdb168013740</guid>
<title>谈 JVM 参数 GC 线程数 ParallelGCThreads 合理性设置</title>
<link>https://toutiao.io/k/n0dn3am</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;preview&quot;&gt;&lt;p&gt;作者：京东零售 刘乐&lt;/p&gt;

&lt;p&gt;导读：本篇文章聚焦JVM参数GC线程数的合理配置，从ParallelGCThreads参数含义、参数设置，到参数实验以及修改意见进行解析。&lt;/p&gt;

&lt;h1&gt;1. ParallelGCThreads参数含义&lt;/h1&gt;

&lt;p&gt;在讲这个参数之前，先谈谈JVM垃圾回收(GC)算法的两个优化标的：吞吐量和停顿时长。JVM会使用特定的GC收集线程，当GC开始的时候，GC线程会和业务线程抢占CPU时间，吞吐量定义为CPU用于业务线程的时间与CPU总消耗时间的比值。为了承接更大的流量，吞吐量越大越好。&lt;/p&gt;

&lt;p&gt;为了安全的垃圾回收，在GC或者GC某个阶段，所有业务线程都会被暂停，也就是STW（Stop The World)，STW持续时间就是停顿时长，停顿时长影响响应速度，因此越小越好。&lt;/p&gt;

&lt;p&gt;这两个优化目标是有冲突的，在一定范围内，参与GC的线程数越多，停顿时长越小，但吞吐量也越小。生产实践中，需要根据业务特点设置一个合理的GC线程数，取得吞吐量和停顿时长的平衡。&lt;/p&gt;

&lt;p&gt;目前广泛使用的GC算法，包括PS MarkSweep/PS Scavenge, ConcurrentMarkSweep/ParNew, G1等，都可以通过ParallelGCThreads参数来指定JVM在并行GC时参与垃圾收集的线程数。该值设置过小，GC暂停时间变长影响RT，设置过大则影响吞吐量，从而导致CPU过高。&lt;/p&gt;

&lt;h1&gt;2. ParallelGCThreads参数设置&lt;/h1&gt;

&lt;p&gt;GC并发线程数可以通过JVM启动参数: -XX:ParallelGCThreads=&lt;n&gt;来指定。在未明确指定的情况下，JVM会根据逻辑核数ncpus，采用以下公式来计算默认值：&lt;/n&gt;&lt;/p&gt;

&lt;p&gt;◦当ncpus小于8时，ParallelGCThreads = ncpus&lt;/p&gt;

&lt;p&gt;◦否则 ParallelGCThreads = 8 + (ncpus - 8 ) ( 5/8 )&lt;/p&gt;

&lt;p&gt;一般来说，在无特殊要求下，ParallelGCThreads参数使用默认值就可以了。&lt;strong&gt;但是在JRE版本1.8.0_131之前，JVM无法感知Docker的CPU限制，会使用宿主机的逻辑核数计算默认值。&lt;/strong&gt; 比如部署在128核物理机上的容器，JVM中默认ParallelGCThreads为83，远超过了容器的核数。过多的GC线程数抢占了业务线程的CPU时间，加上线程切换的开销，较大的降低了吞吐量。因此JRE 1.8.0_131之前的版本，未明确指定ParallelGCThreads会有较大的风险。&lt;/p&gt;

&lt;h1&gt;3. ParallelGCThreads参数实验&lt;/h1&gt;

&lt;p&gt;创建 8C12G 容器，宿主机是128C。模拟线上真实流量，采用相同QPS，观察及对比JVM YoungGC，JVM CPU，容器CPU等监控数据。场景如下：&lt;/p&gt;

&lt;p&gt;◦场景1: JVM ParallelGCThreads 默认值，QPS = 420，持续5分钟，CPU恒定在70%&lt;/p&gt;

&lt;p&gt;◦场景2: JVM ParallelGCThreads=8，QPS = 420，持续5分钟，CPU恒定在65%&lt;/p&gt;

&lt;p&gt;◦场景3: JVM ParallelGCThreads 默认值，QPS瞬时发压到420，&lt;strong&gt;前1min CPU持续100%&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;◦场景4: JVM ParallelGCThreads=8，QPS瞬时发压到420，&lt;strong&gt;前2s CPU持续100%，后面回落&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;从监控数据来看，各场景下CPU差距较明显，特别是场景3和场景4的对比。场景3由于GC线程过多，CPU持续100%时长达1分钟。可以得出以下两个结论：&lt;/p&gt;

&lt;p&gt;1.修改 ParallelGCThreads = 8后，同等QPS情况下，CPU会降低5%左右&lt;/p&gt;

&lt;p&gt;2.修改 ParallelGCThreads = 8后，&lt;strong&gt;瞬间发压且CPU打满情况下，CPU恢复较快&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/24fab31085a9415792cb2994cbab3332%7Etplv-k3u1fbpfcp-watermark.image?&quot; alt=&quot;3-9.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/874f02833cb94931b702905343bab0fe%7Etplv-k3u1fbpfcp-watermark.image?&quot; alt=&quot;10.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;图1: 容器CPU对比图：场景3(上)和场景4(下)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bb397bd2018e4640a05ac63a67370d27%7Etplv-k3u1fbpfcp-watermark.image?&quot; alt=&quot;11.png&quot;/&gt;
&lt;img src=&quot;https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a77617b15d6d4943baee223f550bf07d%7Etplv-k3u1fbpfcp-watermark.image?&quot; alt=&quot;12.png&quot;/&gt;
图2: JVM Young GC对比图：场景3(上)和场景4(下)&lt;/p&gt;

&lt;h1&gt;4. ParallelGCThreads修改建议&lt;/h1&gt;

&lt;p&gt;ParallelGCThreads配置存在风险的应用，修改方式为以下两种方案（任选一种）：&lt;/p&gt;

&lt;p&gt;◦升级JRE版本到&lt;strong&gt;1.8.0_131以上&lt;/strong&gt;，推荐1.8.0_192&lt;/p&gt;

&lt;p&gt;◦在JVM启动参数明确指定 -XX:ParallelGCThreads=&lt;n&gt;，N为下表的推荐值：&lt;/n&gt;&lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;容器核数&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;th&gt;16&lt;/th&gt;
&lt;th&gt;32&lt;/th&gt;
&lt;th&gt;64&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;推荐值&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;建议上下界&lt;/td&gt;
&lt;td&gt;1~2&lt;/td&gt;
&lt;td&gt;2~4&lt;/td&gt;
&lt;td&gt;4~8&lt;/td&gt;
&lt;td&gt;8~16&lt;/td&gt;
&lt;td&gt;16~32&lt;/td&gt;
&lt;td&gt;32~64&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>