<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>7959db143b757f9d1a3e69acab17c6dd</guid>
<title>15000 字拆解 TiDB 和 OceanBase 并细数四代分布式数据库的变迁（主从、中间件、KV、计算与存储分离、列存储、CAP定理）</title>
<link>https://toutiao.io/k/39yvx6o</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content js_underline_content             autoTypeSetting24psection&amp;#10;            &quot; id=&quot;js_content&quot;&gt;&lt;p&gt;&lt;strong&gt;本文大约 16000 字，阅读需要一个小时。&lt;/strong&gt;&lt;/p&gt;&lt;hr/&gt;&lt;section&gt;上一篇文章啃硬骨头差点把我牙给崩了，本文我们还是回到舒适区来，继续挥舞架构大棒，欺负可怜的小数据库。本文虽然不太硬，但是量还是很大的，管饱。&lt;/section&gt;&lt;p&gt;说到后端系统对于数据库的要求，基本上和你老板一样：既要又要还要。数据库扮演的那个单点角色，在单机上已经如此的困难了，换到分布式环境下只会更困难。而分布式数据库的出现也是被迫的：应用规模越来越大，对性能和可用性的要求越来越高，不得不搞分布式数据库了。&lt;/p&gt;&lt;p&gt;接下来请大家坐稳扶好，我们正式开始分布式数据库历史变迁之旅。&lt;/p&gt;&lt;h2&gt;单机数据库的不可能三角&lt;/h2&gt;&lt;p&gt;正如经济政策的不可能三角“不可能同时实现资本流动自由，货币政策的独立性和汇率的稳定”那样，单机数据库也有一个不可能三角，那就是：①持久化 ②事务隔离 ③高性能。&lt;/p&gt;&lt;h3&gt;为什么不可能&lt;/h3&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;持久化需要每一次写数据都要落到磁盘上，宕机再启动以后，数据库可以自修复。如果只要求这一条，很好实现。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;事务隔离需要每一次会话(session)的事务都拥有自己的数据库版本：既要多个并行的事务相互之间不会写到对方的虚拟数据库上(读提交)，又要不能读到对方的虚拟数据库上(可重复读)，还要在一个事务内不能读到别的事务已经提交的新增的数据(幻读)，终极需求则是完全串行化：我的读 session 不结束，你就不能读。这个需求和持久化需求结合以后，会大幅增加日志管理的复杂度，但，还是可以管理的。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;读写都要尽量地快：单独实现也很快，Redis 嘛，但是加上持久化和事务隔离，就很难做了：需要对前两项进行妥协。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;MySQL 选择了哪两个？&lt;/h3&gt;&lt;p&gt;MySQL 首先选择了持久化：失去人性，失去很多，失去持久化，失去一切。没有持久化能力，那还当个毛的核心数据库，所以这一条是所有磁盘数据库的刚需，完全无法舍弃。&lt;/p&gt;&lt;p&gt;然后 MySQL 选择了一部分高性能：MyISAM 就是为了快速读写而创造的，早期 MySQL 在低配 PC 机上就有不错的性能。后来更高级的 InnoDB 出现了，小数据量时它的读取性能不如 MyISAM，写性能更是彻底拉胯，但是在面对大数据量场景时，读性能爆棚，还能提供很多后端程序员梦寐以求的高级功能（例如丰富的索引），承担了大部分互联网核心数据库的角色。&lt;/p&gt;&lt;p&gt;最后，MySQL 将事务隔离拆成了几个级别，任君挑选：你要强事务隔离，性能就差；你能接受弱事务隔离，性能就强。你说无事务隔离？那你用 MySQL 干什么，Redis 它不香吗。&lt;/p&gt;&lt;p&gt;所以 MySQL 其实选择了 持久化*1 + 高性能*0.8 + 事务隔离*0.5，算下来，还赚了 0.3 (￣▽￣)&quot;&lt;/p&gt;&lt;p&gt;不过，从 MySQL 也可以看出，“数据库的不可能三角”并不是完全互斥的，是可以相互妥协的。&lt;/p&gt;&lt;p&gt;在开始细数分布式数据库之前，我们先看一个非分布式的提升数据库性能的方案，读写分离，主从同步。&lt;/p&gt;&lt;h2&gt;读写分离&lt;/h2&gt;&lt;p&gt;由于 web 系统中读写需求拥有明显的二八分特征——读取流量占 80%，写入流量占 20%，所以如果我们能把读性能拆分到多台机器上，在同样的硬件水平下，数据库总 QPS 也是能提高五倍的。&lt;/p&gt;&lt;h3&gt;各种主从架构&lt;/h3&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.40625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxEInYCCHaz651NtKMeNKxjDqiasAeDK5e9otYU99Als9LYQwjXbgQ4dA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;2176&quot;/&gt;&lt;/p&gt;&lt;p&gt;无论是远古时代谷歌的 MMM(Multi-Master Replication Manager for MySQL) 还是中古时代的 MySQL 官方的 MGR(MySQL Group Replication)，还是最近刚刚完成开发且收费的官方 InnoDB Cluster，这些主从架构的实现方式都是一致的：基于行同步或者语句同步，&lt;code&gt;近实时&lt;/code&gt;地从主节点向从节点同步新增和修改的数据。&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5428571428571428&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxXoYgACAHianBa8m9wY465kpZ9sIKgM1rYrHLeQNOzAGKc6XguIPjVgg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1400&quot;/&gt;&lt;/p&gt;&lt;p&gt;由于这种方法必然会让主从之间存在有一段时间的延迟(数百毫秒到数秒)，所以一般在主从前面还要加一个网关进行语句分发：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;select&lt;/code&gt;等读语句默认发送到从节点，以尽量降低主节点负载&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;一旦出现&lt;code&gt;update&lt;/code&gt;、&lt;code&gt;insert&lt;/code&gt;等些语句，立刻发送到主节点&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;并且，本次会话(session)内的所有后续语句，必须全部发送给主节点，不然就会出现数据写入了但是读不到的情况&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.7772020725388601&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxXiaicDXXZ8anUgVHr7XA5BeXyZvoFw7FHsucLnEPHicZR7NT7uMVWLo5g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1544&quot;/&gt;&lt;/p&gt;&lt;center&gt;一主四从架构图&lt;/center&gt;&lt;p&gt;搭建一个一主四从的 MySQL 集群，总 QPS 就能从单节点的 1 万提升到 5 万，顺便还能拥有主节点故障后高可用的能力。主从架构比较简单，也没有什么数据冲突问题，就是有一个很大的弱点：&lt;/p&gt;&lt;h4&gt;写入性能无法提升：由于数据库承载的单点功能实在是太多了(自增、时间先后、事务)，导致哪怕架构玩出了花，能写入数据的节点还是只能有一个，所有这些架构都只能提升读性能。&lt;/h4&gt;&lt;p&gt;那怎么提升写性能呢？这个时候就要掏出分布式数据库了。&lt;/p&gt;&lt;h2&gt;分布式数据库&lt;/h2&gt;&lt;p&gt;由于数据库的单点性非常强，所以在谷歌搞出 GFS、MapReduce、Bigtable 三驾马车之前，业界对于高性能数据库的主要解决方案是买 IOE 套件：IBM 小型机 + Oracle + EMC 商业存储。而当时的需求也确实更加适合商用解决方案。&lt;/p&gt;&lt;p&gt;后来搜索引擎成为了第一代全民网站，而搜索引擎的数据库却“不那么关系型”，所以谷歌搞出了自己的分布式 KV 数据库。后来谷歌发现 SQL 和事务隔离在很多情况下还是刚需，于是在 KV 层之上改了一个强一致支持事务隔离的 Spanner 分布式数据库。而随着云计算的兴起，分布式数据库已经成了云上的“刚需”：业务系统全部上云，总不能还用 Oracle 物理服务器吧？于是云上数据库又开始大踏步发展起来。&lt;/p&gt;&lt;p&gt;下面我们按照时间顺序，逐一梳理分布式数据库的发展史。&lt;/p&gt;&lt;h2&gt;第一代分布式数据库：中间件&lt;/h2&gt;&lt;h3&gt;在 MySQL 体系内演进&lt;/h3&gt;&lt;p&gt;关系型数据库为了解决不可能三角需求，其基本架构 40 年没有变过。&lt;/p&gt;&lt;p&gt;MySQL 自己其实已经是一个非常优秀的满足互联网业务场景的单体数据库了，所以基于 MySQL 的基本逻辑进行架构改进，是最稳妥的方案。&lt;/p&gt;&lt;p&gt;在没有分布式关系型数据库技术出现的时代，后端开发者们往往只能选择唯一的刀耕火种的路：在应用代码里调用多个数据库，以应对单个数据库性能不足的困境。后来，有人把这些调用多个数据的代码抽出来作为单独的一层，称作数据库中间件。&lt;/p&gt;&lt;h4&gt;数据库中间件&lt;/h4&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.7260034904013961&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxnibgjm2icd8Wxsx9RBwUCEVRCnBPjXmmbIqcib8dERzbxyfGtbr6mwefw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;3438&quot;/&gt;&lt;/p&gt;&lt;p&gt;首先，对数据表进行纵向分表：按照一定规则，将一张超多行数的表分散到多个数据库中。&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5630099728014506&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxnKXlPb1R1XLyK9k7Lf5lyN7MkLGNibZ2ZBnEWAwbfrPYyMiaypellaaQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;2206&quot;/&gt;&lt;/p&gt;&lt;center&gt;ShardingSphere 中的 Sharding-Proxy 工作方式&lt;/center&gt;&lt;p&gt;然后，无论是插入、更新还是查询，都通过一个 proxy 将 SQL 进行重定向和拆分，发送给多个数据库，再将结果聚合，返回。&lt;/p&gt;&lt;p&gt;大名鼎鼎的数据库中间件，其基本原理一句话就能描述：使用一个常驻内存的进程，假装自己是个独立数据库，再提供全局唯一主键、跨分片查询、分布式事务等功能，将背后的多个数据库“包装”成一个数据库。&lt;/p&gt;&lt;p&gt;虽然“中间件”这个名字听起来像一个独立组件，但实际上它依然是强业务亲和性的：没有几家公司会自己研发数据库，但每家公司都会研发自己的所谓中间件，因为中间件基本上就代表了其背后的一整套“多数据库分库分表开发规范”。所以，中间件也不属于“通用数据库”范畴，在宏观架构层面，它依然属于应用的一部分。我称这个时代为刀耕火种时代。&lt;/p&gt;&lt;p&gt;那该怎么脱离刀耕火种呢？人类的大脑是相似的：既然应用代码做数据规划和逻辑判断很容易失控，那我们在数据库层面把这件事接管了行不行呢？当然可以，但是需要拿东西&lt;code&gt;找信息之神交换&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;历史上，第一个被放弃的是&lt;code&gt;事务隔离&lt;/code&gt;，而它带来的就是第二代分布式数据库：KV 数据库。&lt;/p&gt;&lt;h2&gt;第二代分布式数据库：KV&lt;/h2&gt;&lt;h3&gt;分布式时代的“新·不可能三角”&lt;/h3&gt;&lt;p&gt;在分布式数据库时代，持久化已经不是分布式数据库“真正的持久化”了，取而代之的是“数据一致性”：由于数据存在了多台机器上，那机器之间数据的一致性就成了新时代的“持久化”。于是新不可能三角出现了：①一致性 ②事务隔离 ③高性能。&lt;/p&gt;&lt;p&gt;你是不是在期待 CAP 理论呀？别着急，我们后面会说。&lt;/p&gt;&lt;h3&gt;分布式 KV 数据库放弃了事务隔离&lt;/h3&gt;&lt;p&gt;数据库技术一共获得过四次图灵奖，后面三次都在关系型数据库领域。事务隔离模型是关系型数据库的核心，非常地简洁、优美、逻辑自恰。&lt;/p&gt;&lt;p&gt;Google 是第一个全民搜索引擎，系统规模也达到了史上最大。但是，搜索引擎技术本身却不需要使用关系型数据库来存储：搜索结果中的网页链接之间是离散的。这块我要挖个坑，本系列完结以后，我会写一篇如何自己开发搜索引擎的文章。&lt;/p&gt;&lt;p&gt;由于搜索不需要关系型数据库，自然谷歌搞的分布式数据库就是 KV 模型。谷歌的三驾马车论文发布以后，业界迅速发展出了一个新的数据库门类 NoSQL(Not Only SQL)，专门针对非结构化和半结构化的海量数据。&lt;/p&gt;&lt;p&gt;目前，缓存（Redis）和文档/日志（MongoDB）大家一般都会用 NoSQL 来承载。在这个领域，最成功的莫过于基于 Hadoop 生态中 HDFS 构建的 HBase 了：它主要提供的是行级数据一致性，即 CAP 理论中的 CP，放弃了事务，可以高性能地存储海量数据。&lt;/p&gt;&lt;p&gt;KV 数据库结构简单，性能优异，扩展性无敌，但是它只能作为核心数据库的高性能补充，绝大多数场景下，核心数据库还是得用关系型。&lt;/p&gt;&lt;h2&gt;第三代分布式数据库：以 Spanner 为代表的 NewSQL&lt;/h2&gt;&lt;p&gt;从 2005 年开始，Google Adwords 开始基于 MySQL 搭建系统，这也推动了 MySQL 在 Google 内部的大规模使用。随着业务的发展，MySQL 集群越来越庞大，其中最痛苦的就是“数据再分片”，据说有一次谷歌对数据库的重新分片持续了 2 年才完成。于是谷歌痛定思痛，搞出了一个支持分布式事务和数据强一致性的分布式关系型数据库：Google Spanner。&lt;/p&gt;&lt;p&gt;2012 年，谷歌发布了 Spanner 论文¹，拉开了分布式强一致性关系型数据库的大幕。这个数据库是真的牛逼，当我第一次看到它机柜照片的时候直接震惊了：&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;1.2247946228528752&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGx52eWItSoeExcaxiciblWuQeTGD9eayzKRvoPg7NUuVxkSOQTzcuRzSsg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1339&quot;/&gt;&lt;/p&gt;&lt;p&gt;这套系统采用了 GPS 授时 + 2 台原子钟 + 4 台时间服务器，让分布在全球多个数据中心的 Spanner 集群进行相当精确的时间同步：基于 TrueTime 服务，时间差可以控制在 10ms 之内。这种真正的全球数据库可以做到即使单个数据中心完全失效，应用也完全无感知。&lt;/p&gt;&lt;p&gt;当然，如此规模的全球数据库全世界也没有几家公司有需求，如果我们只在一个数据中心内署数据库集群，时间同步可以很容易地做到 1ms 之内，原子钟这种高端货还用不到。&lt;/p&gt;&lt;h3&gt;还记得上篇文章中数据持久性的关键步骤——redo log 吗？&lt;/h3&gt;&lt;p&gt;为什么上篇文章要写一万多字，就是为本文打基础的，现在就用到了。&lt;/p&gt;&lt;p&gt;上文中我们说过，写入型 SQL 会在写入缓存页 + 写入磁盘 redo log 之后返回成功，此时，真正的 ibd 磁盘文件并未更新。所以，Spanner 使用 Paxos 协议在多个副本之间同步 redo log，只要 redo log 没问题，多副本数据的最终一致性就没有问题。&lt;/p&gt;&lt;h3&gt;事务的两阶段提交&lt;/h3&gt;&lt;p&gt;由于分布式场景下写请求需要所有节点都完成才算完成，所以两阶段提交是必须存在的。单机架构下的事务，也是某一旦一条 SQL 执行出错，整个事务都是要回滚的嘛。多机架构下这个需求所需要的成本又大幅增加了，两阶段提交的流程是这样的：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;告诉所有节点更新数据&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;收集所有节点的执行结果，如果有一台返回了失败，则再通知所有节点，取消执行该事务&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这个简单模型拥有非常恐怖的理论故障概率：一旦在第一步执行成功后某台机器宕机，则集群直接卡死：大量节点会被锁住。&lt;/p&gt;&lt;p&gt;Spanner 使用 Paxos 化解了这个问题：只要 leader 节点的事务执行成功了，即向客户端返回成功，而后续数据的一致性则会基于&lt;code&gt;prepare timestamp&lt;/code&gt;和&lt;code&gt;commit timestamp&lt;/code&gt;加上 Paxos 算法来保证。&lt;/p&gt;&lt;h4&gt;多版本并发控制（MVCC）&lt;/h4&gt;&lt;p&gt;Spanner 使用时间戳来进行事务之间的 MVCC：为每一次数据的变化分配一个全球统一的时间戳。这么做的本质依然是“空间+时间”换时间，而且是拿过去的时间换现在的时间，特别像支持事后对焦的光场相机。&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;传统的单机 MVCC 是基于单机的原子性实现的事务顺序，再实现的事务隔离，属于即时判断。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Spanner 基于 TrueTime 记录下了每行数据的更新时间，增加了每次写入的时间成本，同时也增加了存储空间。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在进行多节点事务同步时，就不需要再和拥有此行数据的所有节点进行网络通信，只依靠 TrueTime 就可以用 Paxos 算法直接进行数据合并：基于时间戳判断谁前谁后，属于事后判断。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Spanner 放弃了什么&lt;/h3&gt;&lt;p&gt;Spanner 是一个强一致的全球数据库，那他放弃了什么呢？这个时候就需要 CAP 理论登场了。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Google Spanner 数据库首先要保证的其实是分区容错性，这是“全球数据库”的基本要求，也最影响他们赚钱；然后是一致性，“强一致”是核心设计目标，也是 Spanner 的核心价值；谷歌放弃的是可用性(A)，只有 majority available。&lt;/p&gt;&lt;p&gt;除此之外，为了“外部一致性”，即客户端看到的全局强一致性，谷歌为每一个事务增加了 2 倍的时钟延迟，换句话说就是增加了写操作的返回时间，这就是分布式系统的代价：目前平均 TrueTime 的延迟为 3.5ms，所以对 Spanner 的每一次写操作都需要增加 7ms 的等待时间。&lt;/p&gt;&lt;h4&gt;Spanner 一致性的根本来源&lt;/h4&gt;&lt;p&gt;大家应该都发现了，其实 Spanner 是通过给 Paxos 分布式共识算法加了一个“本地外挂” TrueTime 实现的海量数据的分布式管理，它实现全局强一致性的根本来源是&lt;code&gt;Paxos&lt;/code&gt;和&lt;code&gt;TrueTime&lt;/code&gt;。而在普通单机房部署的分布式系统中，不需要 GPS 授时和原子钟，直接搞一个时间同步服务就行。&lt;/p&gt;&lt;h3&gt;NewSQL 时代&lt;/h3&gt;&lt;p&gt;Google Spanner 的推出代表着一个新时代到来了：基于分布式技术的 SQL 兼容数据库（NewSQL），而兼容到什么地步就看各家的水平了。&lt;/p&gt;&lt;p&gt;NewSQL 最大的特点就是使用非 B 树磁盘存储结构（一般为 LSM-Tree），在上面构筑一个兼容 SQL 常用语句和事务的兼容层，这样既可以享受大规模 LSM-Tree 集群带来的扩展性和高性能，也可以尽量少改动现有应用代码和开发习惯，把悲伤留给自己了属于是。&lt;/p&gt;&lt;p&gt;目前比较常见的 NewSQL 有 ClustrixDB、NuoDB、VoltDB，国内的 TiDB 和 OceanBase 也属于 NewSQL，但他们俩有本质区别，下面的番外篇会讨论。&lt;/p&gt;&lt;p&gt;在 NewSQL 时代之后，随着云计算的兴起，云上数据库突然成为了市场的宠儿，市场占有率迅速上涨。它们其实都是对 MySQL 的改造，并不属于 NewSQL 范畴，下面我们认识一下他们。&lt;/p&gt;&lt;h2&gt;第四代分布式数据库：云上数据库&lt;/h2&gt;&lt;p&gt;我实在是不想用“云原生”这个风口浪尖上的词来形容美丽的云上数据库们，它们就像 TCP/IP，简洁但有用。市场从来不会说谎，它们一定是有过人之处的。&lt;/p&gt;&lt;h3&gt;亚马逊 Aurora 开天辟地&lt;/h3&gt;&lt;p&gt;2014 年 10 月，亚马逊发布了 Aurora 云上数据库，开创性地在云环境中将计算节点和存储节点分离：基于云上资源的特点，将计算节点 scale up（增配），将存储节点 scale out（增加节点），实现了极佳的性能/成本平衡。Aurora 将云上关系型数据库产品推向了一个新的高度。&lt;/p&gt;&lt;h3&gt;计算与存储分离&lt;/h3&gt;&lt;p&gt;Aurora 提出的计算与存储分离可以说是目前数据库领域最火的方向，但是它火的原因我相信大多数人都认识的不对：不是因为性能强，而是因为便宜。&lt;/p&gt;&lt;h4&gt;挖掘云计算的价值点&lt;/h4&gt;&lt;p&gt;十年前我在 SAE 实习的时候，中午大家一起吃饭，组长说云计算就是云安全，这句话当然说的很对。从这句话推开，我们很容易就能找到云计算真正的商业价值在哪里：传统托管式部署，哪些资源浪费的最多，哪里就是云计算的商业价值所在。&lt;/p&gt;&lt;p&gt;为了满足业务波动而多采购的 CPU 和内存，可能浪费了 50%；网络安全设备，可以说 95% 以上的资源都是浪费；高端存储，这个已经不能用资源浪费来形容了，而是被云计算颠覆了：云厂商用海量的多地域多机房内廉价的 x86 服务器里面的廉价磁盘，基于软件，构建出了超级便宜、多副本、高可用的存储，唯一的问题是性能不是太好。亚马逊 S3 和阿里云 OSS 就是最佳代表，可以说这类对象存储服务，其单价已经低于本地机房的 2.5 寸 SAS 机械磁盘了，更不要说本地机房还需要另外采购昂贵的存储控制器和 SAN 交换机了。&lt;/p&gt;&lt;h4&gt;云计算与特斯拉&lt;/h4&gt;&lt;p&gt;云数据库可以算是云服务厂商最重要的产品：受众足够广，成本足够低，性能足够高。这一点很像特斯拉汽车，时至今日，特斯拉依然在疯狂地想各种办法压低生产成本，虽然在降价，但是单车毛利依然维持在 30% 以上，是 BBA 的 2-3 倍。&lt;/p&gt;&lt;p&gt;Aurora 和 PolarDB 的核心价值是用一种低成本的方式，制造了一个 Oracle 要高成本才能做出来的软件和服务，这才是真的“创造价值”。&lt;/p&gt;&lt;h4&gt;计算与存储分离是一种“低成本”技术&lt;/h4&gt;&lt;p&gt;计算与存储分离并不是什么“高性能”技术，而是一种“低成本”技术：关系型数据的存储引擎 InnoDB 本身就是面向低性能的磁盘而设计的，而 CPU 和内存却是越快越好、越大越好，如果还把磁盘和 MySQL 进程部署在同一台物理机内，一定会造成磁盘性能的浪费。计算与存储分离的真正价值在于大幅降低了存储的成本。&lt;/p&gt;&lt;h4&gt;计算与存储分离的技术优势&lt;/h4&gt;&lt;p&gt;虽然说这个架构的主要价值在于便宜，但是在技术上，它也是有优势的：&lt;/p&gt;&lt;p&gt;它显著降低了传统 MySQL 主从同步的延迟。传统架构下，无论是语句同步还是行同步，都要等到事务提交后，才能开始同步，这就必然带来很长时间的延迟，影响应用代码的编写。而计算和存储分离之后，基于 redo log 传递的主从同步就要快得多了，从 1-2s 降低到了 100ms 以下。由于主从延迟降低，集群中从节点的个数可以提升，总体性能可以达到更高。&lt;/p&gt;&lt;h4&gt;Aurora 的主从同步机制&lt;/h4&gt;&lt;p&gt;看了上篇文章的人应该都知道，在更新数据时，主节点在完成了 redo log 写入，并对内存缓存 Buffer Pool 中相应的数据页进行修改之后，就会返回成功。这个内存缓存给 Aurora 从节点的数据更新造成了不小的影响：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;主从节点之间只有 redo log 传递&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;从节点在拿到 redo log 之后，会刷新自己 Buffer Pool 中存在的数据页，其它不存在的页的信息会丢弃&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;这带来了两个问题：&lt;/p&gt;&lt;/li&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;从节点的客户端在主从不同步的一段时间内，读到的是旧数据，这个需要网关或者应用代码来处理&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;从节点的 Buffer Pool 有效性变差，命中率下降，引发性能下降&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;h4&gt;Aurora 的架构局限&lt;/h4&gt;&lt;p&gt;Aurora 的出现确实独具慧眼，但是也会被时代所局限。&lt;/p&gt;&lt;p&gt;在 Aurora 论文²中，开篇就提到&lt;code&gt;Instead, the bottleneck moves to the network between the database tier requesting I/Os and the storage tier that performs these I/Os&lt;/code&gt;。Aurora 认为网络速度会成为云数据库的瓶颈，而在它研发的 2012-2013 年也确实如此，当时万兆网卡刚刚普及，CPU 单核性能也不足，软件也没有跟上，可以说速度比千兆网卡也快不了多少，所以亚马逊又搞了神奇的技术：存储节点具有自己将 redo log 写入 ibd 文件的能力。&lt;/p&gt;&lt;p&gt;由于这个神奇能力的存在，Aurora 的多机之间采用传输 redo log 的方式来同步数据，并用一种今天看起来不太靠谱的协议来保证最终一致性：consul 使用的那个 gossip 协议。由于 Aurora 采用六副本技术，所以每次写入都需要发起六次不怎么快的网络 IO，并且在其中 4 个成功以后才给客户端返回成功。Aurora 确实便宜，但是单节点的性能也确实捉鸡，这代表的就是写入性能差，进而限制了整个集群的性能上限。而且，经过比较长的时间(100ms)才能保证从&lt;code&gt;从节点&lt;/code&gt;上读到的数据是最新的，这会让主节点压力增大影响集群性能上限，或者让应用代码做长时间的等待，严重的会引起应用代码的执行逻辑变更，引入持久的技术债务。&lt;/p&gt;&lt;p&gt;那该怎么提升计算存储分离情况下的集群性能呢？我们看阿里云是怎么做的。&lt;/p&gt;&lt;h3&gt;阿里云 PolarDB 后来居上&lt;/h3&gt;&lt;p&gt;阿里云 RDS 集群的成本已经够低了，不需要再用计算存储分离技术降低成本了，而中国市场的用户，普遍需要高性能的 MySQL 数据库：ECS 价格太低了，如果不是运维方便和性能压力过大，谁愿意用你昂贵的数据库服务啊。&lt;/p&gt;&lt;p&gt;2015 年，PolarDB 开始研发，当时 25Gb RDMA 网络已经逐渐普及，所以阿里云将视角放在了网络速度之外：在 IO 速度没有瓶颈以后，基于内核提供的 syscall 所编写的旧代码将会成为新的性能瓶颈。&lt;/p&gt;&lt;p&gt;站在 2023 年初回头看，阿里云的判断是非常准确的。&lt;/p&gt;&lt;h4&gt;计算存储分离架构下的整体性能极限&lt;/h4&gt;&lt;p&gt;由于所有节点都使用了同一块“逻辑磁盘”，所以&lt;code&gt;双主可写&lt;/code&gt;想都不要想，一个计算存储分离的数据库集群的性能上限就是&lt;code&gt;主节点的写入性能上限&lt;/code&gt;。（Aurora 有多主可写数据库，对 ID 进行自动切分，使用时有一堆限制；PolarDB 也有多主可写数据库，但是更绝：每个库/表只支持绑定到一个可写节点，感情就是对多个数据库做了个逻辑聚合，还不如中间件呢。）&lt;/p&gt;&lt;p&gt;在主节点不接受读的情况下，主节点只承接写入操作，以及和写入操作在同一个会话 session 中的后续的读请求。&lt;/p&gt;&lt;p&gt;那 PolarDB 是如何提升主节点性能的呢？&lt;/p&gt;&lt;h4&gt;1. 共享的不是 redo log，而是 ibd 文件&lt;/h4&gt;&lt;p&gt;主从之间并不是依靠纯属 redo log 来同步数据的，而是直接共享同一个 ibd 文件，即真正的共享磁盘。而且，基于块设备的 Raft 算法也比基于文件的 gossip 协议要快很多。&lt;/p&gt;&lt;h4&gt;2. 绕过内核和网路栈：大幅提升存储性能，降低延迟，减少 CPU 消耗&lt;/h4&gt;&lt;p&gt;虽然对 redo log 的解析这一步在 Aurora 那边是存储做的，PolarDB 这边是主节点做的，看似增加了 CPU 消耗，但是这并不是真正的性能瓶颈所在，真正的瓶颈是网络栈和 UNIX 进程模型。看过我《性能之殇》系列文章的人应该都比较熟悉了，这是老生常谈了。那 PolarDB 是怎么优化的呢？&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;跳过 TCP/IP 网络栈，直接使用 RDMA 网络从存储节点读取数据，延迟暴降&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;跳过 kernel 的线程调度，自行开发绑定 CPU 核心的状态机，采用非阻塞 IO，在 CPU 占用下降的情况下，延迟进一步降低&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;3. 提出 ParallelRaft 协议，允许部分乱序提交&lt;/h4&gt;&lt;p&gt;ParallelRaft 协议让 Aurora 那边需要执行六次的网络 IO 变成了一次：只需要向 leader 节点写入成功，剩下的数据同步由 Raft 算法来执行，这和 Google Spanner 的两阶段提交优化是一个思路。&lt;/p&gt;&lt;p&gt;原始的 Raft 协议确实逻辑完备，实现简单，就是一个一个地协商太慢了。ParallelRaft 让收敛协商能够并行起来，加速 redo log 落入 ibd 文件的过程。&lt;/p&gt;&lt;h4&gt;4. 主从之间基于低延迟的共享存储同步 redo log 数据以刷新 Buffer Pool&lt;/h4&gt;&lt;p&gt;基于共享存储的低延迟优势，PolarDB 主从之间使用共享存储来同步 redo log 以刷新缓存，这一点逻辑上和 Aurora 一致，但是实际表现比较好，我实测主从同步时间在 20~70ms 范围内。&lt;/p&gt;&lt;h4&gt;5. 单机性能比标准 MySQL 更强&lt;/h4&gt;&lt;p&gt;RDMA 存储比本地存储更快，因为减少了计算和存储争抢中断的问题：IO 这个 CPU 不擅长的东西完全卸载给了 RDMA 网卡。同配置下 PolarDB 比标准 MySQL 的性能要高几倍。&lt;/p&gt;&lt;h4&gt;PolarDB 有这么多的优势，那它付出了什么代价呢？&lt;/h4&gt;&lt;p&gt;在各种实测里面，PolarDB 在相同规格下对其他的云上数据库都拥有 2 倍的性能优势，但是它基于 RDMA 存储的特点也让它付出了两个代价：1. 硬件成本高昂 2. 扩展性有上限。&lt;/p&gt;&lt;p&gt;是不是感觉很熟悉？Shared-Disk 的代表 Oracle RAC 也有这两个缺点。不知道大家有没有发现，PolarDB 就是云时代的 RAC 数据库：看起来是 Shared-Disk，其实是共享缓存让他们的性能变的超强。&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5353191489361702&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxF6z4KAmu4odptnTrY2Atibpp1fcA27Y3dPCErIQ3SCcWn4xXd6bEZww/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1175&quot;/&gt;&lt;/p&gt;&lt;center&gt;各代分布式数据库的兼容性/扩展性对比&lt;/center&gt;&lt;h4&gt;一句话概括 PolarDB：利用了高性能云存储并且做了性能优化的一主多从 MySQL 集群。&lt;/h4&gt;&lt;h3&gt;简单讨论一下 CAP 理论&lt;/h3&gt;&lt;p&gt;一个分布式系统中，不可能完全满足①一致性、②可用性、③分区容错性。我们以一个两地三中心的数据库为例：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;一致性：同一个时刻发送到三个机房的同一个读请求返回的数据必须一致（强一致读），而且磁盘上的数据也必须在一段时间后变的完全逻辑一致（最终一致）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;可用性：一定比例的机器宕机，其它未宕机服务器必须能够响应客户端的请求（必须是正确格式的成功或失败），这个比例的大小就是可用性级别&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;分区容错性：一个集群由于通信故障分裂成两个集群后，不能变成两个数据不一致的集群（脑裂），对外必须依然表现为一个逻辑集群&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在一个分布式数据库系统中，到底什么是可以放弃的呢？我觉得可以从分布式系统带来了什么优势这个问题开始思考。&lt;/p&gt;&lt;p&gt;相比于单体系统，一个分布式的数据库，在一致性上进步了吗？完全没有。在可用性上进步了吗？进步了很多。在分区容错性上呢？单体系统没有分区，不需要容错。所以，结论已经一目了然了：&lt;/p&gt;&lt;p&gt;①和③都是分布式系统带来的新问题，只有②是进步，那就取长补短，选择牺牲可用性来解决自己引发的一致性和分区容错性两个新问题。这也正是目前常见分布式数据库系统的标准做法。&lt;/p&gt;&lt;h2&gt;TiDB 和 OceanBase 该怎么选？&lt;/h2&gt;&lt;p&gt;TiDB 和 OceanBase 是目前中国 NewSQL 数据库的绝代双骄，争论一直不绝于耳。&lt;/p&gt;&lt;p&gt;TiDB 是承袭 Spanner 思想的 NewSQL，对 MySQL 的兼容性一般，基于&lt;code&gt;key+版本号&lt;/code&gt;的事务控制也比较弱，据说性能比较好，特别是写入性能。&lt;/p&gt;&lt;p&gt;OceanBase 是基于 Shared-Nothing 思想原生开发的分区存储数据库，其每个节点都支持完整的 SQL 查询，相互之间无需频繁通信。OceanBase 还支持多租户隔离，这明显就是为了云服务准备的(无论是公有云还是私有云)，和绝大多数企业无关。另外，OceanBase 对于 MySQL 的兼容性也几乎是 NewSQL 里面最高的，毕竟它需要支持支付宝的真实业务，兼容性是硬性要求，业务屎山可没人移得动 (づ｡◕‿‿◕｡)づ&lt;/p&gt;&lt;p&gt;下面我们详细对比一下两者的设计思路。&lt;/p&gt;&lt;h3&gt;TiDB 的设计思路&lt;/h3&gt;&lt;p&gt;我画的架构图如下：&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5806845965770171&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxqc7YXlTroSa9xSDXGZeH7iaQ4JS5CtshcafOvaqtMXCcicVBg3cTibS0A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1636&quot;/&gt;&lt;/p&gt;&lt;p&gt;上图中的“SQL 层”就是解析 SQL 语句并将其转化为 KV 命令的一层，是无状态的，下面的存储层才是核心，它叫 TiKV。&lt;/p&gt;&lt;h4&gt;TiKV 如何存储数据&lt;/h4&gt;&lt;p&gt;TiKV 官方原理图如下：&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.45519203413940257&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxHyAQEFSEo9VJoD2BxIekoPPT5yo17JOLKnhEiaLjulniarop1PBjTD7w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;703&quot;/&gt;&lt;/p&gt;&lt;p&gt;TiKV 是 TiDB 的核心组件，一致性和事务隔离都是基于它的能力得以实现的。每个 TiKV 拥有两个独立的 RocksDB 实例，一个用于存储 Raft Log，另一个用于存储用户数据和多版本隔离数据（基于&lt;code&gt;key+版本号&lt;/code&gt;实现），从这里可以看出，TiDB 的存储存在大量冗余，所以 TiDB 的测试性能才会如此的高，符合空间换时间的基本原理。&lt;/p&gt;&lt;p&gt;和 TiKV 并列的还有一个 TiFlash 列存储引擎，是为了 OLAP 在线数据分析用的，我们在此不做详细讨论。&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5709534368070953&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxWo4LwkDsSKlTbsmABQaibevVw2tiaPlT57w6CHkPsQu6eT6I14WjsAdw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;3608&quot;/&gt;&lt;/p&gt;&lt;center&gt;TiKV 数据分片&lt;/center&gt;&lt;p&gt;除此之外，TiKV 还发明了一层虚拟的“分片”(Region)，将数据切分成 96MB~144MB 的多个分片，并且用 Raft 算法将其分散到多个节点上存储。注意，在 TiKV 内部存储用户数据的那个 RocksDB 内部，多个分片是致密存储的，分片之间并没有逻辑关系。&lt;/p&gt;&lt;p&gt;TiDB 的实现风格比较狂野，所以不兼容的部分比较多：&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.6810193321616872&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxg3s3W4xBwwRTbdNAtlibgic9BLpf9OyU6MFIotnyJFSXEB5jAeST2dAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;2276&quot;/&gt;&lt;/p&gt;&lt;center&gt;TiDB 和 MySQL 不兼容的部分&lt;/center&gt;&lt;h4&gt;TiDB 对 CAP 和不可能三角的抉择&lt;/h4&gt;&lt;p&gt;TiDB 放弃了新不可能三角中的事务隔离，和 Spanner 一样放弃了 CAP 理论中的“完全可用性”：一旦出现脑裂，就会出现意外的返回结果（如超时），因为 TiDB 选择了保证一致性：如果无法达到数据强一致，就要停止服务。&lt;/p&gt;&lt;h4&gt;一句话概括 TiDB：①搭建在 KV 数据库集群之上的②兼容部分 MySQL 语法的③有一些事务处理能力的高性能数据库。&lt;/h4&gt;&lt;h3&gt;OceanBase 设计思路&lt;/h3&gt;&lt;p&gt;我们以最新的 OceanBase 4.0 版本的架构为目标进行讨论。&lt;/p&gt;&lt;p&gt;TiDB 底层数据叫分片，那 OceanBase 为什么叫分区呢？因为分片的意思只是数据被分开了（本身 KV 数据之间也没关系），但分区表示的是分区内部的数据之间是有联系的：OceanBase 的每个节点本身，依然是一个关系型数据库，拥有自己的 SQL 引擎、存储引擎和事务引擎。&lt;/p&gt;&lt;h4&gt;简单的分区&lt;/h4&gt;&lt;p&gt;OceanBase 在建表时就需要设定数据分区规则，之后每一行数据都属于且仅属于某个分区。在数据插入和查询的时候，需要找到这一行数据所在的区，进行针对性地路由。这和第一代分布式——中间件的思想一致。这么做相当于简单地并行执行多条 SQL，以数据切分和数据聚合为代价，让数据库并行起来。而这个数据切分和数据聚合的代价，可以很小，也可以很大，需要 OceanBase 进行精细的性能优化，我们下面还会说到。&lt;/p&gt;&lt;p&gt;分区之间，通过 Multi-Paxos 协议来同步数据：每一个逻辑分区都会有多个副本分布在多台机器上，只有其中一个副本会成为 leader，并接受写请求。这里的架构和 PolarDB 一样了，此时，客户端的一致性读需要网关(OBProxy)来判断，主从之间的同步是有可感知的延迟的。&lt;/p&gt;&lt;h4&gt;节点存储架构&lt;/h4&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.39636363636363636&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxGyox36znxdNI3NZEXicUibUiarmbbF0c4j1YSLZ6ZoMsNFQYII7HQkEGA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1650&quot;/&gt;&lt;/p&gt;&lt;center&gt;官方存储架构图&lt;/center&gt;&lt;blockquote&gt;&lt;p&gt;OceanBase 数据库的存储引擎基于 LSM Tree 架构，将数据分为静态基线数据（放在 SSTable 中）和动态增量数据（放在 MemTable 中）两部分，其中 SSTable 是只读的，一旦生成就不再被修改，存储于磁盘；MemTable 支持读写，存储于内存。数据库 DML 操作插入、更新、删除等首先写入 MemTable，等到 MemTable 达到一定大小时转储到磁盘成为 SSTable。在进行查询时，需要分别对 SSTable 和 MemTable 进行查询，并将查询结果进行归并，返回给 SQL 层归并后的查询结果。同时在内存实现了 Block Cache 和 Row cache，来避免对基线数据的随机读。&lt;/p&gt;&lt;p&gt;当内存的增量数据达到一定规模的时候，会触发增量数据和基线数据的合并，把增量数据落盘。同时每天晚上的空闲时刻，系统也会自动每日合并。&lt;/p&gt;&lt;p&gt;OceanBase 数据库本质上是一个基线加增量的存储引擎，在保持 LSM-Tree 架构优点的同时也借鉴了部分传统关系数据库存储引擎的优点。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;以上是官方描述，我本来想简化一下，读了一遍觉得还是放原文吧，原文描述的就非常的清晰精炼了：OceanBase 用内存 B+ 树和磁盘 LSM-Tree 共同构成了数据读写体系，和上一篇文章中的 InnoDB 是多么像啊！只是 OceanBase 做的更细：跟 TiDB 相比，就像是在 TiKV 上面加了一层 Buffer Pool 一样。&lt;/p&gt;&lt;p&gt;还有一个细节：OceanBase 除了记录日志(Redo Log)并修改内存缓存(MemTable)之外，只要内存充足，白天 OceanBase 不会主动将内存缓存中的数据刷洗到 SSTable 里的，官方更推荐每天凌晨定时刷洗。这是什么思想？可以说是空间(内存)换时间，也可以说是拿未来的时间换现在的时间。&lt;/p&gt;&lt;h4&gt;OceanBase 性能来源之一——充分的内存缓存&lt;/h4&gt;&lt;p&gt;从基础电气属性上讲，磁盘一定是慢的，内存一定是快的，所以在数据量大于机器的内存容量时，各种数据库的性能差别可以聚焦到一个点上：内存利用效率，即热数据命中内存缓存的概率。&lt;/p&gt;&lt;p&gt;为了提升缓存命中率，OceanBase 设计了很多层内存缓存，尽全力避免了对磁盘的随机读取，只让 LSM-Tree 的磁盘承担它擅长的连续读任务，包子有肉不在褶上，商用环境中捶打过的软件就是不一样，功夫都在细节里：&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.3379396984924623&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxS5kjFkzEj1f4hy2rZ5atR4tjibRRbXvFuK3RIyvJBqib2LpphFu6qdhQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1592&quot;/&gt;&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;BloomFilter Cache：布隆过滤器缓存&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;MemTable：同时使用 B+ 树和 HashTable 作为内存引擎，分别处理不同的场景&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Row Cache：存储 Get/MultiGet 查询的结果&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Block Index Cache：当需要访问某个宏块的微块时，提前装载这个宏块的微块索引&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Block Cache：像 Buffer Pool 一样缓存数据块(InnoDB 页)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Fuse Row Cache：在 LSM-Tree 架构中, 同一行的修改可能存在于不同的 SSTable 中，在查询时需要对各个 SSTable 查询的结果进行熔合，对于熔合结果缓存可以更大幅度地支持热点行查询&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Partition Location Cache：用于缓存 Partition 的位置信息，帮助对一个查询进行路由&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Schema Cache：缓存数据表的元信息，用于执行计划的生成以及后续的查询&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Clog Cache：缓存 clog 数据，用于加速某些情况下 Paxos 日志的拉取&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;OceanBase 性能来源之二——直接变身内存数据库&lt;/h4&gt;&lt;p&gt;为了极致的性能，OceanBase 直接取消了 MySQL 中“后台进程每秒将 redo log 刷写到 ibd 文件”这一步，等于放大了集群宕机重启后恢复数据的时间（重启后需要大量的时间和磁盘 IO 将 redo log 刷写入磁盘），然后把这件事放到半夜去做：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;当内存的增量数据达到一定规模的时候，会触发增量数据和基线数据的合并，把增量数据落盘。同时每天晚上的空闲时刻，系统也会自动每日合并。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;把一整天的新增和修改的数据全部放到内存里，相当于直接变身成了内存数据库（还会用 B 树和 Hashtable 存两份），确实是一种终极的性能优化手段，OceanBase 真有你的。&lt;/p&gt;&lt;h4&gt;填坑：OceanBase 如何提升并行查询和数据聚合的性能&lt;/h4&gt;&lt;p&gt;传统的中间件 Sharding 技术中，也会做一些并行查询，但是他们做的都是纯客户端的查询：proxy 作为标准客户端，分别从多台机器拿到数据之后，用自己的内存进行数据处理，这个逻辑非常清晰，但有两个问题：1. 只能做简单的并行和聚合，复杂的做不了 2. 后端数据库相互之间无通信，没有很好地利用资源，总响应时间很长。&lt;/p&gt;&lt;p&gt;OceanBase 让一切尽可能地并行起来了：在某台机器上的 proxy(OBServer) 接到请求以后，它会担任协调者的角色，将任务并行地分发到多个其他的 OBServer 上执行；同时，将多个子计划划分到各个节点上以后，会在各节点之间建立 M*N 个网络通信 channel，并行地传输信息；此外，OceanBase 还对传统数据库的执行计划优化做了详细的拆分，对特定的关键词一个一个地做分布式优化，才最终达成了地表最强的成就。&lt;/p&gt;&lt;h4&gt;由于本身就是为了兼容 MySQL 而设计的一种新技术实现，所以它拥有非常优秀的兼容性：&lt;/h4&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.22916666666666666&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/qehPQXlzsA23ANfc7akZ1PnsJAoDDTGxPoEicQicBtu9LZrfb3Umen0N7iaXgUSQtDFVgTp39HLecntiaF9kr1fEmQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;2016&quot;/&gt;&lt;/p&gt;&lt;center&gt;OceanBase 和 MySQL 不兼容的部分&lt;/center&gt;&lt;h4&gt;OceanBase 对 CAP 和不可能三角的抉择&lt;/h4&gt;&lt;p&gt;由于数据是分区的，所以当脑裂时，两个大脑的数据肯定已经不完整了，相当于两万行的表只剩一万行数据可以进行查询和更新，此时，如果 OceanBase 梗着脖子非要追求数据强一致，也是可以让所有的 OBProxy 拒绝服务的，但是 OceanBase 选择了继续苟着：互联网公司嘛，能实现最终一致性就行了，要啥自行车。&lt;/p&gt;&lt;p&gt;OceanBase 放弃了 CAP 和新不可能三角中的一致性，只能做到最终一致性：为了事务隔离和性能，哥忍了。&lt;/p&gt;&lt;p&gt;其实，不追求强一致和我们下一篇《站在地球表面》中的终极高并发架构在思想上是一致的，我想这就是经历过大规模生产应用的数据库，被现实世界毒打过后的痕迹吧。&lt;/p&gt;&lt;p&gt;&lt;span&gt;一句话概括 OceanBase：①世界第一性能的②高度兼容 MySQL 的③经历过生产系统考验的高性能分布式关系型数据库。&lt;/span&gt;&lt;/p&gt;&lt;h2&gt;分布式数据库，应该怎么选？&lt;/h2&gt;&lt;p&gt;其实，分布式数据库根本就轮不到你来选：应用准备好了吗？有足够的研发资源吗？性能问题已经大到压倒其他需求了吗？&lt;/p&gt;&lt;p&gt;如果你有一个正在成长的业务，影响最小、成本最低的方案就是选择 Aurora/PolarDB 这种高兼容性数据库，等到这类云数据库的主节点达到性能上限了，再对应用做逐步改造，滚动式地替换每个部分的数据库依赖。&lt;/p&gt;&lt;p&gt;如果压力大到必须换分布式数据库技术方案了，再看看你能获得什么样的分布式数据库呢？无非是在哪个云平台就用哪家呗。&lt;/p&gt;&lt;h2&gt;番外篇&lt;/h2&gt;&lt;h3&gt;Shared-Nothing、Shared-Memory 和 Shared-Disk&lt;/h3&gt;&lt;p&gt;Shared-Nothing 只是一种思想，并不是一种明确的数据库架构，它非常笼统，只是描述了一种状态。在这里我们简单讨论一下 Shared-Nothing。&lt;/p&gt;&lt;p&gt;Shared-Nothing 描述的是一种分布式数据库的运行状态：两台物理机，除了网络通信之外，不进行任何资源共享，CPU、内存、磁盘都是独立的。这样，整个系统的理论性能就可以达到单机的二倍。&lt;/p&gt;&lt;p&gt;怎么理解 Shared-Nothing 思想呢？把它和 Shared-Disk 放到一起就明白了：&lt;/p&gt;&lt;p&gt;Shared-Disk：多台机器通过共享 SAN 磁盘的方式协同工作，让系统整体性能突破单机的极限。Oracle RAC 是这个架构的佼佼者，不过它的成功并不在于磁盘，而在于它的分布式锁(CACHE FUSION)：RAC 利用时间戳和分布式锁实现了分布式事务和多台机器同时可写，大幅提升了集群的性能。注意，时间戳在这里又出现了。CACHE FUSION 其实已经可以被称作 Shared-Memory 了。感兴趣的可以自己了解，我们不再深入。&lt;/p&gt;&lt;p&gt;21 世纪初，Oracle 推出了 Shared-Disk 的 RAC，IBM 推出了 Shared-Nothing 的 DB2 ICE。十年后，Oracle RAC 发展的如火如荼，而 DB2 ICE 已经消失在了历史的长河中。&lt;/p&gt;&lt;p&gt;但是，2012 年 Google 发布了 Spanner 论文，在非常成熟的世界上最大规模的 KV 数据库之上，构建 SQL 层，实现持久化、事务和多版本并发控制，扛起了 Shared-Nothing 技术方向的大旗，直到今天。&lt;/p&gt;&lt;h3&gt;MongoDB 小故事&lt;/h3&gt;&lt;p&gt;十年前我在新浪云(SAE)实习的时候，听过一个关于 MongoDB 的技术小故事：当时，SAE 的 KV 服务是使用 MongoDB 实现的，在规模大到一定程度以后，性能会突然下降，SAE 自己解决不了这个问题，就给 MongoDB 开发组的各位大哥买机票请他们到北京理想国际大厦 17 层现场来帮忙，研究了几天，MongoDB 开发组的人说：你们换技术吧，MongoDB 解决不了你们这个规模的问题，然后 SAE 的 KV 就更换技术方案来实现了。&lt;/p&gt;&lt;h3&gt;DBA 晕倒砸烂花盆&lt;/h3&gt;&lt;p&gt;也是在 SAE，我坐在厕所附近临过道的工位（上厕所很方便），某天早上刚上班，我亲眼看到 SAE 的一名 MySQL DBA 从厕所里出来后，晕倒在我面前，砸烂了一个大花盆。数据库作为系统架构中最重要的那个单点的残酷，可见一斑。&lt;/p&gt;&lt;h3&gt;列存储思想&lt;/h3&gt;&lt;p&gt;与其将列存储认定为数据库的一种，我倒觉得它更应该被称作一种思想：观察数据到底是被如何读取，并加以针对性地优化。&lt;/p&gt;&lt;p&gt;列存储有点像第一性原理在数据库领域的应用：不被现实世界所束缚，没有屈服于 B 树和它亲戚们的淫威，勇敢地向更底层看去，思考着在我们大量读取数据时，数据怎样组织才能读的更快。&lt;/p&gt;&lt;p&gt;在读取一行数据时，显然 B+ 树的效率无人能及，但是当我们需要读取 100 万行数据中的某一列时，B+ 树就需要把这 100 万行数据全部载入内存：每次将一页 16KB 载入内存，循环这一页内的 14 行数据，把这个特定的字段复制出来；重复执行这个操作 71429 次，才能得到我们想要的结果。这显然是 B+ 树非常不擅长的需求。&lt;/p&gt;&lt;p&gt;而列存储将数据基于行的排布翻转过来了：所有数据基于列，致密地排列在磁盘上，这样对某一列的读取就变成了磁盘顺序读，即便是机械磁盘，顺序读也非常快。&lt;/p&gt;&lt;h4&gt;列存储数据库 clickhouse 颇有毛子暴力美学的典范，和 Nginx 的气质很像&lt;/h4&gt;&lt;p&gt;clickhouse 推荐使用尽量多的 CPU 核心，对单核性能无要求，我拿 E5-V2 旧服务器测过，速度确实非常惊人，8000 万行的表，查询起来不仅比 MySQL 快，比 Hadoop 也快特别多。&lt;/p&gt;&lt;h3&gt;还记得我们的目标吗？五百万数据库 QPS&lt;/h3&gt;&lt;p&gt;在中国，我们现在有下面两种方案可以选择：&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;OceanBase 已经蝉联 TPC-C 数年的全球冠军了，每分钟可以处理 7.07 亿个订单，每秒订单数都已经过千万了，更不要说 QPS 500 万了，所以，如果你用 OceanBase，你的百万 QPS 的高并发系统已经搭建完成了！:-D&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果你用阿里云，那 1 主 4 从，88 vCore 710 GB * 5 个节点的 PolarDB 集群可以跑到大约 200 万 QPS³。那离 500 万还有不小的距离呢，不要着急，我们下篇文章解决这个问题。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;接下来&lt;/h3&gt;&lt;p&gt;接下来就是本系列最后一篇文章了：我们不仅要用架构顶住五百万数据库 QPS，还会找出一个哲学♂办法，打造能够服务全人类的系统。&lt;/p&gt;&lt;h3&gt;参考资料&lt;/h3&gt;&lt;ol class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;p&gt;Google Spanner 论文（中文版） https://ying-zhang.github.io/time/2013-Spanner-cn.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;亚马逊 Aurora 论文 https://web.stanford.edu/class/cs245/readings/aurora.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;复盘：我在真实场景下对几款主流云原生数据库进行极限性能压测的一次总结 https://cloud.tencent.com/developer/article/2066823&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;mp-style-type data-value=&quot;3&quot;/&gt;&lt;/p&gt;&lt;/div&gt;

          
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>35bbbb3cd83b3863fa5310a98e705be3</guid>
<title>从历代 GC 算法角度刨析 ZGC</title>
<link>https://toutiao.io/k/led7tbg</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;preview&quot;&gt;&lt;p&gt;&lt;strong&gt;作者：京东科技 文涛&lt;/strong&gt;&lt;/p&gt;

&lt;h1&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;本文所有介绍仅限于HotSpot虚拟机，
本文先介绍了垃圾回收的必要手段，基于这些手段讲解了历代垃圾回收算法是如何工作的，
每一种算法不会讲的特别详细，只为读者从算法角度理解工作原理，从而引出ZGC，方便读者循序渐进地了解。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GC是Garbage Collection的缩写，顾名思义垃圾回收机制，即当需要分配的内存空间不再使用的时候，JVM将调用垃圾回收机制来回收内存空间。&lt;/p&gt;

&lt;p&gt;那么JVM的垃圾机制是如何工作的呢？&lt;/p&gt;

&lt;p&gt;第一步识别出哪些空间不再使用（识别并标记出哪些对象已死）；&lt;/p&gt;

&lt;p&gt;第二步回收不再使用空间（清除已死对象 ）&lt;/p&gt;

&lt;h1&gt;&lt;strong&gt;判断对象是否已死&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;判断对象是否已死通常有两种方式 ,引用计数法和可达性分析法&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;引用计数法&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;给对象中添加一个引用计数器,每当有一个地方引用它时,计数器值就加1:当引用失效时,计数器值就减1;任何时刻计数器为0的对象就是不能再被使用的。&lt;/p&gt;

&lt;p&gt;简单高效，但无法解决循环引用问题，a=b,b=a&lt;/p&gt;

&lt;p&gt;引用计数法并没有在产品级的JVM中得到应用&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;可达性分析法&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;这个算法的基本思路就是通过一系列的称为“ GC Roots”的对象作为起始点,从这些节点开始向下搜索,搜索所走过的路径称为引用链( Reference Chain),当一个对象到 GC Roots没有任何引用链相连(用图论的话来说,就是从 GC Roots到这个对象不可达)时,则证明此对象是不可用的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-63a6e07b33c15c401e93efdc4e2ff512cfe.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;不过可达性算法中的对象并不是立即死亡的，对象拥有一次自我拯救的机会，对象被系统宣告死亡至少要经历两次标记过程，第一次是经过可达性分析之后没有与GC Roots相连的引用链，第二次是在由虚拟机自动建立的Finalize队列中判断是否需要执行finalize()方法。&lt;/p&gt;

&lt;p&gt;HotSopt虚拟机采用该算法。&lt;/p&gt;

&lt;h1&gt;&lt;strong&gt;清除已死对象的方式&lt;/strong&gt;&lt;/h1&gt;

&lt;h2&gt;&lt;strong&gt;标记清除算法&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;先标记再清除&lt;/p&gt;

&lt;p&gt;不足：1 效率问题，标记和清除效率都不高。2 空间问题，产生大量空间碎片&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;复制算法&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;内存分两块，A,B&lt;/p&gt;

&lt;p&gt;A用完了，将存活对象拷贝到B,A清理掉&lt;/p&gt;

&lt;p&gt;代价：内存少了一半。&lt;/p&gt;

&lt;p&gt;HotSopt虚拟机用此算法回收新生代。将新生代内存划分为8：1：1的Eden和Survivor解决复制算法内存使用率低的问题&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-fa45c48c40004eaae7966ab9b229b40f8a3.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;标记整理算法&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;老年代使用，方式和标记清除类似，只是不直接清除，而是将后续对象向一端移动，并清理掉边界以外的内存。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-754d7ae260f04360b22a4f57eec0677ab48.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;分代收集算法&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;分代收集是一个算法方案，整合了以上算法的优点，一般是把Java堆分为新生代和老年代,在新生代中,使用复制算法老年代“标记一清理”或者“标记一整理”&lt;/p&gt;

&lt;h1&gt;&lt;strong&gt;历代垃圾收集器简介&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;通过上文我们了解了怎样识别垃圾，怎样清理垃圾，接下来，讲ZGC之前，我们回顾一下历代垃圾回收是怎样做的，主要是想给读者一种历史的视角，任何技术都不是凭空产生的，更多的是在前人成果之上进行优化整合&lt;/p&gt;

&lt;p&gt;我们先看一个历代JDK垃圾收集器对比表格，以下表格着重说明或引出几个问题：&lt;/p&gt;

&lt;p&gt;1 CMS从来未被当作默认GC，且已废弃&lt;/p&gt;

&lt;p&gt;2 CMS的思想其实部分被ZGC吸收，CMS已死，但他的魂还在&lt;/p&gt;

&lt;p&gt;3 JDK11、JDK17为长期迭代版本，项目中应优先使用这两个版本&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img.toutiao.io/attachment/5f8f7617e1624254b486ffe1524ddeba/w600&quot; alt=&quot;file&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;GC分类&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;我们经常在各种场景听到以下几种GC名词，Young GC、Old GC、Mixed GC、Full GC、Major GC、Minor GC，他们到底什么意思，本人进行了以下梳理&lt;/p&gt;

&lt;p&gt;首先GC分两类，Partial GC（部分回收），Full GC&lt;/p&gt;

&lt;p&gt;Partial GC：并不收集整个GC堆的模式，以下全是Partial GC的子集&lt;/p&gt;

&lt;p&gt;Young GC：只收集young gen的GC&lt;/p&gt;

&lt;p&gt;Old GC：只收集old gen的GC。只有CMS的concurrent collection是这个模式&lt;/p&gt;

&lt;p&gt;Mixed GC：只有G1有这个模式，收集整个young gen以及部分old gen的GC。&lt;/p&gt;

&lt;p&gt;Minor GC：只有G1有这个模式，收集整个young gen&lt;/p&gt;

&lt;p&gt;Full GC：收集整个堆，包括young gen、old gen、perm gen（如果存在的话）等所有部分的模式。&lt;/p&gt;

&lt;p&gt;Major GC：通常是跟full GC是等价的&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;serial收集器&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;单线程收集器，“单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作,更重要的是在它进行垃圾收集时,必须&lt;strong&gt;暂停其他所有的工作线程&lt;/strong&gt;,直到它收集结束。它依然是虚拟机运行在 Client模式下的默认新生代收集器。它也有着优于其他收集器的地方:简单而高效(与其他收集器的单线程比),对于限定单个CPU的环境来说, Serial I收集器由于没有线程交互的开销,专心做垃圾收集自然可以获得最高的单线程收集效率。&lt;/p&gt;

&lt;p&gt;下图彩色部分说明了它的算法，简单粗暴&lt;/p&gt;

&lt;p&gt;1 停止用户线程&lt;/p&gt;

&lt;p&gt;2 单线程垃圾回收新生代&lt;/p&gt;

&lt;p&gt;3 重启用户线程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-012d8f19d0603ea725eb40ad8589b203073.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;ParNew收集器&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Parnew收集器其实就是 Serial l收集器的多线程版本。它是许多运行在 Server模式下的虚拟机中首选的新生代收集器,其中有一个与性能无关但很重要的原因是,除了 Serial 收集器外,目前只有它能与CMS收集器配合工作。Pardew收集器在单CPU的环境中绝对不会有比 Serial收集器更好的效果。它默认开启的收集线程数与CPU的数量相同,在CPU非常多(臂如32个)的环境下,可以使用-XX: ParallelGCThreads参数来限制垃圾收集的线程数。&lt;/p&gt;

&lt;p&gt;ParNew收集器&lt;strong&gt;追求降低GC时用户线程的停顿时间,&lt;/strong&gt;适合交互式应用,良好的反应速度提升用户体验.&lt;/p&gt;

&lt;p&gt;下图彩色部分说明了它的算法，同样简单粗暴&lt;/p&gt;

&lt;p&gt;1 停止用户线程&lt;/p&gt;

&lt;p&gt;2 多线程垃圾回收新生代&lt;/p&gt;

&lt;p&gt;3 重启用户线程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-2c119f92ead677b561299a76bed048a7656.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Parallel Scavenge 收集器&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Parallel Scavenge收集器是一个新生代收集器,它也是使用复制算法的收集器,又是并行的多线程收集器。算法的角度它和ParNew一样，在此就不画图解释了&lt;/p&gt;

&lt;p&gt;Parallel Scavenge收集器的目标则是达到&lt;strong&gt;一个可控制的吞吐量&lt;/strong&gt;( Throughput)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;吞吐量是指用户线程运行时间占CPU总时间的比例&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;通过以下两种方式可达到目的：&lt;/p&gt;

&lt;p&gt;1.在多CPU环境中使用多条GC线程,从而垃圾回收的时间减少,从而用户线程停顿的时间也减少;&lt;/p&gt;

&lt;p&gt;2.实现GC线程与用户线程并发执行。&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Serial Old收集器&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Serial Old是 Serial 收集器的老年代版本,它同样是一个单线程收集器,使用“标记整理”算法。这个收集器的主要意义也是在于给 Client模式下的虚拟机使用。&lt;/p&gt;

&lt;p&gt;如果在 Server模式下,那么它主要还有两大用途:&lt;/p&gt;

&lt;p&gt;一种用途是在JDK1.5以及之前的版本中与 ParallelScavenge收集器搭配使用,&lt;/p&gt;

&lt;p&gt;另一种用途就是作为CMS收集器的后备预案,在并发收集发生Concurrent Mode Failure时使用&lt;/p&gt;

&lt;p&gt;下图彩色部分说明了它的算法，同样简单粗暴&lt;/p&gt;

&lt;p&gt;1 停止用户线程&lt;/p&gt;

&lt;p&gt;2 单线程垃圾回收老年代&lt;/p&gt;

&lt;p&gt;3 重启用户线程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-7c1522951dde9768b14b6f4a3e49d767626.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Parallel Old收集器&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Paralle Old是 Parallel Scavenge收集器的老年代版本,一般它们搭配使用,追求CPU吞吐量,使用多线程和“标记一整理”算法。&lt;/p&gt;

&lt;p&gt;下图彩色部分说明了它的算法，同样简单粗暴&lt;/p&gt;

&lt;p&gt;1 停止用户线程&lt;/p&gt;

&lt;p&gt;2 多线程垃圾回收老年代&lt;/p&gt;

&lt;p&gt;3 重启用户线程&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-9e688d2f440308ff08a15794bbecaca2f33.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;CMS收集器&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;以上5种垃圾回收原理不难理解，算法之所以如此简单个人理解在当时使用这种算法就够了，随着JAVA的攻城略地，有一种垃圾回收需求出现，即使用尽量短的回收停顿时间，以避免过久的影响用户线程，CMS登场了。&lt;/p&gt;

&lt;p&gt;CMS( Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。&lt;/p&gt;

&lt;p&gt;想要达到目的，就要分析GC时最占用时间的是什么操作，比较浪费时间的是标记已死对象、清除对象，那么如果可以和用户线程并发的进行，GC的停顿基本就限制在了标记所花费的时间。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-d8de16803a7beba877489d0e65dcdf37c6b.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;如上图，CMS收集器是基于“标记一清除”法实现的,它的运作过程分为4个步骤&lt;/p&gt;

&lt;p&gt;• 初始标记( EMS initial mark) stop the world&lt;/p&gt;

&lt;p&gt;• 并发标记( CMS concurrent mark)&lt;/p&gt;

&lt;p&gt;• 重新标记( CMS remark) stop the world&lt;/p&gt;

&lt;p&gt;• 并发清除( CMS concurrent sweep)&lt;/p&gt;

&lt;p&gt;初始标记的作用是查找GC Roots集合的过程，这个过程处理对象相对较少，速度很快。(为什么要进行初始标记：枚举根结点。&lt;a href=&quot;https://www.zhihu.com/question/502729840&quot;&gt;https://www.zhihu.com/question/502729840&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;并发标记是实际标记所有对象是否已死的过程，比较耗时，所以采用并发的方式。&lt;/p&gt;

&lt;p&gt;重新标记主要是处理并发标记期间所产生的新的垃圾。重新标记阶段不需要再重新标记所有对象，只对并发标记阶段改动过的对象做标记即可。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;并发收集、低停顿&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CMS收集器对CPU资源非常敏感。&lt;/p&gt;

&lt;p&gt;CMS收集器无法处理浮动垃圾( Floating Garbage),可能出现“Concurrent ModeFailure”失败而导致另一次 Full GC的产生。&lt;/p&gt;

&lt;p&gt;“标记一清除”法导致大量空间碎片产生，以至于老年代还有大量空间，却没有整块空间存储某对象。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Concurrent ModeFailure可能原因及方案
原因1：CMS触发太晚
方案：将-XX:CMSInitiatingOccupancyFraction=N调小 (达到百分比进行垃圾回收)；
原因2：空间碎片太多
方案：开启空间碎片整理，并将空间碎片整理周期设置在合理范围；
-XX:+UseCMSCompactAtFullCollection （空间碎片整理）
-XX:CMSFullGCsBeforeCompaction=n
原因3：垃圾产生速度超过清理速度
晋升阈值过小；
Survivor空间过小，导致溢出；
Eden区过小，导致晋升速率提高；存在大对象；
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;&lt;strong&gt;G1收集器&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;G1是一款面向服务端应用的垃圾收集器。下文会简单讲解一下它的“特点”和“内存分配与回收策略”，有基础或不感兴趣的同学直接跳到“G1垃圾回收流程”&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;特点&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;并行与并发&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;G1能充分利用多CPU、多核环境下的硬件优势,使用多个CPU(CPU或者CPU核心)来缩短Stop-The- World停顿的时间,部分其他收集器原本需要停顿Java线程执行的GC动作,G1收集器仍然可以通过并发的方式让Java程序继续执行。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;分代收集&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;与其他收集器一样,分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆,但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;空间整合&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;与CMS的“标记一清理”算法不同,G1从整体来看是基于“标记一整理”算法实现的收集器,从局部(两个 Region之间)上来看是基于“复制”算法实现的,但无论如何,这两种算法都意味着G1运作期间不会产生内存空间碎片,收集后能提供规整的可用内存。这种特性有利于程序长时间运行,分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;可预测的停顿&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这是G1相对于CMS的另一大优势,降低停顿时间是G1和CMS共同的关注点,但G1除了追求低停顿外,还能建立可预测的停顿时间模型,能让使用者明确指定在一个长度为M毫秒的时间片段内,消耗在垃圾收集上的时间不得超过N毫秒,这几乎已经是实时Java(RTSJ)的垃圾收集器的特征了。&lt;/p&gt;

&lt;p&gt;在G1之前的其他收集器进行收集的范围都是整个新生代或者老年代,而G1不再是这样。使用G1收集器时,Java堆的内存布局就与其他收集器有很大差别,它将整个Java堆划分为多个大小相等的独立区域( Region),虽然还保留有新生代和老年代的概念,但新生代和老年代不再是物理隔离的了,它们都是一部分 Region(不需要连续)的集合&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;内存分配与回收策略&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;对象优先在Eden分配&lt;/p&gt;

&lt;p&gt;大多数情况下,对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时,虚拟机将发起一次 Minor[ˈmaɪnə(r)] GC&lt;/p&gt;

&lt;p&gt;大对象直接进入老年代&lt;/p&gt;

&lt;p&gt;所谓的大对象是指,需要大量连续内存空间的Java对象,最典型的大对象就是那种很长的字符串以及数组。大对象对虚拟机的内存分配来说就是一个坏消息(比遇到一个大对象更加坏的消息就是遇到一群“朝生夕灭”的“短命大对象”写程序的时候应当避免),经常出现大对象容易导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来“安置”它们。&lt;/p&gt;

&lt;p&gt;长期存活的对象将进入老年代&lt;/p&gt;

&lt;p&gt;虚拟机给每个对象定义了一个对象年龄(Age)计数器。如果对象在Eden出生并经过第一次 Minor GC后仍然存活,并且能被 Survivor容纳的话,将被移动到 Survivor空间中,并且对象年龄设为1。对象在 Survivor区中每“熬过”一次 Minor GC,年龄就增加1岁,当它的年龄增加到一定程度（默认15岁）会被晋升到老年代中。对象晋升老年代的年龄阈值,可以通过参数据-XX : MaxTenuringThreshold设置&lt;/p&gt;

&lt;p&gt;动态对象年龄判定&lt;/p&gt;

&lt;p&gt;为了能更好地适应不同程序的内存状况,虚拟机并不是水远地要求对象的年龄必须达到了 MaxTenuringThreshold才能晋升老年代,如果在 Survivor空间中相同年龄所有对象大小的总和大于 Survivor空间的一半,年龄大于或等于该年龄的对象就可以直接进入老年代,无须等到 MaxTenuringThreshold中要求的年龄。&lt;/p&gt;

&lt;p&gt;空间分配担保&lt;/p&gt;

&lt;p&gt;在发生 Minor GC之前,虚拟机会先检査老年代最大可用的连续空间是否大于新生代所有对象总空间,如果这个条件成立,那么 Minor GC可以确保是安全的。如果不成立,则虚拟机会查看 HandlePromotionFailure设置值是否允许担保失败。如果允许,那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小,如果大于,将尝试着进行一次 Minor GC,尽管这次 Minor GC是有风险的;如果小于,或者HandlePromotionFailure设置不允许冒险,那这时也要改为进行一次 Full GC.&lt;/p&gt;

&lt;p&gt;为什么要担保:&lt;/p&gt;

&lt;p&gt;Minor GC后还有大量对象存活且空间不够存放新对象，就要直接在老年代存放&lt;/p&gt;

&lt;p&gt;为什么是历次晋升到老年代对象的平均大小:&lt;/p&gt;

&lt;p&gt;取平均值进行比较其实仍然是一种动态概率的手段,也就是说,如果某次 Minor GCd存活后的对象突增,远远高于平均值的话,依然会导致担保失败( HandlePromotionFailure)如果出现了 HandlePromotionFailure失败,那就只好在失败后重新发起一次 Full GC。虽然担保失败时绕的子是最大的,但大部分情况下都还是会将 HandlePromotionFailure开关打开,避免 Full GC过于频繁。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;eden的大小范围默认是 =【-XX:G1NewSizePercent，-XX:G1MaxNewSizePercent】=【整堆5%，整堆60%】&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;humongous如果一个对象的大小已经超过Region大小的50%了，那么就会被放入大对象专门的Region中，这种Region我们叫humongous&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;G1垃圾回收流程&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-79e7a6b4736e8df36b81fdad2f1177e0d66.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;网上对G1的回收阶段有不同的说法，参考Oracle JVM工程师的一个说法：&lt;/p&gt;

&lt;p&gt;他把整个 G1 的垃圾回收阶段分成了这么三个，第一个叫 Minor GC，就是对新生代的垃圾收集，第二个阶段呢叫 Minor GC + Concurrent Mark，就是新生代的垃圾收集同时呢会执行一些并发的标记，这是第二个阶段，第三个阶段呢它叫 Mixed GC 混合收集，这三个阶段是一个循环的过程。刚开始是这个新生代的垃圾收集，经过一段时间，当老年代的内存超过一个阈值了，它会在新生代垃圾收集的同时进行并发的标记，等这个阶段完成了以后，它会进行一个混合收集，混合收集就是会对新生代、幸存区还有老年代都来进行一个规模较大的一次收集，等内存释放掉了，混合收集结束。这时候伊甸园的内存都被释放掉，它会再次进入新生代的一个垃圾收集过程，那我们先来看看这个新生代的收集 Minor GC。&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;Minor GC的回收过程（eden满了回收）&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;选定所有Eden Region放入CSet，使用多线程复制算法将CSet的存活对象复制到Survivor Region或者晋升到Old Region。&lt;/p&gt;

&lt;p&gt;下图分7步演示了这个过程&lt;/p&gt;

&lt;p&gt;1 初始状态，堆无占用&lt;/p&gt;

&lt;p&gt;2 Eden Region满了进行标记&lt;/p&gt;

&lt;p&gt;3 将存活对象复制到Survivor Region&lt;/p&gt;

&lt;p&gt;4 清理Eden Region&lt;/p&gt;

&lt;p&gt;5 Eden Region又满了进行再次标记，此时会连带Survivor Region一起标记&lt;/p&gt;

&lt;p&gt;6 将存活对象复制到另一个Survivor Region&lt;/p&gt;

&lt;p&gt;7 再次清理Eden Region和被标记过的Survivor Region&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-b2e5fe53522ef688777d414392adcd262eb.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Minor GC结束后自动进行并发标记，为以后可能的Mixed GC做准备&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;Mixed GC的回收过程（专注垃圾最多的分区）&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;选定所有Eden Region和全局并发标记计算得到的收益较高的部分Old Region放入CSet，使用多线程复制算法将CSet的存活对象复制到Survivor Region或者晋升到Old Region。&lt;/p&gt;

&lt;p&gt;当堆空间的占用率达到一定阈值后会触发Mixed GC（默认45%，由参数决定）&lt;/p&gt;

&lt;p&gt;Mixed GC它一定会回收年轻代，并会采集部分老年代的Region进行回收的，所以它是一个“混合”GC。&lt;/p&gt;

&lt;p&gt;下图分3步演示了这个过程&lt;/p&gt;

&lt;p&gt;1 并发标记所有Region&lt;/p&gt;

&lt;p&gt;2 并发复制&lt;/p&gt;

&lt;p&gt;3 并发清理&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.cn-north-1.jdcloud-oss.com/shendengbucket1/2023-02-01-13-51Wd141B7V1yLiWeyR.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h1&gt;&lt;strong&gt;ZGC&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;ZGC（Z Garbage Collector） 是一款性能比 G1 更加优秀的垃圾收集器。ZGC 第一次出现是在 JDK 11 中以实验性的特性引入，这也是 JDK 11 中最大的亮点。在 JDK 15 中 ZGC 不再是实验功能，可以正式投入生产使用了。&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;目标低延迟&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;• 保证最大停顿时间在几毫秒之内，不管你堆多大或者存活的对象有多少。&lt;/p&gt;

&lt;p&gt;• 可以处理 8MB-16TB 的堆&lt;/p&gt;

&lt;p&gt;通过以上历代垃圾回收器的讲解，我们大致了解到减少延迟的底层思想不外乎将stop the world进行极限压缩，将能并行的部分全部采用和用户线程并行的方式处理，然而ZGC更&quot;过分&quot;它甚至把一分部垃圾回收的工作交给了用户线程去做，那么它是怎么做到的呢？ZGC的标记和清理工作同CMS、G1大致差不多，仔细看下图的过程，和CMS特别像，这就是我在上文说的CMS其实并没有真正被抛弃，它的部分思想在ZGC 有发扬。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-cd1c070b495b682ef951820a1eb8343b846.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;ZGC 的步骤大致可分为三大阶段分别是标记、转移、重定位。&lt;/p&gt;

&lt;p&gt;标记：从根开始标记所有存活对象&lt;/p&gt;

&lt;p&gt;转移：选择部分活跃对象转移到新的内存空间上&lt;/p&gt;

&lt;p&gt;重定位：因为对象地址变了，所以之前指向老对象的指针都要换到新对象地址上。&lt;/p&gt;

&lt;p&gt;并且这三个阶段都是并发的。&lt;/p&gt;

&lt;p&gt;初始转移需要扫描 GC Roots 直接引用的对象并进行转移，这个过程需要 STW，STW 时间跟 GC Roots 成正比。&lt;/p&gt;

&lt;p&gt;并发转移准备 ：分析最有回收价值GC分页（无STW） 初始转移应对初始标记的数据&lt;/p&gt;

&lt;p&gt;并发转移应对并发标记的数据&lt;/p&gt;

&lt;p&gt;除了标记清理过程继承了CMS和G1的思想，ZGC要做了以下优化&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;并发清理（转移对象）&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;在 CMS 和 G1 中都用到了写屏障，而 ZGC 用到了读屏障。&lt;/p&gt;

&lt;p&gt;写屏障是在对象引用赋值时候的 AOP，而读屏障是在读取引用时的 AOP。&lt;/p&gt;

&lt;p&gt;比如 &lt;code&gt;Object a = obj.foo;&lt;/code&gt;，这个过程就会触发读屏障。&lt;/p&gt;

&lt;p&gt;也正是用了读屏障，ZGC 可以并发转移对象，而 G1 用的是写屏障，所以转移对象时候只能 STW。&lt;/p&gt;

&lt;p&gt;简单的说就是 GC 线程转移对象之后，应用线程读取对象时，可以利用读屏障通过指针上的标志来判断对象是否被转移。&lt;/p&gt;

&lt;p&gt;读屏障会对应用程序的性能有一定影响，据测试，对性能的最高影响达到 4%，但提高了 GC 并发能力，降低了 STW。这就是上面所说的ZGC“过分”地将部分垃圾回收工作交给用户线程的原因。&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;染色指针&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;染色指针其实就是从 64 位的指针中，拿几位来标识对象此时的情况，分别表示 Marked0、Marked1、Remapped、Finalizable。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-27f4c95fd3bfa9f4a9868f506f7aec00f69.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;0-41 这 42 位就是正常的地址，所以说 ZGC 最大支持 4TB (理论上可以16TB)的内存，因为就 42 位用来表示地址&lt;/p&gt;

&lt;p&gt;也因此 ZGC 不支持 32 位指针，也不支持指针压缩。&lt;/p&gt;

&lt;p&gt;其实对象只需要两个状态Marked，Remapped，对象被标记了，对象被重新映射了，为什么会有M0，M1，用来区分本次GC标记和上次GC标记&lt;/p&gt;

&lt;p&gt;以下是标记转移算法说明：&lt;/p&gt;

&lt;p&gt;1 在垃圾回收开始前：Remapped&lt;/p&gt;

&lt;p&gt;2 标记过程：&lt;/p&gt;

&lt;p&gt;标记线程访问&lt;/p&gt;

&lt;p&gt;发现对象地址视图是 Remapped 这时候将指针标记为 M0&lt;/p&gt;

&lt;p&gt;发现对象地址视图是 M0，则说明这个对象是标记开始之后新分配的或者已经标记过的对象，所以无需处理&lt;/p&gt;

&lt;p&gt;应用线程&lt;/p&gt;

&lt;p&gt;如果创建新对象，则将其地址视图置为 M0&lt;/p&gt;

&lt;p&gt;3 标记阶段结束后&lt;/p&gt;

&lt;p&gt;ZGC 会使用一个对象活跃表来存储这些对象地址，此时活跃的对象地址视图是 M0&lt;/p&gt;

&lt;p&gt;4 并发转移阶段&lt;/p&gt;

&lt;p&gt;转移线程：&lt;/p&gt;

&lt;p&gt;转移成功后对象地址视图被置为 Remapped（也就是说 GC 线程如果访问到对象，此时对象地址视图是 M0，并且存在或活跃表中，则将其转移，并将地址视图置为 Remapped ）&lt;/p&gt;

&lt;p&gt;如果在活跃表中，但是地址视图已经是 Remapped 说明已经被转移了，不做处理。&lt;/p&gt;

&lt;p&gt;应用线程：&lt;/p&gt;

&lt;p&gt;如果创建新对象，地址视图会设为 Remapped&lt;/p&gt;

&lt;p&gt;5 下次标记使用M1&lt;/p&gt;

&lt;p&gt;M1 标识本次垃圾回收中活跃的对象&lt;/p&gt;

&lt;p&gt;M0 是上一次回收被标记的对象，但是没有被转移，且在本次回收中也没有被标记活跃的对象。&lt;/p&gt;

&lt;p&gt;下图展示了Marked，Remapped的过程，&lt;/p&gt;

&lt;p&gt;初始化时A，B，C三个对象处于Remapped状态&lt;/p&gt;

&lt;p&gt;第一次GC，A被转移，B未被转移，C无引用将被回收&lt;/p&gt;

&lt;p&gt;第二次GC，由于A被转移过了（Remapped状态），所以被标记M1，此时恰好B为不活跃对象，将被清理&lt;/p&gt;

&lt;p&gt;第三次GC，A又被标记成M0&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-741544ab8c5236c385bcf9fd22845238e33.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;多重映射&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Marked0、Marked1和Remapped三个视图&lt;/p&gt;

&lt;p&gt;ZGC为了能高效、灵活地管理内存，实现了两级内存管理：虚拟内存和物理内存，并且实现了物理内存和虚拟内存的映射关系 在ZGC中这三个空间在同一时间点有且仅有一个空间有效，利用虚拟空间换时间，这三个空间的切换是由垃圾回收的不同阶段触发的，通过限定三个空间在同一时间点有且仅有一个空间有效高效的完成GC过程的并发操作&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-1511a5277d3fe55b43a51c5e0ee2f792c21.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;支持NUMA&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;NUMA是非一致内存访问的缩写 （Non-Uniform Memory Access，NUMA）&lt;/p&gt;

&lt;p&gt;早年如下图：SMP架构 （Symmetric Multi-Processor），因为任一个 CPU 对内存的访问速度是一致的，不用考虑不同内存地址之间的差异，所以也称一致内存访问（Uniform Memory Access， UMA ）。这个核心越加越多，渐渐的总线和北桥就成为瓶颈，那不能够啊，于是就想了个办法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-289bd0c9046afe3dc7a6afce9b30491c23b.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;把 CPU 和内存集成到一个单元上，这个就是非一致内存访问 （Non-Uniform Memory Access，NUMA）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://oscimg.oschina.net/oscnet/up-6b324232a1aaff643cf5ee5f54d47569007.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;ZGC 对 NUMA 的支持是小分区分配时会优先从本地内存分配，如果本地内存不足则从远程内存分配。&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;ZGC优劣&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;综上分析，ZGC在战略上沿用了上几代GC的算法策略，采用并发标记，并发清理的思路，在战术上，通过染色指针、多重映射，读屏障等优化达到更理想的并发清理，通过支持NUMA达到了更快的内存操作。但ZGC同样不是银弹，它也有自身的优缺点，如下&lt;/p&gt;

&lt;p&gt;优势：&lt;/p&gt;

&lt;p&gt;1、一旦某个Region的存活对象被移走之后，这个Region立即就能够被释放和重用掉，而不必等待整个堆中所有指向该Region的引用都被修正后才能清理，这使得理论上只要还有一个空闲Region，ZGC就能完成收集。&lt;/p&gt;

&lt;p&gt;2、颜色指针可以大幅减少在垃圾收集过程中内存屏障的使用数量，ZGC只使用了读屏障。&lt;/p&gt;

&lt;p&gt;3、颜色指针具备强大的扩展性，它可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据，以便日后进一步提高性能。&lt;/p&gt;

&lt;p&gt;劣势：&lt;/p&gt;

&lt;p&gt;1、它能承受的对象分配速率不会太高&lt;/p&gt;

&lt;p&gt;ZGC准备要对一个很大的堆做一次完整的并发收集。在这段时间里面，由于应用的对象分配速率很高，将创造大量的新对象，这些新对象很难进入当次收集的标记范围，通常就只能全部当作存活对象来看待——尽管其中绝大部分对象都是朝生夕灭的，这就产生了大量的浮动垃圾。如果这种高速分配持续维持的话，每一次完整的并发收集周期都会很长，回收到的内存空间持续小于期间并发产生的浮动垃圾所占的空间，堆中剩余可腾挪的空间就越来越小了。目前唯一的办法就是尽可能地增加堆容量大小，获得更多喘息的时间。&lt;/p&gt;

&lt;p&gt;2、吞吐量低于G1 GC&lt;/p&gt;

&lt;p&gt;一般来说，可能会下降5%-15%。对于堆越小，这个效应越明显，堆非常大的时候，比如100G，其他GC可能一次Major或Full GC要几十秒以上，但是对于ZGC不需要那么大暂停。这种细粒度的优化带来的副作用就是，把很多环节其他GC里的STW整体处理，拆碎了，放到了更大时间范围内里去跟业务线程并发执行，甚至会直接让业务线程帮忙做一些GC的操作，从而降低了业务线程的处理能力。&lt;/p&gt;

&lt;h1&gt;总结&lt;/h1&gt;

&lt;p&gt;综上，其实ZGC并不是一个凭空冒出的全新垃圾回收，它结合前几代GC的思想，同时在战术上做了优化以达到极限的STW，ZGC的优秀表现有可能会改变未来程序编写方式，站在垃圾收集器的角度，垃圾收集器特别喜欢不可变对象，原有编程方式鉴于内存、GC能力所限使用可变对象来复用对象而不是销毁重建，试想如果有了ZGC的强大回收能力的加持，是不是我们就可以无脑的使用不可变对象进行代码编写&lt;/p&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;p&gt;《深入理解java虚拟机》&lt;/p&gt;

&lt;p&gt;《JAVA性能权威指南》&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/Baron_ND/article/details/125993457&quot;&gt;JDK 发展至今的垃圾回收机制&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.51cto.com/u_11440114/5103211&quot;&gt;全网最全JDK1~JDK15十一种JVM垃圾收集器的原理总结&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/502729840&quot;&gt;为什么CMS需要初始标记？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/405142523&quot;&gt;一步一图带你理清G1垃圾回收流程&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://baijiahao.baidu.com/s?id=1708808098014477777&amp;amp;wfr=spider&amp;amp;for=pc&quot;&gt;美团面试官问我：ZGC 的 Z 是什么意思？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/356585590/answer/2298574930?utm_campaign=shareopn&amp;amp;utm_content=group1_Answer&amp;amp;utm_medium=social&amp;amp;utm_oi=36325755453440&amp;amp;utm_psn=1576361293098979328&amp;amp;utm_source=wechat_session&quot;&gt;ZGC有什么缺点?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/287945354/answer/458761494?utm_campaign=shareopn&amp;amp;utm_content=group3_Answer&amp;amp;utm_medium=social&amp;amp;utm_oi=36325755453440&amp;amp;utm_psn=1576355927254675456&amp;amp;utm_source=wechat_session&quot;&gt;ZGC 原理是什么，它为什么能做到低延时？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文档示意图原型：&lt;a href=&quot;https://www.processon.com/view/link/63771d355653bb3a840c4027&quot;&gt;https://www.processon.com/view/link/63771d355653bb3a840c4027&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>432dfb8e1eb26e74e0558acb4a5e2136</guid>
<title>从实战出发，聊聊缓存数据库一致性</title>
<link>https://toutiao.io/k/bp2hkj2</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;profile_inner&quot;&gt;
                  &lt;strong class=&quot;profile_nickname&quot;&gt;又拍云&lt;/strong&gt;
                  &lt;img class=&quot;profile_avatar&quot; id=&quot;js_profile_qrcode_img&quot; src=&quot;&quot; alt=&quot;&quot;/&gt;

                  &lt;p class=&quot;profile_meta&quot;&gt;
                  &lt;label class=&quot;profile_meta_label&quot;&gt;Weixin ID&lt;/label&gt;
                  &lt;span class=&quot;profile_meta_value&quot;&gt;upaiyun&lt;/span&gt;
                  &lt;/p&gt;

                  &lt;p class=&quot;profile_meta&quot;&gt;
                  &lt;label class=&quot;profile_meta_label&quot;&gt;About Feature&lt;/label&gt;
                  &lt;span class=&quot;profile_meta_value&quot;&gt;加速在线业务。为客户提供CDN、PrismCDN、云存储、SSL证书、云处理、直播云、点播云等服务。&lt;/span&gt;
                  &lt;/p&gt;
                &lt;/div&gt;
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>c46beae4050a8b572ab3df63490a3b68</guid>
<title>七款云上共享文件系统 POSIX 兼容性大比拼</title>
<link>https://toutiao.io/k/83x9jgr</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;entry&quot;&gt;
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;当用户在进行文件系统选型时，POSIX 语义兼容性是必不可缺的一项考察指标。JuiceFS 一直非常重视对 POSIX 标准的高度兼容，在持续完善功能、提高性能的同时，尽力保持最大程度的 POSIX 兼容性。&lt;/p&gt;&lt;p data-block-key=&quot;fujcb&quot;&gt;近期，就 POSIX 兼容性，&lt;b&gt;我们对腾讯云 CFS、阿里云 NAS、华为云SFS、 GCP Filestore、Amazon EFS、Azure Files 以及 JuiceFS 进行了一次测试&lt;/b&gt;，便于用户了解这些主流文件系统的兼容性表现。&lt;/p&gt;&lt;blockquote data-block-key=&quot;cukbu&quot;&gt;关于 POSIX&lt;br/&gt;POSIX 是可移植操作系统接口(Portable Operating System Interface) 的缩写， 简单来说是文操作系统包括件存储领域应用最广泛的操作系统接口规范。 更多关于POSIX 标准的讨论，可以参考 Quora 上的一个问答 &lt;a href=&quot;https://www.quora.com/What-does-POSIX-conformance-compliance-mean-in-the-distributed-systems-world&quot;&gt;“What does POSIX conformance/compliance mean in the distributed systems world?”&lt;/a&gt;&lt;/blockquote&gt;&lt;h3 data-block-key=&quot;7mciv&quot;&gt;测试方法&lt;/h3&gt;&lt;p data-block-key=&quot;273a&quot;&gt;针对文件系统 POSIX 兼容性的测试，比较流行的一个测试用例集是 &lt;a href=&quot;https://github.com/pjd/pjdfstest&quot;&gt;pjdfstest&lt;/a&gt;，来源于 FreeBSD，也适用于 Linux 等系统。&lt;/p&gt;&lt;h3 data-block-key=&quot;9m0p9&quot;&gt;测试结果&lt;/h3&gt;&lt;img alt=&quot;1&quot; class=&quot;richtext-image full-width&quot; src=&quot;https://static1.juicefs.com/images/1_CnpuENI.width-800.png&quot;/&gt;&lt;p data-block-key=&quot;9p8p3&quot;&gt;测试结果如上图显示，JuiceFS 的失败用例是0，展现出了最好的兼容性。GCP Filestore 次之，有两项失败；华为云 SFS，Amazon EFS 与 Azure Files 失败的测试用例相比其他产品大了几个数量级，为了方便比较，上图的横坐标使用了对数坐标。&lt;/p&gt;&lt;h3 data-block-key=&quot;fj9l3&quot;&gt;失败用例分析&lt;/h3&gt;&lt;p data-block-key=&quot;336s9&quot;/&gt;&lt;img alt=&quot;2&quot; class=&quot;richtext-image full-width&quot; src=&quot;https://static1.juicefs.com/images/2_0ytIfBK.width-800.png&quot;/&gt;&lt;p data-block-key=&quot;jbtc&quot;/&gt;&lt;p data-block-key=&quot;cjks4&quot;&gt;华为云 SFS，Amazon EFS 与 Azure Files 的失败用例无论从总数及类别均大大超出其它几种文件系统，无法放入同一图表对比，后面将单独分析。&lt;/p&gt;&lt;h4 data-block-key=&quot;46al3&quot;&gt;GCP Filestore&lt;/h4&gt;&lt;p data-block-key=&quot;467rs&quot;&gt;GCP Filestore 共失败 2 项测试，unlink 和 utimensat 这两个类别各一。&lt;/p&gt;&lt;p data-block-key=&quot;1utfg&quot;&gt;第一项是 unlink 测试集中的 &lt;a href=&quot;https://github.com/pjd/pjdfstest/blob/master/tests/unlink/14.t&quot;&gt;unlink/14.t&lt;/a&gt;， 对应日志如下。&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;/root/pjdfstest/tests/unlink/14.t ...........&amp;#13;
not ok 4 - tried &#x27;open pjdfstest_b03f52249a0c653a3f382dfe1237caa1 O_RDONLY : unlink pjdfstest_b03f52249a0c653a3f382dfe1237caa1 : fstat 0 nlink&#x27;, expected 0, got 1&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;该测试集（&lt;a href=&quot;https://github.com/pjd/pjdfstest/blob/master/tests/unlink/14.t&quot;&gt;unlink/14.t&lt;/a&gt;）用于验证 一个文件在打开状态下被删除 时的行为：&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;desc=&quot;An open file will not be immediately freed by unlink&quot;&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;删除文件的操作在系统层面实际对应于 unlink，即移除该文件名到对应 inode 的链接，对应 nlink 的值减 1，这个测试用例就是要验证这一点。&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;# A deleted file&#x27;s link count should be 0&amp;#13;
expect 0 open ${n0} O_RDONLY : unlink ${n0} : fstat 0 nlink&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;文件内容只有在链接数（nlink）减少至 0 并且没有打开的文件描述符（fd）指向该文件时才会被真正删除。如果 nlink 没有被正确更新，可能会导致本该删除的文件仍然残留在系统里。&lt;/p&gt;&lt;p data-block-key=&quot;7g4tr&quot;&gt;另一项是 utimensat 测试集中的&lt;a href=&quot;https://github.com/pjd/pjdfstest/blob/master/tests/utimensat/09.t&quot;&gt;utimensat/09.t&lt;/a&gt;，对应日志如下：&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;/root/pjdfstest/tests/utimensat/09.t ........&amp;#13;
not ok 5 - tried &#x27;lstat pjdfstest_909f188e5492d41a12208f02402f8df6 mtime&#x27;, expected 4294967296, got 4294967295&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;该测试用例要求支持 64 位时间戳。&lt;b&gt;GCP Filestore 支持 64 位时间戳，但是会在此基础上减少1，所以在此这个测试用例上虽然失败但是应该不影响使用。&lt;/b&gt;&lt;/p&gt;&lt;h4 data-block-key=&quot;dhgh4&quot;&gt;腾讯云 CFS&lt;/h4&gt;&lt;img alt=&quot;3&quot; class=&quot;richtext-image full-width&quot; src=&quot;https://static1.juicefs.com/images/3_eQ3NTHw.width-800.png&quot;/&gt;&lt;p data-block-key=&quot;f1qp9&quot;/&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;腾讯云 CFS 共失败 7 项，来自三个类别：utimensat, symlink 和 unlink。我们选取了一些重要的失败项进行了分析说明。&lt;/p&gt;&lt;p data-block-key=&quot;2ju53&quot;&gt;symlink 失败用例对应测试日志如下：&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;/root/pjdfstest/tests/symlink/03.t ..........&amp;#13;
not ok 1 - tried &#x27;symlink 7ea12171c487d234bef89d9d77ac8dc2929ea8ce264150140f02a77fc6dcad7c3b2b36b5ed19666f8b57ad861861c69cb63a7b23bcc58ad68e132a94c0939d5/.../... pjdfstest_57517a47d0388e0c84fa1915bf11fe4a&#x27;, expected 0, got EINVAL&amp;#13;
not ok 2 - tried &#x27;unlink pjdfstest_57517a47d0388e0c84fa1915bf11fe4a&#x27;, expected 0, got ENOENT&amp;#13;
Failed 2/6 subtests&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;该测试集（&lt;a href=&quot;https://github.com/pjd/pjdfstest/blob/master/tests/symlink/03.t&quot;&gt;symlink/03.t&lt;/a&gt;）用于测试路径超出 PATH_MAX 长度时 symblink 的行为&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;desc=&quot;symlink returns ENAMETOOLONG if an entire length of either path name exceeded {PATH_MAX} characters&quot;&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;失败的用例对应代码如下：&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;n0=`namegen`nx=`dirgen_max`nxx=&quot;${nx}x&quot;&amp;#13;
mkdir -p &quot;${nx%/*}&quot;&amp;#13;
expect 0 symlink ${nx} ${n0}&amp;#13;
expect 0 unlink ${n0}&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;该测试用例是要创建长度为 PATH_MAX (包括结尾的0在内）的符号链接，通不过表明无法在 腾讯云 NAS 上创建长度为 PATH_MAX 的符号链接。&lt;/p&gt;&lt;h4 data-block-key=&quot;88a06&quot;&gt;阿里云 NAS&lt;/h4&gt;&lt;p data-block-key=&quot;cdi1p&quot;/&gt;&lt;img alt=&quot;4&quot; class=&quot;richtext-image full-width&quot; src=&quot;https://static1.juicefs.com/images/4_sZZuZsZ.width-800.png&quot;/&gt;&lt;p data-block-key=&quot;8v81q&quot;&gt;阿里云 NAS 未能通过 chmod 、utimensat、unlink 上的几项测试用例。&lt;/p&gt;&lt;p data-block-key=&quot;5ok8f&quot;&gt;在 chmod &lt;a href=&quot;https://github.com/pjd/pjdfstest/blob/master/tests/chmod/12.t&quot;&gt;chmod/12.t&lt;/a&gt; 这个测试集中，阿里云 NAS 失败了以下几个项目&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;/root/pjdfstest/tests/chmod/12.t ............&amp;#13;
not ok 3 - tried &#x27;-u 65534 -g 65534 open pjdfstest_db85e6a66130518db172a8b6ce6d53da O_WRONLY : write 0 x : fstat 0 mode&#x27;, expected 0777, got 04777&amp;#13;
not ok 4 - tried &#x27;stat pjdfstest_db85e6a66130518db172a8b6ce6d53da mode&#x27;, expected 0777, got 04777&amp;#13;
not ok 7 - tried &#x27;-u 65534 -g 65534 open pjdfstest_db85e6a66130518db172a8b6ce6d53da O_RDWR : write 0 x : fstat 0 mode&#x27;, expected 0777, got 02777&amp;#13;
not ok 8 - tried &#x27;stat pjdfstest_db85e6a66130518db172a8b6ce6d53da mode&#x27;, expected 0777, got 02777&amp;#13;
not ok 11 - tried &#x27;-u 65534 -g 65534 open pjdfstest_db85e6a66130518db172a8b6ce6d53da O_RDWR : write 0 x : fstat 0 mode&#x27;, expected 0777, got 06777&amp;#13;
not ok 12 - tried &#x27;stat pjdfstest_db85e6a66130518db172a8b6ce6d53da mode&#x27;, expected 0777, got 06777&amp;#13;
Failed 6/14 subtests&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;该测试集（&lt;a href=&quot;https://github.com/pjd/pjdfstest/blob/master/tests/chmod/12.t&quot;&gt;chmod/12.t&lt;/a&gt;）用于测试 SUID/SGID 位的行为&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;desc=&quot;verify SUID/SGID bit behaviour&quot;&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;我们选取其中的第11和12个测试用例来详细解释一下，同时覆盖了这两个权限位&lt;/p&gt;
                        
                    
                        
                        &lt;pre class=&quot;line-numbers rounded-md&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;# Check whether writing to the file by non-owner clears the SUID+SGID.&amp;#13;
expect 0 create ${n0} 06777&amp;#13;
expect 0777 -u 65534 -g 65534 open ${n0} O_RDWR : write 0 x : fstat 0 mode&amp;#13;
expect 0777 stat ${n0} mode&amp;#13;
expect 0 unlink ${n0}&lt;/code&gt;&lt;/pre&gt;
                        
                    
                        
                            &lt;p data-block-key=&quot;esjf6&quot;&gt;此处，我们先以 06777 的权限创建目标文件，然后修改文件内容，检查 SUID 和 SGID 是否被正确清除。文件权限里的 777 大家会比较熟悉，分别对应 owner，group和 other 的 rwx，即可读、可写、可执行。最前面的 0 表示八进制数。&lt;/p&gt;&lt;p data-block-key=&quot;arq8n&quot;&gt;第二位 6 需要着重解释下，这个八位元组（octet）代表特殊权限位，其中前两位分别对应 setuid/setgid（或称 SUID/SGID），可以应用于可执行文件及公共目录。该权限位被设置时，任何用户都会以 owner （或 group）身份来运行该文件。这个特殊的属性允许用户获取通常只对 owner 开放的文件和目录访问权限。例如 passwd 命令就设置了 setuid 权限，这允许普通用户修改密码，因为保存密码的文件是只允许 root 访问的，用户不可直接修改。&lt;/p&gt;&lt;p data-block-key=&quot;7ofvp&quot;&gt;setuid/setgid 设计的出发点是提供一种方法，让用户以限定的方式（指定可执行文件）访问受限文件（非当前用户所有）。因此，当文件被非 owner 修改时应自动清除此权限位，以避免用户通过这个途径获取其他权限。&lt;/p&gt;&lt;p data-block-key=&quot;9nujk&quot;&gt;从测试结果中我们可以看到在阿里云 NAS 中，&lt;b&gt;文件被非 owner 修改时，setuid/setgid 均未被清除，这样实际上用户可以通过修改文件内容以该 owner 身份进行任意操作，这将会是个安全隐患。&lt;/b&gt;&lt;/p&gt;&lt;p data-block-key=&quot;44aj&quot;&gt;参考阅读： &lt;a href=&quot;https://docs.oracle.com/cd/E19683-01/816-4883/secfile-69/index.html&quot;&gt;Special File Permissions (setuid, setgid and Sticky Bit) (System Administration Guide: Security Services)&lt;/a&gt;&lt;/p&gt;&lt;h4 data-block-key=&quot;dhmtk&quot;&gt;华为云 SFS 与 Amazon EFS&lt;/h4&gt;&lt;p data-block-key=&quot;95se8&quot;&gt;华为云 SFS 与 Amazon Elastic File System (EFS) 在 pjdfstest 测试中失败内容类似。失败比例大概为21%，失败用例几乎覆盖了所有类别。&lt;/p&gt;&lt;p data-block-key=&quot;f68rl&quot;/&gt;&lt;img alt=&quot;5&quot; class=&quot;richtext-image full-width&quot; src=&quot;https://static1.juicefs.com/images/5_uSMbcEv.width-800.png&quot;/&gt;&lt;p data-block-key=&quot;86ah7&quot;/&gt;&lt;img alt=&quot;6&quot; class=&quot;richtext-image full-width&quot; src=&quot;https://static1.juicefs.com/images/6_GcsqnKV.width-800.png&quot;/&gt;&lt;p data-block-key=&quot;clbme&quot;&gt;两者都支持以 NFS 方式挂载，但对 NFS 特性的支持并不完整。比如都不支持块设备和字符设备，这直接导致了 pjdfstest 中大量测试用例的失败。排除这两类文件之后，仍然有上百项不同类别的失败，&lt;b&gt;所以在复杂场景中应用二者必须慎之又慎。&lt;/b&gt;&lt;/p&gt;&lt;h4 data-block-key=&quot;45rf9&quot;&gt;Azure Files&lt;/h4&gt;&lt;p data-block-key=&quot;70a9c&quot;/&gt;&lt;img alt=&quot;1111&quot; class=&quot;richtext-image full-width&quot; src=&quot;https://static1.juicefs.com/images/1111_3pla2Xu.width-800.png&quot;/&gt;&lt;p data-block-key=&quot;fne6f&quot;/&gt;&lt;p data-block-key=&quot;382nn&quot;&gt;而 Azure Files 失败率达到了 62%，这说明一些基本的 POSIX 场景可能都会有不兼容的问题。比如 Azure Files 文件与文件夹默认权限 0777，所有者为 root，且都不支持修改，也就是说没有任何权限限制。另外 Azure File shares 也不支持硬链接与符号链接。&lt;b&gt;所以使用 Azure Files 需要仔细测试并慎重考虑场景是否足够简单。&lt;/b&gt;&lt;/p&gt;&lt;h3 data-block-key=&quot;1e71n&quot;&gt;总结&lt;/h3&gt;&lt;ul&gt;&lt;li data-block-key=&quot;387qj&quot;&gt;JuiceFS 在兼容性方面表现最好，通过了全部的测试项。&lt;/li&gt;&lt;li data-block-key=&quot;athg1&quot;&gt;Google Filestore 次之，有两类未能通过，其中有一项不影响实际使用。&lt;/li&gt;&lt;li data-block-key=&quot;82mog&quot;&gt;腾讯云 CFS 与阿里云 NAS 相差不多，皆有 7-8 项未通过。&lt;/li&gt;&lt;li data-block-key=&quot;dj0jf&quot;&gt;华为云 SFS ，Amazon EFS 与 Azure Files的兼容性较差，有大量的兼容性测试通不过，其中包括有严重安全隐患的若干个测试用例，使用前建议做安全性评估。&lt;/li&gt;&lt;/ul&gt;
                        
                    
                    &lt;/div&gt;
                    &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>a30383b5648518bcdf64d7cf315cf706</guid>
<title>JVM G1GC 小册子</title>
<link>https://toutiao.io/k/9y33xk8</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div id=&quot;content&quot; class=&quot;content&quot;&gt;
                    &lt;main&gt;
                        
&lt;p&gt;最近在看中村成洋的《深入 Java 虚拟机 -- JVM G1GC 的算法与实现》，把 G1 算法介绍得比较明白。这本书算是读书笔记，也算是教程，把自己之前的理解结合书里讲解的内容，重新表述，进而增进自己的理解，也算是在践行费曼学习法。&lt;/p&gt;
&lt;p&gt;有很多看书时认为理解的内容，在写作过程中发现其实一窍不通，只能不断通过查其它资料、源码来增进自己的理解。因此读者如果时间允许，也欢迎像我一样写个教程来增进自己的理解。&lt;/p&gt;
&lt;p&gt;GC 一直在不断发展，一些机制、源码都在不断更新。《深入 Java 虚拟机》里的一些内容也已经过时，本书的一些内容基于对 OpenJDK 11 源码的理解做了更新。当然这些内容可能在最新的 JDK 中也已经有了变化，这点也请读者注意。&lt;/p&gt;
&lt;p&gt;另外由于我并不是专业做虚拟机的，对 GC 中一些机制的理解难免有误，欢迎批评指正。&lt;/p&gt;

                    &lt;/main&gt;

                    &lt;nav class=&quot;nav-wrapper&quot; aria-label=&quot;Page navigation&quot;&gt;
                        
                        

                        
                            &lt;a rel=&quot;next&quot; href=&quot;algo/gcs.html&quot; class=&quot;mobile-nav-chapters next&quot; title=&quot;Next chapter&quot; aria-label=&quot;Next chapter&quot; aria-keyshortcuts=&quot;Right&quot;&gt;
                                &lt;i class=&quot;fa fa-angle-right&quot;/&gt;
                            &lt;/a&gt;
                        

                        &lt;p/&gt;
                    &lt;/nav&gt;
                &lt;/div&gt;
            &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>